<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey on In-context Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-05">5 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ce</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingyuan</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heming</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<addrLine>3 ByteDance</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey on In-context Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-05">5 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">A1E7D131563107C0F8849966F2E204CB</idno>
					<idno type="arXiv">arXiv:2301.00234v6[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-01T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the scaling of model size and data size <ref type="bibr" target="#b17">(Brown et al., 2020;</ref><ref type="bibr" target="#b23">Chowdhery et al., 2023;</ref><ref type="bibr" target="#b82">OpenAI, 2023;</ref><ref type="bibr">Touvron et al., 2023a,b)</ref>, large language models (LLMs) demonstrate the in-context learning (ICL) ability, that is, learning from a few examples in the context. Many studies have shown that LLMs can perform a series of complex tasks through ICL, such as solving mathematical reasoning problems <ref type="bibr">(Wei et al., 2022c)</ref>. These strong abilities have been widely verified as emerging abilities for large language models <ref type="bibr">(Wei et al., 2022b)</ref>.</p><p>The key idea of in-context learning is to learn from analogy. Figure <ref type="figure" target="#fig_0">1</ref> gives an example that describes how language models make decisions via ICL. First, ICL requires a few demonstration examples to form a prompt context. These examples 1 We list the author contributions and roles in Appendix D.4. are usually written in natural language templates. Then, ICL concatenates a query question and the piece of prompt context together to form the input, which is then fed into the language model for prediction. Different from supervised learning, which requires a training stage that uses backward gradients to update model parameters, ICL does not perform parameter updates. The model is expected to learn the pattern hidden in the demonstration and accordingly make the right prediction.</p><p>As a new paradigm, ICL has multiple attractive advantages. First, since the demonstration is written in natural language, it provides an interpretable interface to communicate with LLMs <ref type="bibr" target="#b17">(Brown et al., 2020)</ref>. This paradigm makes it much easier to incorporate human knowledge into LLMs by changing the demonstration and templates <ref type="bibr" target="#b66">(Liu et al., 2022;</ref><ref type="bibr" target="#b72">Lu et al., 2022;</ref><ref type="bibr">Wei et al., 2022c;</ref><ref type="bibr">Wu et al., 2023b)</ref>. Second, in-context learning is similar to the decision process of human beings by learning from analogy <ref type="bibr" target="#b135">(Winston, 1980)</ref>. Third, compared to supervised training, ICL is a training-free learning framework. This could not only greatly reduce the computational costs for adapting the model to new tasks, but also make language-model-as-aservice <ref type="bibr" target="#b118">(Sun et al., 2022)</ref> possible and can be easily applied to large-scale real-world tasks. Despite being promising, there are also interesting questions and intriguing properties that require further investigation in ICL. Although a range of vanilla GPT models show excellent ICL capability, several studies have found that this capability can be significantly improved through adaptation during pretraining <ref type="bibr">(Min et al., 2022b;</ref><ref type="bibr">Li et al., 2024c)</ref>. Moreover, the performance of ICL is sensitive to specific settings, including the prompt template, the selection and order of demonstration examples, and other factors <ref type="bibr">(Wang et al., 2023e;</ref><ref type="bibr">Liu et al., 2024b)</ref>. Additionally, optimizing the conciseness of demonstration examples and improving the computational efficiency of ICL are critical areas of ongoing research <ref type="bibr">(Liu et al., 2024a)</ref>. Furthermore, despite preliminary explanations <ref type="bibr">(Dai et al., 2023a;</ref><ref type="bibr" target="#b50">Jiang, 2023)</ref>, the underlying working mechanism of ICL remains unclear and requires further investigation.</p><p>With the rapid growth of studies in ICL, our survey aims to sensitize the community toward the current progress. In the following sections, we delve into an in-depth discussion of related studies, and we summarize the taxonomy in Figure <ref type="figure" target="#fig_1">2</ref> and the key findings in Appendix A. We highlight the challenges and potential directions and hope our work provide a useful roadmap for beginners interested in this area and shed light on future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definition and Formulation</head><p>Following <ref type="bibr" target="#b17">Brown et al. (2020)</ref>, we here provide a formal definition of in-context learning:</p><p>In-context learning is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration.</p><p>Formally, given a query input text x and a set of candidate answers Y = {y 1 , . . . , y m }, a pretrained language model M takes the candidate answer with the maximum score as the prediction, 1 conditioned a demonstration set C. C contains an optional task instruction I and k demonstration examples, thus C = {I, s(x 1 , y 1 ), . . . , s(x k , y k )} or C = {s ′ (x 1 , y 1 , I), . . . , s ′ (x k , y k , I)}, where s ′ (x i , y i , I) is an example written in natural language according to the task. Depending on whether k and the demonstration examples belong to the same task, it can be categorized as task-specific ICL and cross-task ICL. In the latter, different examples have their own instructions. The likelihood of a candidate answer y j comes from a scoring function f on the whole input sequence:</p><formula xml:id="formula_0">P (y j | x) ≜ f M (y j , C, x)<label>(1)</label></formula><p>The final predicted label ŷ is the candidate answer with the highest probability:</p><formula xml:id="formula_1">ŷ = arg max y j ∈Y P (y j | x).<label>(2)</label></formula><p>According to the definition, we can see that ICL differs from related concepts as follows: (1) Prompt Learning: prompts can be discrete templates or soft parameters that encourage the model to predict the desired output. ICL can be regarded as a subclass of prompt tuning where the demonstration examples are part of the prompt. <ref type="bibr">Liu et al. (2023c)</ref> made a thorough survey on prompt learning, but ICL was not included in their study. (2) Few-shot Learning: few-shot learning is a general machine learning approach that involves adapting model parameters to perform a task with a limited number of supervised examples <ref type="bibr" target="#b125">(Wang and Yao, 2019)</ref>. In contrast, ICL does not require parameter updates and is directly performed on pretrained LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Training</head><p>Although LLMs have demonstrated promising ICL capability directly, many studies revealed that these ICL capabilities can be further enhanced through specialized training before inference <ref type="bibr" target="#b22">(Chen et al., 2022;</ref><ref type="bibr" target="#b36">Gu et al., 2023;</ref><ref type="bibr" target="#b95">Shi et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pretraining</head><p>One straightforward direction to boost the ICL capability of LLMs is through pretraining or continual pretraining. For instance, <ref type="bibr" target="#b36">Gu et al. (2023)</ref> and <ref type="bibr" target="#b95">Shi et al. (2024)</ref> proposed to reorganize pretraining corpora by aggregating related contexts, making models learn to reason across prior demonstrations. Differently, <ref type="bibr">Li et al. (2024c)</ref> introduced a meta-distillation pretraining process, which allows LLMs to reason with distilled demonstration vectors, thereby enhancing ICL efficiency without compromising its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Warmup</head><p>Another way to enhance ICL ability is adding a continual training stage between pretraining and ICL inference, which we call model warmup for short. Warmup is an optional procedure for ICL, which adjusts LLMs before inference by modifying or adding parameters.</p><p>As most pretraining data are not tailored for ICL <ref type="bibr" target="#b22">(Chen et al., 2022)</ref>, researchers have introduced various warmup strategies to bridge the gap between pretraining and ICL inference. Both <ref type="bibr">Min et al. (2022b)</ref> and <ref type="bibr">Wang et al. (2022b)</ref> proposed to continually finetune LLMs on a broad range of tasks with multiple demonstration examples, which boosts ICL abilities. To encourage the model to learn input-label mappings from the context, <ref type="bibr">Wei et al. (2023a)</ref> proposed symbol tuning, which substitutes natural language labels (e.g., "positive/negative sentiment") with arbitrary symbols (e.g., "foo/bar"). <ref type="bibr" target="#b22">Chen et al. (2022)</ref> proposed a self-supervised method to align raw text with ICL formats in downstream tasks. Besides, multiple studies have indicated the potential value of instructions <ref type="bibr" target="#b79">(Mishra et al., 2021;</ref><ref type="bibr">Wei et al., 2022a)</ref>. Tuning the 137B LaMDA-PT <ref type="bibr">(Thoppilan et al., 2022)</ref> on over 60 datasets verbalized via natural language instruction templates, FLAN <ref type="bibr">(Wei et al., 2022a)</ref> improves the ability of LLMs to follow instructions, boosting both the zero-shot and few-shot ICL performance. <ref type="bibr">Chung et al. (2022)</ref> and <ref type="bibr">Wang et al. (2022b)</ref> proposed to further scale up instruction tuning with more than 1000+ task instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prompt Designing</head><p>In this section, we focus on the principles of ICL during inference, including demonstration organization ( §4.1) and instruction formatting ( §4.2) .</p><p>Table 1: Summary of representative demonstration designing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Demonstration Organization</head><p>Many studies have shown that the performance of ICL strongly relies on the demonstration surface, including the selection, formatting, and ordering of demonstration examples <ref type="bibr" target="#b154">(Zhao et al., 2021;</ref><ref type="bibr" target="#b72">Lu et al., 2022)</ref>. In this subsection, we survey demonstration organization strategies and classify them into three categories, as shown in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Demonstration Selection</head><p>Demonstrations selection aims to answer a fundamental question: Which samples are good examples for ICL? We categorize the related studies into two approaches: unsupervised methods based on predefined metrics and supervised methods.</p><p>Unsupervised Method A straightforward approach to selecting ICL examples is to choose the nearest neighbors of input instances based on their similarities <ref type="bibr" target="#b66">(Liu et al., 2022;</ref><ref type="bibr" target="#b109">Tanwar et al., 2023;</ref><ref type="bibr" target="#b86">Qin et al., 2023)</ref>. Distance metrics, such as L2 distance or cosine similarity based on sentence embeddings, are commonly used for this purpose. For example, <ref type="bibr" target="#b66">Liu et al. (2022)</ref> proposed KATE, the first kNN-based unsupervised retriever for selecting in-context examples. Similarly, k-NN cross-lingual demonstrations can be retrieved for multi-lingual ICL to strengthen source-target language alignment <ref type="bibr" target="#b109">(Tanwar et al., 2023)</ref>. <ref type="bibr" target="#b102">Su et al. (2023)</ref> proposed to combine graphs and confidence scores to select diverse and representative examples. In addition to distance metrics, mutual information <ref type="bibr" target="#b100">(Sorensen et al., 2022)</ref> and perplexity <ref type="bibr" target="#b35">(Gonen et al., 2023)</ref> have proven valuable for prompt selection without labeled examples or specific LLMs. Furthermore, using output scores of LLMs as unsupervised metrics has shown effectiveness in demonstration selection <ref type="bibr">(Wu et al., 2023b;</ref><ref type="bibr" target="#b80">Nguyen and Wong, 2023;</ref><ref type="bibr" target="#b60">Li and Qiu, 2023)</ref>. Particularly, <ref type="bibr">Wu et al. (2023b)</ref> selected the best subset permutation of kNN examples based on the code length for data transmission to compress label y given x and C. <ref type="bibr" target="#b60">Li and Qiu (2023)</ref> used infoscore, i.e., the average of P (y|x i , y i , x)P (y|x) for all (x, y) pairs in a validation set with a diversity regularization.</p><p>Supervised Method Though off-the-shelf retrievers offer convenient services for extensive NLP tasks, they are heuristic and sub-optimal due to the lack of task-specific supervision. To address this issue, numerous supervised methods have been developed <ref type="bibr" target="#b91">(Rubin et al., 2022;</ref><ref type="bibr" target="#b145">Ye et al., 2023;</ref><ref type="bibr">Wang et al., 2023e;</ref><ref type="bibr">Zhang et al., 2022a)</ref>. EPR <ref type="bibr" target="#b91">(Rubin et al., 2022)</ref> introduced a two-stage method to train a dense retriever for demonstration selection. For a specific input, it first utilized unsupervised methods (e.g., BM25) to recall similar examples as candidates and then used this data to build a supervised dense retriever. Following EPR, <ref type="bibr">Li et al. (2023d)</ref> adopted a unified demonstration retriever to select demonstrations across different tasks. Unlike prior work that retrieves individual demonstrations, <ref type="bibr" target="#b145">Ye et al. (2023)</ref> proposed retrieving entire demonstration sets to model inter-relationships between examples. Additionally, <ref type="bibr" target="#b74">Mavromatis et al. (2023)</ref> introduced AdaICL, a model-adaptive method that employs LLM to predict the unlabeled data set, generating an uncertainty score for each instance.</p><p>Based on prompt tuning, Wang et al. (2023e) viewed LLMs as topic models that can infer concepts θ from a few demonstrations and generate tokens based on these concepts. They represent latent concepts with task-related concept tokens, which are learned to maximize P (y|x, θ). Demonstrations are selected based on their likelihood to infer the concept variable using P (θ|x, y). Additionally, reinforcement learning was introduced by Zhang et al. (2022a) for example selection. They formulated demonstration selection as a Markov decision process <ref type="bibr" target="#b12">(Bellman, 1957)</ref> and selected demonstrations via Q-learning. The action is choosing an example, and the reward is defined as the accuracy  <ref type="bibr" target="#b66">(Liu et al., 2022)</ref>, votek <ref type="bibr" target="#b102">(Su et al., 2023)</ref>, mdl <ref type="bibr">(Wu et al., 2023b)</ref> show that the effectiveness of ICL example selection methods are model-dependent. On GPT-2, the mdl method performs the best, while on the other three models, topk performs the best.</p><p>of a labeled validation set.</p><p>In order to have a more intuitive comparison of the performance of several unsupervised methods, we select topk <ref type="bibr" target="#b66">(Liu et al., 2022)</ref>, votek <ref type="bibr" target="#b102">(Su et al., 2023)</ref>, mdl <ref type="bibr">(Wu et al., 2023b)</ref> to conduct experiments. The result is shown in Table <ref type="table" target="#tab_1">2</ref>. The details of the experiment can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Demonstration Reformatting</head><p>In addition to directly selecting examples from training data, another research trend involves utilizing LLMs to reformat the representation of existing demonstrations <ref type="bibr">(Kim et al., 2022;</ref><ref type="bibr">Yang et al., 2023a;</ref><ref type="bibr">Hao et al., 2022b;</ref><ref type="bibr">Yang et al., 2023b;</ref><ref type="bibr">Liu et al., 2024a;</ref><ref type="bibr">Li et al., 2024a)</ref>. For instance, Kim et al. ( <ref type="formula">2022</ref>) proposed generating demonstrations directly from LLMs to reduce the reliance on external demonstration data. Structured Prompting <ref type="bibr">(Hao et al., 2022b)</ref> proposed to encode demonstration examples separately with special positional embeddings, which are then provided to the test examples using a rescaled attention mechanism. Diverging from these methods, other approaches focus on modifying the latent representation of demonstrations <ref type="bibr">(Liu et al., 2024a;</ref><ref type="bibr">Li et al., 2024a)</ref>. Specifically, <ref type="bibr">Liu et al. (2024a)</ref> developed In-Context Vectors (ICVs) derived from the latent embeddings of demonstration examples in LLMs. These ICVs are used during inference to adjust the latent states of the LLM, thereby enhancing the model's ability to follow the demonstrations more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Demonstration Ordering</head><p>Ordering the selected demonstration examples is also an important aspect of demonstration organization. <ref type="bibr" target="#b72">Lu et al. (2022)</ref> have proven that order sensitivity is a common problem and always exists for various models. To handle this problem, previous studies have proposed several training-free methods for sorting demonstration examples. Particularly, <ref type="bibr" target="#b66">Liu et al. (2022)</ref> arranged examples based on their proximity to the input, positioning the closest example as the rightmost demonstration. <ref type="bibr" target="#b72">Lu et al. (2022)</ref> introduced global and local entropy metrics, finding a positive correlation between these metrics and the ICL performance. Consequently, they utilized the entropy metric to determine the optimal demonstration ordering. Additionally, ICCL <ref type="bibr">(Liu et al., 2024b)</ref> suggested ranking demonstrations from simple to complex, thereby gradually increasing the complexity of demonstration examples during the inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instruction Formatting</head><p>A common way to format demonstrations is concatenating examples (x 1 , y 1 ), . . . , (x k , y k ) with a template T directly. However, in some tasks that need complex reasoning (e.g., math word problems and commonsense reasoning), it is not easy to learn the mapping from x i to y i with only k demonstrations. Although template engineering has been studied in prompting <ref type="bibr">(Liu et al., 2023c)</ref>, some researchers aim to design a better format of demonstrations for ICL by describing tasks with the instruction I. <ref type="bibr" target="#b45">Honovich et al. (2023)</ref> found that given several demonstration examples, LLMs can generate task instructions themselves. Considering the generation abilities of LLMs, <ref type="bibr">Zhou et al. (2023c)</ref> proposed an Automatic Prompt Engineer for automatic instruction generation and selection. To further improve the quality of the automatically generated instructions, several strategies have proposed using LLMs to bootstrap off its own generations <ref type="bibr">(Wang et al., 2023f;</ref><ref type="bibr" target="#b21">Chen et al., 2024)</ref>. Additionally, chain-of-thought (CoT) <ref type="bibr">(Wei et al., 2022c)</ref> introduces intermediate reasoning steps between inputs and outputs to enhance problem-solving and comprehension. Recent advancements have also emphasized the process of enhancing step-by-step reasoning in models <ref type="bibr">(Zhang et al., 2023c;</ref><ref type="bibr">Wang et al., 2022a;</ref><ref type="bibr">Zhou et al., 2023a)</ref>.</p><p>Method Target Efficiency Coverage Stability Direct M(y j | C, x) +++ + + PPL PPL(S j ) + +++ + Channel M(x | C, y j ) + + ++</p><p>Table 3: Summary of different scoring functions. Coverage refers to task coverage. The qualitative results for 'Efficiency' and 'Stability' metrics are elaborated in Table <ref type="table" target="#tab_6">4</ref> and <ref type="table">Table 5</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scoring Function</head><p>The scoring function determines how to transform the predictions of a language model into an estimation of the likelihood of a specific answer. The Direct method uses the conditional probability of candidate answers represented by tokens in the model's vocabulary <ref type="bibr" target="#b17">(Brown et al., 2020</ref>). The answer with the highest probability is selected as the final answer, but this method restricts template design by requiring answer tokens to be at the end of input sequences. Perplexity (PPL) is another commonly used metric that computes the sentence perplexity of the entire input sequence S j = {C, s(x, y j , I)}, which includes tokens from demonstration examples C, the input query x, and the candidate label y j . PPL evaluates the probability of the sentence, eliminating token position limitations but requiring additional computation time. <ref type="bibr">Min et al. (2022a)</ref> proposed using channel models (Channel) to compute the conditional probability in reverse, estimating the likelihood of the input query given the label. This approach requires language models to generate every token in the input, potentially boosting performance under imbalanced training data. We summarize all three scoring functions in Table <ref type="table">3</ref>. Note that in Table <ref type="table">3</ref>, 'Efficiency' refers to the language model inference latency; 'Coverage' reflects whether the method utilizes the output probability of the local or all token positions in the input sequence; and 'Stability' indicates whether the in-context learning ability is easily affected by changes in the demonstration examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>To understand ICL, recent studies attempt to investigate what influence ICL performance <ref type="bibr" target="#b96">(Shin et al., 2022;</ref><ref type="bibr">Yoo et al., 2022;</ref><ref type="bibr" target="#b53">Kossen et al., 2023)</ref> and why ICL works <ref type="bibr">(Dai et al., 2023a;</ref><ref type="bibr" target="#b48">Irie et al., 2022)</ref>.</p><p>In this section, we present a detailed elaboration of influencing factors ( §5.1) and learning mechanisms ( §5.2) of ICL, as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Influencing Factors</head><p>We discuss relevant research addressing what influences ICL performance, including factors both in the pretraining stage and in the inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Pretraining Stage</head><p>We first introduce factors that influence the pretraining stage. The diversity of pretraining corpora significantly impacts ICL performance <ref type="bibr" target="#b96">(Shin et al., 2022;</ref><ref type="bibr" target="#b142">Yadlowsky et al., 2023;</ref><ref type="bibr" target="#b90">Raventós et al., 2023)</ref>. In particular, <ref type="bibr" target="#b96">Shin et al. (2022)</ref> found that the source domain is more important than the corpus size, suggesting that combining multiple corpora may lead to the emergence of ICL ability. Similarly, Raventós et al. (2023) empirically identified a task diversity threshold beyond which LLMs exhibit strong ICL capabilities in unseen tasks. Another line of research investigates the impact of data distribution on ICL (Chan et al., 2022; Wies et al., 2023). For instance, Chan et al. (2022) demonstrated that ICL capability emerges when the training data exhibits specific distributional properties, such as burstiness, wherein items appear in clusters rather than being uniformly distributed over time. Beyond these works, several studies have investigated the impact of model architecture and training process on ICL performance <ref type="bibr">(Wei et al., 2022b;</ref><ref type="bibr" target="#b17">Brown et al., 2020;</ref><ref type="bibr" target="#b21">Ding et al., 2024)</ref>. <ref type="bibr">Wei et al. (2022b)</ref> investigated the emergent abilities of many large-scale models on multiple tasks. They suggested that a pretrained model acquires some emergent ICL abilities when it reaches a large scale of pretraining steps or model parameters. <ref type="bibr" target="#b21">Ding et al. (2024)</ref> pointed out that the in-context samples should attend to each other during inference, indicating that current causal LLMs may lead to suboptimal ICL performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Inference Stage</head><p>During inference, there are also multiple properties of demonstration examples that influence ICL performance. <ref type="bibr">Min et al. (2022c)</ref> proved that inputlabel settings such as the pairing format, the exposure of label space, and the input distribution contribute substantially to ICL performance. However, contrary to the conclusion in <ref type="bibr">Min et al. (2022c)</ref> that input-label mapping matters little to ICL, latter studies showed that the accurate mapping influence ICL performance significantly <ref type="bibr">(Yoo et al., 2022;</ref><ref type="bibr">Pan et al., 2023a;</ref><ref type="bibr">Tang et al., 2023a)</ref>. <ref type="bibr">Wei et al. (2023b)</ref> further pointed that flipped or semanticallyunrelated input-label mapping also can be learned. From the perspective of demonstration construction, recent literature focuses on the diversity and simplicity of demonstrations <ref type="bibr">(An et al., 2023)</ref>, the order of samples <ref type="bibr" target="#b72">(Lu et al., 2022;</ref><ref type="bibr">Zhang et al., 2022b;</ref><ref type="bibr">Liu et al., 2023b)</ref>, and the similarity between demonstrations and queries <ref type="bibr" target="#b66">(Liu et al., 2022)</ref>. For example, <ref type="bibr" target="#b66">Liu et al. (2022)</ref> found that demonstration samples with embeddings closer to those of the query samples typically yield better performance than those with more distant embeddings. Notably, despite efforts to refine demonstrations to optimize the performance, there still remain clear feature biases during ICL inference <ref type="bibr" target="#b97">(Si et al., 2023)</ref>. Overcoming strong prior biases and ensuring the model gives equal weight to all contextual information remain challenges <ref type="bibr" target="#b53">(Kossen et al., 2023)</ref>.</p><formula xml:id="formula_2">Similarity Label 1 Label 2 ••• Label k Query Input 1 Input 2 ••• Input k Bayes Gradient Descent</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theoretical Interpretation Inference Stage Factor</head><note type="other">Corpus Architecture Train Pretraining Stage Factor Functional Modules Induction Heads Self</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learning Mechanism</head><p>From a learning mechanism perspective, we delve into the research addressing why ICL is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Functional Modules</head><p>The ICL capability is intimately connected to specific functional modules within Transformers. As one of the core components, the attention module is a focal point in the study of ICL mechanism <ref type="bibr" target="#b81">(Olsson et al., 2022;</ref><ref type="bibr" target="#b14">Bietti et al., 2023;</ref><ref type="bibr">Dai et al., 2023a;</ref><ref type="bibr" target="#b48">Irie et al., 2022;</ref><ref type="bibr">Li et al., 2023c;</ref><ref type="bibr" target="#b33">Gao et al., 2023;</ref><ref type="bibr">Zhang et al., 2023b)</ref>. Particularly, <ref type="bibr" target="#b81">Olsson et al. (2022)</ref> identified specific attention heads, referred to as "induction heads", that can replicate previous patterns for next-token prediction, thus progressively developing ICL capabilities. Additionally, <ref type="bibr">Wang et al. (2023b)</ref> focused on the information flow in Transformers and found that during the ICL process, demonstration label words serve as anchors, which aggregate and distribute key information for the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Theoretical Interpretation</head><p>In this subsection, we introduce the theoretical interpretations of ICL from different views.</p><p>Bayesian View In the Bayesian framework, ICL is explained as implicit Bayesian inference, where models perform ICL by identifying a shared latent concept among examples <ref type="bibr" target="#b138">(Xie et al., 2022;</ref><ref type="bibr" target="#b134">Wies et al., 2023;</ref><ref type="bibr" target="#b4">Ahuja et al., 2023;</ref><ref type="bibr" target="#b50">Jiang, 2023;</ref><ref type="bibr">Wang et al., 2023e)</ref>. Additional perspectives suggest that LLMs encode the Bayesian Model Averaging algorithm via the attention mechanism <ref type="bibr">(Zhang et al., 2023b)</ref>. As the number of in-context examples increases, implicit Bayesian inference becomes analogous to kernel regression <ref type="bibr">(Han et al., 2023a)</ref>.</p><p>Gradient Descent View Gradient descent offers another valuable lens for understanding ICL. <ref type="bibr">Dai et al. (2023a)</ref> identified a dual form between Transformer attention and gradient descent, finding that GPT-based ICL behaves similarly to explicit finetuning from multiple perspectives. Other studies have attempted to establish connections between ICL and gradient descent in simplified regression settings <ref type="bibr" target="#b115">(von Oswald et al., 2023;</ref><ref type="bibr" target="#b3">Ahn et al., 2023;</ref><ref type="bibr" target="#b73">Mahankali et al., 2023;</ref><ref type="bibr">Li et al., 2023c)</ref>. For instance, von Oswald et al. <ref type="bibr">(2023)</ref> showed that linear attention-only Transformers with manually constructed parameters are closely related to models learned by gradient descent. <ref type="bibr">Li et al. (2023c)</ref> found that self-attention-only Transformers exhibit similarities with models trained via gradient descent. However, the simplified settings used in these studies have led to debates about the direct applicability of these connections in real-world contexts <ref type="bibr" target="#b93">(Shen et al., 2024)</ref>. <ref type="bibr" target="#b32">Fu et al. (2023)</ref> argued that Transformers perform ICL on linear regression using higher-order optimization techniques rather than gradient descent.</p><p>Other Views Beyond connecting ICL with a single algorithm, researchers have analyzed it from various perspectives, including ability decoupling, algorithmic learning, and information theory. <ref type="bibr">Pan et al. (2023b)</ref> decoupled ICL capabilities into task recognition ability and task learning ability, each manifesting under different conditions. Another typical theory abstracts ICL as an algorithmic learning problem <ref type="bibr" target="#b6">(Akyürek et al., 2023;</ref><ref type="bibr" target="#b34">Garg et al., 2022;</ref><ref type="bibr">Li et al., 2023e;</ref><ref type="bibr">Bai et al., 2023b)</ref>, where Transformers dynamically select algorithms, such as gradient descent and ridge regression, tailored to different ICL instances. Moreover, <ref type="bibr" target="#b37">Hahn and Goyal (2023)</ref> utilized information theory to show an error bound for ICL under linguistically motivated assumptions, explaining how next-token prediction can bring about the ICL ability.</p><p>These analytical studies have taken an essential step to explain ICL. However, most of them focused on simple tasks and small models. Extending analysis on extensive tasks and large models may be the next step to be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Application</head><p>Given its user-friendly interface and lightweight prompting method, ICL has broad applications on traditional NLP tasks <ref type="bibr">(Kim et al., 2022;</ref><ref type="bibr">Min et al., 2022b;</ref><ref type="bibr">Zhu et al., 2023b)</ref>. Particularly, by using demonstrations that explicitly guide the reasoning process, ICL manifests remarkable effects on tasks requiring complex reasoning <ref type="bibr">(Wei et al., 2022c;</ref><ref type="bibr">Li et al., 2023b;</ref><ref type="bibr" target="#b156">Zhou et al., 2022)</ref> and compositional generalization <ref type="bibr">(Zhou et al., 2023a)</ref>.</p><p>We explore several emerging and prevalent applications of ICL, including data engineering, model augmentation, and knowledge updating. 1) Data Engineering: Unlike traditional methods such as human annotation and noisy automatic annotation, ICL generates relatively high-quality data at a lower cost, leading to improved performance. <ref type="bibr">(Wang et al., 2021;</ref><ref type="bibr" target="#b51">Khorashadizadeh et al., 2023;</ref><ref type="bibr" target="#b29">Ding et al., 2023)</ref>. 2) Model Augmentation:</p><p>The context-flexible nature of ICL shows promise in model augmentation. It can enhance retrievalaugmented methods by prepending grounding documents to the input <ref type="bibr">(Ram et al., 2023)</ref>. Additionally, ICL for retrieval demonstrates potential in steering models toward safer outputs <ref type="bibr" target="#b85">(Panda et al., 2023;</ref><ref type="bibr" target="#b75">Meade et al., 2023)</ref>. 3) Knowledge Updating: LLMs often contain outdated or incorrect knowledge <ref type="bibr" target="#b31">(Dong et al., 2023)</ref>. ICL has demonstrated efficacy in revising such knowledge through carefully crafted demonstrations, yielding higher success rates compared to gradient-based methods <ref type="bibr" target="#b28">(De Cao et al., 2021)</ref>.</p><p>As mentioned above, ICL has yielded significant benefits on both traditional and emergent NLP ap-plications. The tremendous success of ICL in NLP has inspired researchers to explore its potential in various modalities beyond text (elaborated in Appendix D), including vision <ref type="bibr" target="#b11">(Bar et al., 2022;</ref><ref type="bibr">Wang et al., 2023c)</ref>, vision-language <ref type="bibr" target="#b113">(Tsimpoukelli et al., 2021;</ref><ref type="bibr" target="#b7">Alayrac et al., 2022)</ref>, as well as speech applications <ref type="bibr">(Wang et al., 2023a;</ref><ref type="bibr">Zhang et al., 2023d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Challenges and Future Directions</head><p>In this section, we review existing challenges and discuss future directions for ICL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficiency and Scalability</head><p>The use of demonstrations in ICL introduces two challenges: (1) higher computational costs with an increasing number of demonstrations (efficiency), and (2) fewer learnable samples due to the maximum input length of LLMs (scalability). Prior research has attempted to mitigate these issues by distilling lengthy demonstrations into compact vectors <ref type="bibr">(Li et al., 2024d,c)</ref> or expediting LLM inference times <ref type="bibr">(Liu et al., 2023d)</ref>. However, these methods often involve a trade-off in performance or necessitate access to model parameters, which is impractical for closed-source models like ChatGPT and Claude <ref type="bibr">(Zhou et al., 2023b)</ref>. Thus, enhancing the scalability and efficiency of ICL with more demonstrations remains a significant challenge.</p><p>Generalization ICL heavily relies on highquality demonstrations selected from annotated examples, which are often scarce in low-resource languages and tasks. This scarcity poses a challenge to the generalization ability of ICL <ref type="bibr" target="#b43">(He et al., 2024)</ref>. Given that there is a substantial discrepancy in the availability of annotated high-resource data and low-resource data, the potential to leverage high-resource data to address low-resource tasks is highly appealing <ref type="bibr" target="#b20">(Chatterjee et al., 2024;</ref><ref type="bibr" target="#b109">Tanwar et al., 2023)</ref>.</p><p>Long-context ICL Recent advances in contextextended LLMs have spurred research into the impact of ICL when using an increasing number of demonstration examples <ref type="bibr" target="#b2">(Agarwal et al., 2024;</ref><ref type="bibr" target="#b13">Bertsch et al., 2024)</ref>. However, researchers have found that increasing the number of demonstrations does not necessarily enhance performance and may even be detrimental. These performance declines indicate a need for further investigation. Additionally, <ref type="bibr">Li et al. (2024b)</ref> developed LongICLBench, which includes diverse extreme-label classification tasks, revealing further weaknesses of LLMs in comprehending extended demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we comprehensively review the existing literature on ICL, examining advanced techniques, conducting analytical studies, discussing relevant applications, and identifying critical challenges and potential directions for future research. To our knowledge, this is the first comprehensive survey dedicated to ICL. We aim to highlight the current state of research in ICL and provide insights to guide future work in this promising area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>This paper offers a comprehensive examination and summary of current methodologies and analyses in the area of In-Context Learning (ICL). However, given the extensive body of related work, particularly in demonstration design and the principle analysis of ICL, we may have overlooked some equally valuable contributions. Additionally, we outline several future directions for research in ICL, including long-context ICL, efficiency and scalability in ICL, etc. We plan to leave these aspects for future work. Furthermore, many papers covered by this survey did not utilize the most up-to-date models while running experiments. We advocate for more thorough and up-to-date research to provide actionable insights for practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Takeaway</head><p>Through a comprehensive literature review of ICL, we have discovered takeaways across several domains. These include training, demonstration design, scoring functions, analysis, and ICL applications that go beyond text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training</head><p>To further enhanced ICL capabilities, methods propose to train the LLMs in the stage of pre-training and warmup before ICL inference.</p><p>3 Takeaway: (1) The key idea of training before inference is to bridge the gap between pretraining and downstream ICL formats by introducing objectives close to in-context learning. Warmup is optional for ICL as many pretrained LLMs have manifested the ICL ability. ( <ref type="formula" target="#formula_1">2</ref>) Compared to incontext finetuning involving demonstration, instruction finetuning without a few examples as demonstration is simpler and more popular. All these warmup methods improve the ICL capability by updating the model parameters, which implies that the ICL capability of the original LLMs has great potential for improvement. Therefore, although ICL does not strictly require model warmup, we recommend adding a warmup stage before ICL inference. (3) The performance advancement made by warmup encounters a plateau when increasingly scaling up the training data, indicating that LLMs only need a small amount of data to adapt to learn from the context during warmup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Demonstration Organization</head><p>The performance of ICL strongly relies on the demonstration surface, including the selection, formatting, and ordering of demonstration examples.</p><p>3 Takeaway: (1) Demonstration selection strategies improve the ICL performance, but most of them are instance level. Since ICL is mainly evaluated under few-shot settings, the corpus-level selection strategy is more important yet underexplored. (2) The output score or probability distribution of LLMs plays an important role in instance selecting. (3) For k demonstrations, the size of search space of permutations is k!. How to find the best orders efficiently or how to approximate the optimal ranking better is also a challenging question. (4) Adding chain-of-thoughts can effectively decompose complex reasoning tasks into intermediate reasoning steps. During inference, multi-stage demonstration designing strategies are applied to generate CoTs better. How to improve the CoT prompting ability of LLMs is also worth exploring. (5) In addition to human-written demonstrations, the generative nature of LLMs can be utilized in demonstration designing. LLMs can generate instructions, demonstrations, probing sets, chainof-thoughts, and so on. By using LLM-generated demonstrations, ICL can largely get rid of human efforts on writing templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Scoring Function</head><p>The scoring function determines how to transform the predictions of a language model into an estimation of the likelihood of a specific answer. The answer with the highest probability is selected as the final answer.</p><p>3 Takeaway: (1) Although directly adopting the conditional probability of candidate answers is efficient, this method still poses some restrictions on the template design. Perplexity is also a simple and widely scoring function. This method has universal applications, including both classification tasks and generation tasks. However, both methods are still sensitive to demonstration surface, while Channel is a remedy that especially works under imbalanced data regimes. (2) Existing scoring functions all compute a score straightforwardly from the conditional probability of LLMs. There is limited research on calibrating the bias or mitigating the sensitivity via scoring strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Analysis</head><p>Numerous analytical studies investigate influencing factors of ICL during both the pretraining and inference stages, and attempt to figure out the learning mechanisms of ICL from the perspective of functional modules and theoretical interpretation.</p><p>3 Takeaway: (1) Knowing and considering why ICL works and what factors may influence can help us improve the ICL performance. (2) Although some analytical studies have taken a preliminary step to explain ICL, most of them are limited to simple tasks and small models. Extending analysis on extensive tasks and large models may be the next step to be considered. (3) Among existing work, explaining ICL with gradient descent seems to be a reasonable, general, and promising direction for future research. If we build clear connections between ICL and gradient-descent-based learning, we can borrow ideas from the history of traditional deep learning to improve ICL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 In-context Learning Beyond Text</head><p>The tremendous success of ICL in NLP has inspired researchers to explore in-context learning in different modalities beyond natural language with promising results.</p><p>3 Takeaway: (1) Properly formatted data (e.g., interleaved image-text datasets for vision-language tasks) and architecture designs are key factors for activating the potential of in-context learning. Exploring it in a more complex structured space such as for graph data is challenging and promising <ref type="bibr">(Huang et al., 2023a)</ref>. ( <ref type="formula" target="#formula_1">2</ref>) Findings in textual in-context learning demonstration design and selection cannot be trivially transferred to other modalities. Domain-specific investigation is required to fully leverage the potential of in-context learning in various modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Detail</head><p>In the experiment, we utilize 8 demonstrations and test on gpt2 <ref type="bibr" target="#b87">(Radford et al., 2019)</ref>, gptj <ref type="bibr" target="#b117">(Wang and Komatsuzaki, 2021)</ref>, LLaMA3-8B-Instruct(AI@Meta, 2024) and Qwen2-7B-Instruct <ref type="bibr">(Bai et al., 2023a)</ref>. All experiments are executed on a single NVIDIA A100 (80G). For datasets we choose sst2 <ref type="bibr">(Socher et al., 2013a)</ref>, sst5 <ref type="bibr">(Socher et al., 2013b</ref><ref type="bibr">), commonsense_qa (Talmor et al., 2019)</ref>, ag_news <ref type="bibr" target="#b147">(Zhang et al., 2015)</ref> and snli <ref type="bibr" target="#b16">(Bowman et al., 2015)</ref>. For the last two datasets, we only select 1000 data from the training set for retrieval and the first 1000 data from the test set for testing. During the inference phase, a PPL-based approach is employed. The entire code framework is built upon OpenICL <ref type="bibr">(Wu et al., 2023a)</ref>, for which we extend our gratitude to the authors.</p><p>Table <ref type="table" target="#tab_6">4</ref> and Table <ref type="table">5</ref> show the quantitative results on the efficiency and stability metrics for different scoring functions in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Evaluation and Resources</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Traditional Tasks</head><p>As a general learning paradigm, ICL can be examined on various traditional datasets and benchmarks, e.g., SuperGLUE <ref type="bibr">(Wang et al., 2019)</ref>, SQuAD <ref type="bibr" target="#b88">(Rajpurkar et al., 2018)</ref>. Implementing ICL with 32 randomly sampled examples on <ref type="bibr">Su-perGLUE, Brown et al. (2020)</ref> found that GPT-3 can achieve results comparable to state-of-theart (SOTA) finetuning performance on COPA and ReCoRD, but still falls behind finetuning on most</p><p>Model Direct PPL Channel GPT2 44.13(1.00) 114.02(2.58) 157.70(3.57) GPT-J 611.04(1.00) 1766.82(2.89) 1793.27(2.93) Qwen2 745.89(1.00) 1886.63(2.53) 1957.97(2.63) Llama3 790.46(1.00) 1935.04(2.45) 1956.21(2.47) AVG 1.00 2.61 2.90 Table 5: The qualitative results of the Stability metric in Table 3 which reflect whether the in-context learning ability is easily affected by changes in demonstration examples. We conducted experiments using a test set of size 10k and set up 5 different random seeds. Each time, 8 examples were randomly selected from 5k training examples for the experiments. The table records the variance of performance.</p><p>NLU tasks. <ref type="bibr">Hao et al. (2022b)</ref> showed the potential of scaling up the number of demonstration examples. However, the improvement brought by scaling is very limited. At present, compared to finetuning, there still remains some room for ICL to reach on traditional NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 New Challenging Tasks</head><p>In the era of large language models with in-context learning capabilities, researchers are more interested in evaluating the intrinsic capabilities of large language models without downstream task finetuning <ref type="bibr">(Bommasani et al., 2021)</ref>.</p><p>To explore the capability limitations of LLM on various tasks, <ref type="bibr" target="#b101">Srivastava et al. (2022)</ref> proposed the BIG-Bench <ref type="bibr" target="#b101">(Srivastava et al., 2022)</ref>, a large benchmark covering a large range of tasks, including linguistics, chemistry, biology, social behav-  <ref type="bibr" target="#b114">(Valmeekam et al., 2022)</ref>.</p><p>ior, and beyond. The best models have already outperformed the average reported human-rater results on 65% of the BIG-Bench tasks through ICL <ref type="bibr" target="#b105">(Suzgun et al., 2023)</ref>. To further explore tasks actually unsolvable by current language models, Suzgun et al. (2023) proposed a more challenging ICL benchmark, BIG-Bench Hard (BBH). BBH includes 23 unsolved tasks, constructed by selecting challenging tasks where the state-of-art model performances are far below the human performances. Besides, researchers are searching for inverse scaling tasks, 2 that is, tasks where model performance reduces when scaling up the model size. Such tasks also highlight potential issues with the current paradigm of ICL. To further probe the model generalization ability, Iyer et al. (2022) proposed OPT-IML Bench, consisting of 2000 NLP tasks from 8 existing benchmarks, especially benchmark for ICL on held-out categories. Specifically, a series of studies focus on exploring the reasoning ability of ICL. Saparov and He (2023) generated an example from a synthetic world model represented in first-order logic and parsed the ICL generations into symbolic proofs for formal analysis. They found that LLMs can make correct individual deduction steps via ICL. Shi et al. (2022) constructed the MGSM benchmark to evaluate the chain-of-thought reasoning abilities of LLMs in multilingual settings, finding that LLMs manifest complex reasoning across multiple languages. To further probe more sophisticated planning and reasoning abilities of LLMs, Valmeekam et al. (2022) provided multiple test cases for evaluating various reasoning abilities on actions and change, where existing ICL methods Visual prompt image Output Inpainting Model f x1 y1 xq vp Concatenate into single image x Edge detection Colorization Inpainting Segmentation Style transfer Task Input Example Task Output Example Query Visual prompt image Output Inpainting Model f x1 y1 xq vp Concatenate into single image x Edge detection Colorization Inpainting Segmentation Style transfer Task Input Example Task Output Example Query Visual prompt image Output Inpainting Model f x1 y1 xq vp Concatenate into single image x Edge detection Colorization Inpainting Segmentation Style transfer Task Input Example Task Output Example Query Visual prompt image Output Inpainting Model f x1 y1 xq vp Concatenate into single image x Edge detection Colorization Inpainting Segmentation Style transfer k Input ample Task Output Example Query Task Input Image Task Output Image Query Image Visual Prompt Grid Image Inpainting Model Visual prompt image Output Inpainting Model f x1 y1 xq vp Concatenate into single image x Edge detection Colorization Inpainting Segmentation Style transfer Task Input Example Task Output Example Query Task Text Prompt Task Input Image Task Output Image "Segment the horses from the rest of the image and generate a new image where the horse regions are white and the other regions are black." Query Image Diffusion Model Output Image Output Image Text Visual Prompt on LLMs show poor performance.</p><p>In addition, <ref type="bibr">Tang et al. (2023b)</ref> proposed a benchmark called SAMSum, which is a humanannotated dataset specifically designed for multiturn dialogue summarization, to evaluate the quality of dialogue summaries generated by LLMs via ICL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Open-source Tools</head><p>Noticing that ICL methods are often implemented differently and evaluated using different LLMs and tasks, <ref type="bibr">Wu et al. (2023a)</ref> developed OpenICL, an open-source toolkit enabling flexible and unified ICL assessment. With its adaptable architecture, OpenICL facilitates the combination of distinct components and offers state-of-the-art retrieval and inference techniques to accelerate the integration of ICL into advanced research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D In-Context Learning Beyond Text</head><p>The tremendous success of ICL in NLP has inspired researchers to explore its potential in different modalities, including visual, vision+language and speech tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Visual In-Context Learning</head><p>Employing masked auto-encoders (MAE) for image patch infilling, the model trained by <ref type="bibr" target="#b11">Bar et al. (2022)</ref> generates consistent output images at inference, demonstrating robust ICL capabilities for tasks like image segmentation. This method is expanded in Painter <ref type="bibr">(Wang et al., 2023c)</ref>, which incorporates multiple tasks to develop a generalist model with competitive performance. SegGPT <ref type="bibr">(Wang et al., 2023d)</ref> further builds on this by integrating diverse segmentation tasks and exploring ensemble techniques to enhance example quality. Additionally, <ref type="bibr">Wang et al. (2023g)</ref> introduce the Prompt Diffusion model, the first diffusion-based model with ICL abilities, guided by an extra text prompt for more precise image generation, as illustrated in Figure <ref type="figure" target="#fig_4">5</ref>.</p><p>Similar to ICL in NLP, the effectiveness of visual in-context learning greatly depends on the choice of demonstration images, as shown in research by <ref type="bibr">(Zhang et al., 2023a)</ref> and <ref type="bibr" target="#b104">(Sun et al., 2023)</ref>. To optimize this, <ref type="bibr">Zhang et al. (2023a)</ref> examine two strategies: using an unsupervised retriever to select the nearest samples with an existing model, and a supervised approach to train a specialized retriever to boost ICL performance. These approaches improve results by ensuring semantic similarity and better alignment in viewpoint, background, and appearance. Beyond retrieval, <ref type="bibr" target="#b104">Sun et al. (2023)</ref> also investigate a prompt fusion technique to further enhance outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Multi-Modal In-Context Learning</head><p>In the vision-language domain, a vision encoder paired with a frozen language model demonstrates multi-modal few-shot learning capabilities after training on image-caption datasets, as shown by the Frozen model <ref type="bibr" target="#b113">(Tsimpoukelli et al., 2021)</ref>. Extending this, Flamingo integrates a vision encoder with large language models (LLMs) for enhanced incontext learning across multi-modal tasks, leveraging large-scale web corpora <ref type="bibr" target="#b7">(Alayrac et al., 2022)</ref>. Similarly, Kosmos-1 exhibits zero-shot, few-shot, and multi-modal chain-of-thought prompting abilities <ref type="bibr">(Huang et al., 2023b)</ref>. METALM introduces a semi-causal language modeling objective to achieve strong ICL performance across visionlanguage tasks <ref type="bibr">(Hao et al., 2022a)</ref>. The ICL-D3IE approach employs a novel in-context learning framework that iteratively updates diverse demonstrations-including hard, layout-aware, and formatting demonstrations to train large language models (LLMs) for enhanced document information extraction (DIE) <ref type="bibr" target="#b42">(He et al., 2023)</ref>. Recent advancements include creating instruction tuning datasets from existing vision-language tasks or with advanced LLMs like GPT-4, connecting LLMs with powerful vision foundational models like BLIP-2 for multi-modal learning <ref type="bibr">(Xu et al., 2023b;</ref><ref type="bibr">Li et al., 2023a;</ref><ref type="bibr">Liu et al., 2023a;</ref><ref type="bibr">Zhu et al., 2023a;</ref><ref type="bibr">Dai et al., 2023b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Speech In-Context Learning</head><p>In the speech area, <ref type="bibr">Wang et al. (2023a)</ref> treated textto-speech synthesis as a language modeling task.</p><p>They use audio codec codes as an intermediate representation and propose the first TTS framework with strong in-context learning capability. Subsequently, VALLE-X <ref type="bibr">(Zhang et al., 2023d)</ref> extend the idea to multi-lingual scenarios, demonstrating superior performance in zero-shot cross-lingual textto-speech synthesis and zero-shot speech-to-speech translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Author Contributions</head><p>Qingxiu Dong led the project. Qingxiu Dong, Lei Li, Damai Dai, and Ce Zheng discussed and wrote the initial draft of the paper and the second version update. Jingyuan Ma, Rui Li, Heming Xia, and Qingxiu Dong discussed and wrote the third version update, incorporating new work that emerged over the year. Tianyu Liu was responsible for the fourth version update, adding new relevant work and polishing the manuscript. Jingjing Xu, Zhiyong Wu, and Lei Li (CMU) contributed to the revisions and discussions of the first two versions of the paper. Baobao Chang, <ref type="bibr">Xu Sun, Lei Li (CMU)</ref>, and Zhifang Sui guided the research of Qingxiu Dong, Lei Li, Damai Dai, and Ce Zheng.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Comparison with other survey papers</head><p>Our survey was drafted and posted on the Arxiv at the end of 2022, which is, to the best of our knowledge, the very first to review in-context learning in the field. We also regularly update this survey in a timely manner, with four major revisions.</p><p>Starting from 2023, we notice the emerge of several related survey in the field of in-context learning. <ref type="bibr" target="#b140">Xu et al. (2024)</ref> made a comprehensive review on the choices for models, training procedures and inference algorithms to retrieve demonstrative examples of in-context learning. <ref type="bibr" target="#b63">Li (2023)</ref> provided practical suggestions on prompt engineering for incontext learning. <ref type="bibr">Zhou et al. (2023d)</ref> and <ref type="bibr" target="#b44">Highmore (2024)</ref> focused on the theoretical interpretation and analysis of ICL, which corresponds to Section 5 in this survey. All the above-mentioned survey papers differ with ours in terms of scope and topics. This survey focused on the general development of ICL, including the formal definition of ICL, training strategies, prompt designing strategies, analysis and applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of in-context learning. ICL requires a prompt context containing a few demonstration examples written in natural language templates. Taking this prompt and a query as the input, large language models are responsible for making predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Taxonomy of in-context learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of model training methods to enhance ICL capabilities through two different stages: pretraining and warmup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Summary of factors that have a relatively strong correlation to ICL performance and different perspectives to explain why ICL works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Image-only and textual augmented prompting for visual in-context learning.</figDesc><graphic coords="21,176.03,221.78,50.83,50.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Fair comparison of demonstration selection methods. CQA and News are abbreviations of Commonsense QA and AG News, respectively. The best results are bolded. Our experiments on topk</figDesc><table><row><cell cols="3">Model Method SST5 SST2 CQA SNLI News Avg</cell></row><row><cell></cell><cell cols="2">topk 40.1 74.9 30.2 39.7 62.7 49.5</cell></row><row><cell>GPT2</cell><cell cols="2">votek 32.4 51.0 29.8 35.8 25.5 34.9</cell></row><row><cell></cell><cell>mdl</cell><cell>43.3 86.7 32.7 41.4 68.0 54.4</cell></row><row><cell></cell><cell cols="2">topk 46.9 84.6 58.4 60.7 69.1 63.9</cell></row><row><cell>GPT-J</cell><cell cols="2">votek 33.8 87.3 63.4 43.1 25.3 50.6</cell></row><row><cell></cell><cell>mdl</cell><cell>37.6 87.9 64.1 59.8 68.2 63.5</cell></row><row><cell></cell><cell cols="2">topk 54.1 83.3 76.3 68.2 64.9 69.4</cell></row><row><cell>Qwen2</cell><cell cols="2">votek 55.3 86.9 76.1 51.6 65.3 67.0</cell></row><row><cell></cell><cell>mdl</cell><cell>54.6 86.1 77.1 65.0 63.2 69.2</cell></row><row><cell></cell><cell cols="2">topk 53.0 90.3 76.1 64.0 74.0 71.5</cell></row><row><cell>Llama3</cell><cell cols="2">votek 54.9 88.9 72.6 57.7 78.3 70.5</cell></row><row><cell></cell><cell>mdl</cell><cell>54.4 89.1 76.5 59.9 74.6 70.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>The qualitative results of the Efficiency metric in Table3which record the language model inference latency (including the time for scoring with different scoring functions, with input data containing 8 in-context examples). The unit is milliseconds (ms). Each cell's parentheses contain the ratio of the latency for the current column model using the current row scoring function to the latency using direct inference. The final calculated average is the average of these ratios.</figDesc><table><row><cell>Model</cell><cell>Direct</cell><cell>PPL</cell><cell>Channel</cell></row><row><cell>GPT2</cell><cell>1.12</cell><cell>0.85</cell><cell>3.18</cell></row><row><cell>GPT-J</cell><cell>1.00</cell><cell>0.77</cell><cell>4.06</cell></row><row><cell>Qwen2</cell><cell>0.72</cell><cell>0.70</cell><cell>2.43</cell></row><row><cell>Llama3</cell><cell>0.89</cell><cell>0.78</cell><cell>2.43</cell></row><row><cell>AVG</cell><cell>0.93</cell><cell>0.78</cell><cell>3.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>New challenging evaluation benchmarks for ICL. For short, we use LLMAS to represent LLM Assessment Suite</figDesc><table><row><cell>Benchmark</cell><cell>Tasks</cell><cell>#Tasks</cell></row><row><cell>BIG-Bench (Srivastava et al., 2022)</cell><cell>Mixed tasks</cell><cell>204</cell></row><row><cell>BBH (Suzgun et al., 2023)</cell><cell>Unsolved problems</cell><cell>23</cell></row><row><cell>PRONTOQA (Saparov and He, 2023)</cell><cell>Question answering</cell><cell>1</cell></row><row><cell>MGSM (Shi et al., 2022)</cell><cell>Math problems</cell><cell>1</cell></row><row><cell>LLMAS (Valmeekam et al., 2022)</cell><cell>Plan and reasoning tasks</cell><cell>8</cell></row><row><cell>OPT-IML Bench (Iyer et al., 2022)</cell><cell>Mixed tasks</cell><cell>2000</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Y could be class labels or a set of free-text phrases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/inverse-scaling/prize</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human design GPT-Neo-2.7B Unified Retrieval Demonstration Reformatting SG-ICL (Kim et al., 2022) LM generated GPT-J Auto Demonstration Generation AutoICL (Yang et al., 2023a) LM generated GPT-3.5-Turbo-0301 Reasoning Path Generation MSP (Yang et al., 2023b) Human design GPT series Adjusting Demonstration Weight ICV (Liu et al., 2024a) Human design Falcon-7b / Llama-7b Demonstration Embedding Demonstration Ordering GlobalE</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methods Demonstration Acquisition LLMs Features Demonstration Selection KATE</title>
		<imprint>
			<date type="published" when="2022">2022. 2022. 2022. 2023. 2023. 2023. 2022</date>
		</imprint>
	</monogr>
	<note>Human design GPT-3 KNN Selection MI Human design GPT-3 Mutual Information EPR Human design GPT-3.5 Iterative Selection AdaICL Human design GPT-{J Neo} Selective Demonstration UDR Human design GPT-{J, 3}/CodeX Score-based Retrieval IDS Human design GPT-{2, 3} Best Order Selection ICCL (Liu et al., 2024b) Human design</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Llama2/Mixtral/Qwen Ordering from Simple to Complex References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Many-shot incontext learning</title>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Rosias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankesh</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaheer</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azade</forename><surname>Nova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feryal</forename><surname>Behbahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.11018</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transformers learn to implement preconditioned gradient descent for in-context learning</title>
		<author>
			<persName><forename type="first">Kwangjun</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Daneshmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kabir</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhur</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2306.04891</idno>
		<idno>CoRR, abs/2306.04891</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>In-context learning through the bayesian prism</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Llama 3 model card</title>
		<author>
			<persName><surname>Ai@meta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<pubPlace>Meta</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What learning algorithm is in-context learning? investigations with linear models</title>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Akyürek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2023-05-01">2023. May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23716" to="23736" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How do in-context examples affect compositional generalization?</title>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Shengnan An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.618</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11027" to="11052" />
		</imprint>
	</monogr>
	<note>ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyuan</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengguang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Xiaohuan Zhou, and Tianhang Zhu. 2023a. Qwen technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2023b. Transformers as statisticians: Provable in-context learning with in-context algorithm selection</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual prompting via image inpainting</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Gandelsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25005" to="25017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A markovian decision process</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematics and mechanics</title>
		<imprint>
			<biblScope unit="page" from="679" to="684" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">In-context learning with long-context models: An in-depth exploration</title>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Bertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2405.00200</idno>
		<idno>CoRR, abs/2405.00200</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Birth of a transformer: A memory viewpoint</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Bietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Cabannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. 2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladri</forename><forename type="middle">S</forename><surname>Castellon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><forename type="middle">S</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Creel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dora</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moussa</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Doumbouya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Etchemendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Ethayarajh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><forename type="middle">E</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelby</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyusha</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fereshte</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><forename type="middle">Wei</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohith</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kuditipudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Levent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suvir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mirchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zanele</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Munyikwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avanika</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Hamed Nilforoshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giray</forename><surname>Nyarko</surname></persName>
		</author>
		<author>
			<persName><surname>Ogut</surname></persName>
		</author>
		<editor>Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R&apos;e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishna Parasuram Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei A. Zaharia, Michael Zhang</editor>
		<imprint>
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
	<note>Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process-ing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ICL markup: Structuring incontext learning using soft-token tags</title>
		<author>
			<persName><forename type="first">Marc-Etienne</forename><surname>Brunet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashton</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2312.07405</idno>
		<idno>CoRR, abs/2312.07405</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data distributional properties drive emergent in-context learning in transformers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaditya</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-11-28">2022. 2022. November 28 -December 9, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Language models can exploit cross-task in-context learning for datascarce novel tasks</title>
		<author>
			<persName><forename type="first">Anwoy</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Tanwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmoy</forename><surname>Chakraborty</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2405.10548</idno>
		<idno>CoRR, abs/2405.10548</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Grimoire is all you need for enhancing large language models</title>
		<author>
			<persName><forename type="first">Ding</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingchen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2401.03385</idno>
		<idno>CoRR, abs/2401.03385</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving in-context few-shot learning via self-supervised training</title>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3558" to="3573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><surname>Omernick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thanumalayan</forename><surname>Sankaranarayana Pillai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marie</forename><surname>Pellat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Erica</forename><surname>Moreira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Brennan</forename><surname>Saeta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><surname>Diaz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kathy</forename><surname>Meier-Hellstern</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fine-tune language models to approximate unbiased in-context learning</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiwun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2310.03331</idno>
		<idno>CoRR, abs/2310.03331</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Castro-Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasha</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Valter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">2023a. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers</title>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.FINDINGS-ACL.247</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">July 9-14, 2023</date>
			<biblScope unit="page" from="4005" to="4019" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Instructblip: Towards general-purpose visionlanguage models with instruction tuning</title>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huat</forename><surname>Tiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. 2023. 2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Editing factual knowledge in language models</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6491" to="6506" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Is GPT-3 a good data annotator?</title>
		<author>
			<persName><forename type="first">Bosheng</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.626</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11173" to="11195" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2023 Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">CausalLM is not optimal for in-context learning</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomer</forename><surname>Levinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistical knowledge assessment for large language models</title>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="29812" to="29830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Transformers learn higher-order optimization methods for in-context learning: A study with linear models</title>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian-Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vatsal</forename><surname>Sharan</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2310.17086</idno>
		<idno>CoRR, abs/2310.17086</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Incontext learning for attention scheme: from single softmax regression to multiple softmax regression via a tensor trick</title>
		<author>
			<persName><forename type="first">Yeqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2307.02419</idno>
		<idno>CoRR, abs/2307.02419</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What can transformers learn incontext? A case study of simple function classes</title>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-11-28">2022. 2022. 2022. November 28 -December 9, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Demystifying prompts in language models via perplexity estimation</title>
		<author>
			<persName><forename type="first">Srini</forename><surname>Hila Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terra</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Blevins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.FINDINGS-EMNLP.679</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-12-06">2023. December 6-10, 2023</date>
			<biblScope unit="page" from="10136" to="10148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pre-training to learn in context</title>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.267</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4849" to="4870" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A theory of emergent in-context learning as implicit structure induction</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.07971</idno>
		<idno>CoRR, abs/2303.07971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">2023a. Explaining emergent in-context learning as kernel regression</title>
		<author>
			<persName><forename type="first">Chi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.12766</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">2023b. Understanding in-context learning via supportive pretraining data</title>
		<author>
			<persName><forename type="first">Xiaochuang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.708</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12660" to="12673" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.06336</idno>
		<title level="m">Language models are general-purpose interfaces</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">2022b. Structured prompting: Scaling in-context learning to 1,000 examples</title>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2212.06713</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ICL-D3IE: in-context learning with diverse demonstrations updating for document information extraction</title>
		<author>
			<persName><forename type="first">Jiabang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV51070.2023.01785</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2023</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-10-01">2023. October 1-6, 2023</date>
			<biblScope unit="page" from="19428" to="19437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Self-demos: Eliciting out-of-demonstration generalizability in large language models</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2404.00884</idno>
		<idno>CoRR, abs/2404.00884</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">In-context learning in large language models: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Clyde</forename><surname>Highmore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Instruction induction: From few examples to natural language task descriptions</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Honovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1935" to="1952" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PRODIGY: enabling in-context learning over graphs</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Krzmanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans</title>
		<meeting><address><addrLine>LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Language is not all you need: Aligning perception with language models</title>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengchao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owais</forename><surname>Khan Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barun</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Johan Bertil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Bjorck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhojit</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Som</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. 2023. 2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention</title>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Róbert</forename><surname>Csordás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07">2022. July 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="9639" to="9659" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Opt-iml: Scaling language model instruction meta learning through the lens of generalization</title>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">A latent space theory for emergent abilities in large language models</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2304.09960</idno>
		<idno>CoRR, abs/2304.09960</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Exploring in-context learning capabilities of foundation models for generating knowledge graphs from text</title>
		<author>
			<persName><forename type="first">Hanieh</forename><surname>Khorashadizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandana</forename><surname>Mihindukulasooriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanju</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghua</forename><surname>Groppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Groppe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.08804</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator</title>
		<author>
			<persName><forename type="first">Joon</forename><surname>Hyuhng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyeob</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/2206.08082</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">In-context learning in large language models learns label relationships but is not conventional learning</title>
		<author>
			<persName><forename type="first">Jannik</forename><surname>Kossen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2307.12375</idno>
		<idno>CoRR, abs/2307.12375</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03726</idno>
		<title level="m">A multi-modal model with in-context instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">2023b. Towards enhancing in-context learning for code generation</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongmin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17780</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">2024a. Feature-adaptive and data-scalable in-context learning</title>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.10738</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The closeness of in-context learning and weight shifting for softmax regression</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.13276</idno>
		<idno>CoRR, abs/2304.13276</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Quy Duc Do, Xiang Yue, and Wenhu Chen. 2024b. Long-context llms struggle with long in-context learning</title>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv, abs/2404.02060</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Xiaoling Wang, and Xipeng Qiu. 2023d. Unified demonstration retriever for incontext learning</title>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.256</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023">July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4644" to="4668" />
		</imprint>
	</monogr>
	<note>ACL 2023 Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Finding supporting examples for in-context learning</title>
		<author>
			<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2302.13539</idno>
		<idno>CoRR, abs/2302.13539</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">2024c. MEND: meta demonstration distillation for efficient and effective in-context learning</title>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyumin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlei</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2403.06914</idno>
		<idno>CoRR, abs/2403.06914</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">2023e. Transformers as algorithms: Generalization and stability in in-context learning</title>
		<author>
			<persName><forename type="first">Yingcong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammed</forename><surname>Emrullah Ildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">July 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="19565" to="19594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">A practical survey on zero-shot prompt design for in-context learning</title>
		<author>
			<persName><forename type="first">Yinheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.13205</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Zhuowei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.14660</idno>
		<title level="m">2024d. Implicit in-context learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08485</idno>
		<title level="m">Visual instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2022.DEELIO-1.10</idno>
	</analytic>
	<monogr>
		<title level="m">DeeLIO@ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland and Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-27">2022. May 27, 2022</date>
			<biblScope unit="page" from="100" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">2023b. Lost in the middle: How language models use long contexts</title>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2307.03172</idno>
		<idno>CoRR, abs/2307.03172</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">2023c. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1145/3560815</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">2024a. In-context vectors: Making in context learning more effective and controllable through latent space steering</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.06668</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">2024b. Let&apos;s learn step by step: Enhancing in-context learning ability with curriculum learning</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qikai</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.10738</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deja vu: Contextual sparsity for efficient llms at inference time</title>
		<author>
			<persName><forename type="first">Zichang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binhang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07-29">2023. 23-29 July 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="22137" to="22176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2022.ACL-LONG.556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05-22">2022. May 22-27, 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8086" to="8098" />
		</imprint>
	</monogr>
	<note>ACL 2022</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Mahankali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2307.03576</idno>
		<idno>CoRR, abs/2307.03576</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Which examples to annotate for in-context learning? towards effective and efficient selection</title>
		<author>
			<persName><forename type="first">Costas</forename><surname>Mavromatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huzefa</forename><surname>Rangwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2310.20046</idno>
		<idno>CoRR, abs/2310.20046</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Using in-context learning to improve dialogue safety</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Meade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.FINDINGS-EMNLP.796</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-12-06">2023. December 6-10, 2023</date>
			<biblScope unit="page" from="11882" to="11910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">2022a. Noisy channel language model prompting for few-shot text classification</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="5316" to="5330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">MetaICL: Learning to learn in context</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2791" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Rethinking the role of demonstrations: What makes in-context learning work?</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinxi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2022.EMNLP-MAIN.759</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12-07">2022. December 7-11, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="11048" to="11064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Cross-task generalization via natural language crowdsourcing instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08773</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">In-context example selection with influences</title>
		<author>
			<persName><forename type="first">Tai</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11042</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2209.11895</idno>
		<idno>CoRR, abs/2209.11895</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">GPT-4 technical report</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2303.08774</idno>
		<idno>CoRR, abs/2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">2023a. What in-context learning &quot;learns&quot; in-context: Disentangling task recognition and task learning</title>
		<author>
			<persName><forename type="first">Jane</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">2023b. What in-context learning &quot;learns&quot; in-context: Disentangling task recognition and task learning</title>
		<author>
			<persName><forename type="first">Jane</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.FINDINGS-ACL.527</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">July 9-14, 2023</date>
			<biblScope unit="page" from="8298" to="8319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Differentially private in-context learning</title>
		<author>
			<persName><forename type="first">Ashwinee</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2305.01639</idno>
		<idno>CoRR, abs/2305.01639</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">In-context learning with iterative demonstration selection</title>
		<author>
			<persName><forename type="first">Chengwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Dagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2310.09881</idno>
		<idno>CoRR, abs/2310.09881</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>OpenAi</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/P18-2124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20, 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models</title>
		<author>
			<persName><forename type="first">Ori</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Dalmedigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dor</forename><surname>Muhlgay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2302.00083</idno>
		<idno>CoRR, abs/2302.00083</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Pretraining task diversity and the emergence of non-bayesian in-context learning for regression</title>
		<author>
			<persName><forename type="first">Allan</forename><surname>Raventós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansheej</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Learning to retrieve prompts for in-context learning</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2655" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Language models are greedy reasoners: A systematic formal analysis of chain-of-thought</title>
		<author>
			<persName><forename type="first">Abulhair</forename><surname>Saparov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2023-05-01">2023. May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Do pretrained transformers learn in-context by gradient descent?</title>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aayush</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08540</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Language models are multilingual chain-of-thought reasoners</title>
		<author>
			<persName><forename type="first">Freda</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Srivats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/2210.03057</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">In-context pretraining: Language modeling beyond document boundaries</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">On the effect of pretraining corpora on in-context learning by a large-scale language model</title>
		<author>
			<persName><forename type="first">Seongjin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwijeen</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoungseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boseop</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gichang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woomyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nako</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5168" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Measuring inductive biases of in-context learning with underspecified demonstrations</title>
		<author>
			<persName><forename type="first">Chenglei</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.632</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11289" to="11310" />
		</imprint>
	</monogr>
	<note>ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of SIG-DAT</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-10">2013. 18-21 October 2013</date>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">An information-theoretic approach to prompt engineering without ground truth labels</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Rytting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexia</forename><surname>Delorey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Fulda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wingate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="819" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Adam R Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Garriga-Alonso</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2206.04615</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Selective annotation makes language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Hongjin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2023-05-01">2023. May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Xuanjing Huang, and Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service</title>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Qian</surname></persName>
		</author>
		<idno>abs/2201.03514</idno>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Exploring effective factors for improving visual in-context learning</title>
		<author>
			<persName><forename type="first">Yanpeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04748</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Challenging big-bench tasks and whether chain-of-thought can solve them</title>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.FINDINGS-ACL.824</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="page" from="13003" to="13051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">2023a. Large language models can be lazy learners: Analyze shortcuts in in-context learning</title>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longtao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.FINDINGS-ACL.284</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">July 9-14, 2023</date>
			<biblScope unit="page" from="4645" to="4657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">2023b. In-context learning of large language models for controlled dialogue summarization: A holistic benchmark and empirical analysis</title>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratish</forename><surname>Puduppully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.newsum-1.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th New Frontiers in Summarization Workshop</title>
		<meeting>the 4th New Frontiers in Summarization Workshop<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="56" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Multilingual llms are better cross-lingual in-context learners with alignment</title>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Tanwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanmoy</forename><surname>Chakraborty</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.346</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="page" from="6292" to="6307" />
		</imprint>
	</monogr>
	<note>ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Lamda: Language models for dialog applications</title>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renelito Delos</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><surname>Olson</surname></persName>
		</author>
		<idno>abs/2201.08239</idno>
		<editor>Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak</editor>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.13971</idno>
		<idno>CoRR, abs/2302.13971</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Canton-Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Molybog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2307.09288</idno>
		<editor>Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang</editor>
		<imprint>
			<publisher>Aurélien Rodriguez</publisher>
			<pubPlace>Robert Stojnic, Sergey Edunov</pubPlace>
		</imprint>
	</monogr>
	<note>and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021</title>
		<imprint>
			<date type="published" when="2021-12-06">2021. December 6-14, 2021</date>
			<biblScope unit="page" from="200" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Large language models still can&apos;t plan (a benchmark for llms on planning and reasoning about change)</title>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Valmeekam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Olmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Sreedharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subbarao</forename><surname>Kambhampati</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2206.10498</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Transformers learn in-context by gradient descent</title>
		<author>
			<persName><forename type="first">Eyvind</forename><surname>Johannes Von Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ettore</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Randazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><surname>Vladymyrov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">2023. July 2023</date>
			<biblScope unit="page" from="35151" to="35174" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="3261" to="3275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">2022a. Iteratively prompt pre-trained language models for chain of thought</title>
		<author>
			<persName><forename type="first">Boshi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2022.EMNLP-MAIN.174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">December 7-11, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="2714" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">2023a. Neural codec language models are zero-shot text to speech synthesizers</title>
		<author>
			<persName><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.02111</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Label words are anchors: An information flow perspective for understanding in-context learning</title>
		<author>
			<persName><forename type="first">Lean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.EMNLP-MAIN.609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-12-06">2023. December 6-10, 2023</date>
			<biblScope unit="page" from="9840" to="9855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Want to reduce labeling cost? GPT-3 can help</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2021.FINDINGS-EMNLP.354</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">2021. 16-20 November, 2021</date>
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Images speak in images: A generalist painter for in-context visual learning</title>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6830" to="6839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Seggpt: Towards segmenting everything in context</title>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV51070.2023.00110</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2023</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023-10-01">2023. October 1-6, 2023</date>
			<biblScope unit="page" from="1130" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11916</idno>
		<title level="m">Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Few-shot learning: A survey</title>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<idno>CoRR, abs/1904.05046</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Self-instruct: Aligning language models with self-generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.754</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13484" to="13508" />
		</imprint>
	</monogr>
	<note>ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Tanay Dixit, and Xudong Shen. 2022b. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atharva</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arut</forename><surname>Selvan Dhanasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjana</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Stap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Karamanolakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Haizhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishani</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirby</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krima</forename><surname>Kuznia</surname></persName>
		</author>
		<author>
			<persName><surname>Doshi</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2022.EMNLP-MAIN.340</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Kuntal</forename><surname>Kumar Pal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maitreya</forename><surname>Patel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mehrad</forename><surname>Moradshahi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mihir</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mirali</forename><surname>Purohit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neeraj</forename><surname>Varshney</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rohitha</forename><surname>Phani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pulkit</forename><surname>Kaza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravsehaj</forename><surname>Verma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rushang</forename><surname>Singh Puri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Savan</forename><surname>Karia</surname></persName>
		</editor>
		<editor>
			<persName><surname>Doshi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Keyur</forename><surname>Shailaja</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siddhartha</forename><surname>Sampat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sujan</forename><surname>Mishra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Reddy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sumanta</forename><surname>Patro</surname></persName>
		</editor>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">December 7-11, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="5085" to="5109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">2023g. In-context learning unlocked for diffusion models</title>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang (atlas)</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">2022a. Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</editor>
		<editor>
			<persName><surname>Le</surname></persName>
		</editor>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2022">April 25-29, 2022</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Emergent abilities of large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Chain-of-thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-11-28">2022. 2022. 2022. November 28 -December 9, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Symbol tuning improves in-context learning in language models</title>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">K</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.EMNLP-MAIN.61</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-12-06">2023. December 6-10, 2023</date>
			<biblScope unit="page" from="968" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Larger language models do in-context learning differently</title>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.03846</idno>
		<idno>CoRR, abs/2303.03846</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">The learnability of in-context learning</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-12-10">2023. December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Learning and reasoning by analogy</title>
		<author>
			<persName><surname>Patrick H Winston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="689" to="703" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">2023a. Openicl: An open-source framework for in-context learning</title>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.02913</idno>
		<idno>CoRR, abs/2303.02913</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Self-adaptive in-context learning: An information compression perspective for incontext example selection and ordering</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.79</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1423" to="1436" />
		</imprint>
	</monogr>
	<note>ACL 2023 Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">An explanation of in-context learning as implicit bayesian inference</title>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2022-04-25">2022. April 25-29, 2022</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Qiaoqiao She, and Yongdong Zhang. 2023a. k nn prompting: Learning beyond the context with nearest neighbor inference</title>
		<author>
			<persName><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Kazemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.11624</idno>
		<title level="m">-context learning with retrieved demonstrations for language models: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning</title>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.ACL-LONG.641</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023-07-09">2023. July 9-14, 2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11445" to="11465" />
		</imprint>
	</monogr>
	<note>Long Papers), ACL 2023</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main">Pretraining data mixtures enable narrow model selection capabilities in transformer models</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Yadlowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyric</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Tripuraneni</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2311.00871</idno>
		<idno>CoRR, abs/2311.00871</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">2023a. Auto-icl: In-context learning without human supervision</title>
		<author>
			<persName><forename type="first">Jinghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2311.09263</idno>
		<idno>CoRR, abs/2311.09263</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">2023b. Not all demonstration examples are equally beneficial: Reweighting demonstration examples for in-context learning</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2023.FINDINGS-EMNLP.880</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">December 6-10, 2023</date>
			<biblScope unit="page" from="13209" to="13221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Compositional exemplars for in-context learning</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2023</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-07">2023. July 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="39818" to="39833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Ground-truth labels matter: A deeper look into input-label demonstrations</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyeob</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename><surname>Hyuhng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwiyeol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Goo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2022.EMNLP-MAIN.155</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12-07">2022. December 7-11, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="2422" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Active example selection for in-context learning</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2022.EMNLP-MAIN.622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12-07">2022. December 7-11, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="9134" to="9148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Active example selection for in-context learning</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/2022.EMNLP-MAIN.622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12-07">2022. December 7-11, 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="9134" to="9148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">2023a. What makes good examples for visual in-context learning?</title>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">December 10 -16, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<monogr>
		<title level="m" type="main">2023b. What and how does incontext learning learn? bayesian model averaging, parameterization, and generalization</title>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengzhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoran</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2305.19420</idno>
		<idno>CoRR, abs/2305.19420</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Automatic chain of thought prompting in large language models</title>
		<author>
			<persName><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023-05-01">2023. May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03926</idno>
		<title level="m">Speak foreign languages with your own voice: Cross-lingual neural codec language modeling</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
	<note>Proc. of ICML</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">2023a. Least-to-most prompting enables complex reasoning in large language models</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2023">May 1-5, 2023</date>
			<biblScope unit="volume">2023</biblScope>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Teaching algorithmic reasoning via in-context learning</title>
		<author>
			<persName><forename type="first">Hattie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azade</forename><surname>Nova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.09066</idno>
		<idno>CoRR, abs/2211.09066</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">Ryan Cotterell, and Mrinmaya Sachan. 2023b. Efficient prompting via dynamic in-context learning</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eleanor</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2305.11170</idno>
		<idno>CoRR, abs/2305.11170</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Large language models are human-level prompt engineers</title>
		<author>
			<persName><forename type="first">Yongchao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">Ioan</forename><surname>Muresanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keiran</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Pitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, ICLR 2023</title>
		<meeting><address><addrLine>Kigali, Rwanda</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2023-05-01">2023. May 1-5, 2023</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzheng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.00237</idno>
		<title level="m">The mystery and fascination of llms: A comprehensive survey on the interpretation and analysis of emergent abilities</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b160">
	<monogr>
		<title level="m" type="main">2023a. Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">2023b. Multilingual machine translation with large language models: Empirical results and analysis</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxiu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04675</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
