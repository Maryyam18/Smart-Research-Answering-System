<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Can Natural Language Processing Do for Peer Review?</title>
				<funder>
					<orgName type="full">CIFAR Canada AI Research Chair (Alberta Machine Intelligence Institute</orgName>
				</funder>
				<funder>
					<orgName type="full">National Research Center for Applied Cybersecurity ATHENE</orgName>
				</funder>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
				<funder>
					<orgName type="full">Jai Gupta chair fellowship</orgName>
				</funder>
				<funder>
					<orgName type="full">German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science</orgName>
				</funder>
				<funder>
					<orgName type="full">IBM-IITD AI Horizons network</orgName>
				</funder>
				<funder ref="#_GGYHNVy">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">States</orgName>
				</funder>
				<funder ref="#_NbgNfvw">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder>
					<orgName type="full">Excellence Strategy of the German Federal Government</orgName>
				</funder>
				<funder ref="#_SwN3P2V">
					<orgName type="full">NSERC</orgName>
				</funder>
				<funder ref="#_yp8EDkX">
					<orgName type="full">German Research Foundation (DFG)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-10">10 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ilia</forename><surname>Kuznetsov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">UKP Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Osama</forename><forename type="middle">Mohammed</forename><surname>Afzal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Koen</forename><surname>Dercksen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Radboud University Nijmegen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nils</forename><surname>Dycke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">UKP Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Hope</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<addrLine>6 -Bocconi University</addrLine>
									<settlement>Milan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Data Science Group</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<addrLine>8 -Universität Hamburg</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">University of British Columbia</orgName>
								<orgName type="institution" key="instit2">Indian Institute of Technology</orgName>
								<address>
									<settlement>Delhi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">UKP Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Mausam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Margot</forename><surname>Mieskes</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">University of Applied Sciences</orgName>
								<address>
									<addrLine>12 -</addrLine>
									<settlement>Darmstadt</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélie</forename><surname>Névéol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Danish</forename><surname>Pruthi</surname></persName>
							<affiliation key="aff8">
								<orgName type="institution" key="instit1">Université Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LISN</orgName>
								<orgName type="institution" key="instit4">Indian Institute of Science</orgName>
								<address>
									<addrLine>14 -Monash University; 15</addrLine>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<addrLine>6 -Bocconi University</addrLine>
									<settlement>Milan</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingyan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nihar</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Technical University of Darmstadt</orgName>
								<orgName type="institution" key="instit2">UKP Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution" key="instit1">The Hebrew University of Jerusalem</orgName>
								<orgName type="institution" key="instit2">16 -University of Washington</orgName>
								<address>
									<addrLine>17 -</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">18 -Queen&apos;s University</orgName>
								<address>
									<addrLine>19 -</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">IT University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Can Natural Language Processing Do for Peer Review?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-10">10 May 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">E01D24D07AD720255A74EECBA3EC3575</idno>
					<idno type="arXiv">arXiv:2405.06563v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-28T12:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The number of scientific articles produced every year is growing rapidly. Providing quality control over them is crucial for scientists and, ultimately, for the public good. In modern science, this process is largely delegated to peer review-a distributed procedure in which each submission is evaluated by several independent experts in the field. Peer review is widely used, yet it is hard, time-consuming, and prone to error. Since the artifacts involved in peer review-manuscripts, reviews, discussions-are largely text-based, Natural Language Processing (NLP) has great potential to improve reviewing. As the emergence of large language models (LLMs) has enabled NLP assistance for many new tasks, the discussion on machine-assisted peer review is picking up the pace. Yet, where exactly is help needed, where can NLP help, and where should it stand aside? The goal of our paper is to provide a foundation for the future efforts in NLP for peer-reviewing assistance. We discuss peer review as a general process, exemplified particularly by reviewing at artificial intelligence (AI) conferences. We detail each step of the process from manuscript submission to camera-ready revision, and discuss the associated challenges and opportunities for NLP assistance, illustrated by existing work. We then turn to the big challenges in NLP for peer review as a whole, including data acquisition and licensing, operationalization and experimentation, and ethical issues. To help consolidate community efforts, we create a companion repository that aggregates key datasets pertaining to peer review (<ref type="url" target="https://github.com/OAfzal/nlp-for-peer-review">https://github.com/OAfzal/nlp-for-peer-review</ref>). Finally, we issue a detailed call for action for the scientific community, NLP and AI researchers, policymakers, and funding bodies to help bring the research in NLP for peer review forward. We hope that our work will help set the agenda for research in machine-assisted scientific quality control in the age of AI, within the NLP community and beyond.</p><p>This paper is an outcome of Dagstuhl seminar 24052 "Reviewer No. 2: Old and New Problems in Peer Review", <ref type="url" target="https://www.dagstuhl.de/24052">https://www.dagstuhl.de/24052</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One can argue that the two of this year's most discussed topics at Natural Language Processing and Artificial Intelligence conferences will be (1) the unprecedented expansiveness of potential applications of our rapidly advancing technology (in spite of many new and remaining open fundamental questions) and (2) the frustration with peer review. This white paper proposes bringing (1) and (2) together, by arguing that the progress in NLP creates a unique opportunity for the NLP research community to tackle the challenges associated with the scientific review process. How could NLP researchers focus their efforts to improve the effectiveness and efficiency of peer review, and what concrete steps can be taken to this end? We argue that peer review is inherently interesting to the NLP community as an application area, opening up new problems, revealing new variants of familiar ones, and perhaps inspiring fresh ideas about data, methods, and evaluation.</p><p>Peer review is a critical component of sound scientific discovery and, by extension, of the accountability to the public that is increasingly affected by science and its applications. The core peer review process was originally established in research communities of a few hundred people, relatively few of whom were in trainee career stages, and none of whom were subjected to the "fast science" pressure to disseminate findings early. Given the rapid growth experienced by AI research communities in particular <ref type="bibr" target="#b77">(Künzli et al., 2022)</ref>, and by science as a whole <ref type="bibr" target="#b79">(Landhuis, 2016)</ref>, existing processes do not scale well. This leads to increasing frustration with peer review. After months (or years) of hard work, authors expect informed, well-reasoned, unbiased evaluation of their results by qualified people in relatively short period of time. Yet reviewing is a complex and time-consuming task, and qualified reviewers are in short supply and overloaded with papers to review. Additional burden is placed on the organizers of the process, who need to deal with finding reviewers, assigning them to the papers, monitoring the process, and resolving conflicts-all of which requires increasing effort as the community and the number of submissions grow.</p><p>The scientific community widely recognizes the new challenges faced by peer review. With the emergence of large language models (LLMs), the topic of fully or partially automated peer review keeps re-emerging both within and outside NLP <ref type="bibr">(Liu and Shah, 2023;</ref><ref type="bibr" target="#b14">Biswas et al., 2023;</ref><ref type="bibr" target="#b90">Liang et al., 2023;</ref><ref type="bibr" target="#b119">Robertson, 2023;</ref><ref type="bibr" target="#b28">D'Arcy et al., 2024;</ref><ref type="bibr">Drori and Te'eni, 2024, and others)</ref>. Related work expresses both the hope that AI can help deal with the reviewing crisis in science, and the worries about the potentially grave consequences of automating this crucial stage in scientific work <ref type="bibr" target="#b129">(Schintler et al., 2023)</ref>. Our paper contributes to this debate with an informed discussion of the potential contributions of NLP to peer review. As we show, applying NLP to peer review meets a range of serious challenges that need to be addressed before any approaches to full automation of peer review can even be considered. Yet, peer review is not one isolated action and not a single NLP task. It is a complex process with dozens of tasks, which all require human effort, and are all prone to failure. While some of these tasks might be out of reach, others are closely related to well-known general NLP challenges such as reasoning, scientific content understanding, cross-document modeling and summarization, human and automatic evaluation, mitigating bias, and upholding ethics and privacy. Thus, while full automation might be not feasible, some individual well-defined problems within peer review can and should be addressed -with or without LLMs-and addressing them could make peer review more efficient and robust, save valuable researcher time, and increase satisfaction and trust in the peer review process. We believe that the NLP community plays the key role in addressing this challenge, due to our collective experience with fast community growth and our technical expertise with NLP technologies, their capabilities, and their limitations. Importantly, the social processes underlying peer review can open up new research directions and new connections to other fields of research such as game theory, sociology, psychology, meta-science, and human-computer interaction. Being at the core of the scientific process, the topic is inherently intriguing and extremely challenging, and has potential to draw new talent toward our community.</p><p>With this paper, we aim to map out the problem space of NLP for peer review assistance. We discuss peer review as a general process, using reviewing at AI conferences as a running example (Section 2). We then dive into each step of the process-from paper submission to publication-and outline core challenges and automation opportunities that emerge along the way (Sections 4-6), based on first-hand experience in reviewing and organizing scientific events, and on the available NLP technology. This is followed by a general discussion of experimental methods, data collection, and legal and ethical aspects of NLP for peer review (Sections 7-9), and a practical call for action (Section 10). The companion repository for this white paper aggregates datasets related to NLP for peer review and welcomes new contributions. We hope that our work will help foster community efforts in developing NLP technologies for machine-assisted scientific quality control that will benefit reviewers, authors, conference organizers, readers-and the scientific process as a whole.</p><p>an experienced reviewer will not have equally deep expertise in every research area. The combination of lacking qualification, hard task, and time pressure can lead to low-quality reviewing. This includes both generic, hastily written reviews, and the use of fast-reject heuristics like "not state of the art", "too niche", or "writing too bad" instead of a thorough evaluation (e.g., <ref type="url" target="https://aclrollingreview.org/reviewertutorial">https://aclrollingreview.org/  reviewertutorial</ref>). While there exist organizational measures to improve reviewing quality, such as reviewer mentoring <ref type="bibr" target="#b141">(Stelmakh et al., 2021d)</ref> and training courses<ref type="foot" target="#foot_0">foot_0</ref> , low-quality reviewing remains a persistent issue.</p><p>Strategic and dishonest behavior: Finally, peer review relies upon reviewer impartiality. Yet, by definition, reviewers are themselves researchers, and they often have their own work under review for the same venue. Given the competitive nature of modern research, reviewers have an incentive to abuse their role. This includes downscoring others' work to reduce potential competition, establishing collusion rings in which a group of reviewers conspire to pick each others' work for review and give it a favourable evaluation (Section 4.3.2), and other behaviors <ref type="bibr">(Shah, 2022, Section 4)</ref>.</p><p>Together, these factors can make peer review less successful at achieving what it is designed to do. A lot is at stake: passing peer review gives manuscripts the special status of a "peer-reviewed scientific publication", which often-for the better or worse-plays a large role in formal evaluations ("x peer-reviewed papers to get a PhD"), researchers' standing within the field ("over 100 papers in A* venues"), and perception of the work both by fellow researchers and by non-experts and media ("a peer-reviewed article has shown that..."). Mistakenly approved bad work can misguide follow-up research and misinform policymaking and public opinion. Mistakenly rejected good work, in turn, delays the dissemination of findings, causes unnecessary resubmission effort, and has a direct impact on researchers' careers. This motivates the ongoing search for policies, incentives, and tools that can help peer review stay robust to the realities of modern research. Our paper contributes to this important line of work by outlining the ways in which state-of-the-art NLP technology can support peer review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Walkthrough: Reviewing at AI Conferences</head><p>Science is diverse, and so is peer review. While distant research communities might have similar expectations and face similar challenges related to peer review, their specific practices and workflows can widely differ.</p><p>To structure the discussion throughout this paper, we focus on a particular use case most familiar to the primary audience of this paper: conference peer review in AI-related computer science communities. Example communities include NLP (e.g., ACL, NAACL, EMNLP), computer vision (e.g., CVPR, ICCV, ECCV), general AI (e.g., AAAI, IJCAI, UAI) and general machine learning (e.g., NeurIPS, ICML, ICLR). Although there are differences between specific conferences (and even between different editions of the same conference), the underlying peer-reviewing process is by and large similar. We now describe this process in detail, while introducing necessary terminology along the way. We note that our description is modular, and many challenges and solutions outlined in this paper will be applicable to a wider range of reviewing systems, e.g., journal review and open post-publication review. We also note that while particular artifacts and roles might be specific to AI conferences, most concepts involved are discipline-agnostic and can be easily adapted to other research communities and non-academic use cases.</p><p>Following the initial call for papers, the organizers of an AI conference initiate a reviewing campaign, illustrated in Figure <ref type="figure" target="#fig_1">1a</ref>. Reviewing is managed by a program committee which is headed by program chairs (PCs). They assemble the reviewer pool by recruiting reviewers-other members of the community who often submit manuscripts themselves. PCs are responsible for setting up the technical infrastructure for peer review, using conference management systems (CMS) like SoftConf (<ref type="url" target="https://softconf.com">https://softconf.com</ref>), Microsoft CMT (<ref type="url" target="https://cmt3.research.microsoft.com">https://cmt3.research.microsoft.com</ref>), and, increasingly, OpenReview (<ref type="url" target="https://openreview.net">https://openreview.net</ref>). In addition, PCs decide on the review forms and guidelines to be used in the campaign, including the scoring system, checklists, and submission rules. Program chairs are supported by meta-reviewers (sometimes also called area chairs), and-at larger conferences-by senior meta-reviewers (also called senior area chairs).</p><p>Submission Screening Matching Evaluation Review writing Discussion Meta-review Final Decision Revision &amp; Camera-Ready Before review During review After review (a) Key stages of peer review manuscript Review Review Review Rebuttal Rebuttal Rebuttal Discussion Meta-review revision amendment auxiliary data forms and guidelines A A A R A R R M metadata A (b) Artifacts Some conferences additionally enlist a dedicated ethics committee. Prior to the start of the reviewing process, the participants-authors, reviewers, and meta-reviewers-are often requested to fill out profiles that contain information about their affiliation, topics of expertise, prior publications, conflicts of interest, etc.</p><p>In the meantime, authors prepare their manuscripts (papers), which are submitted to the conference via the CMS at a due date along with metadata such as keywords, track and contribution type. AI conferences typically follow a double anonymized peer review model: by design, the authors and the reviewers do not know each others' identities.<ref type="foot" target="#foot_1">foot_1</ref> Upon submission, the manuscript goes through semi-automatic screening to ensure that it adheres to basic criteria like formatting, length limit, and anonymity. Failing this step might result in desk rejection without review. If a submission passes the formal checks, it is assigned to a set of reviewers (typically 3-5) from the reviewer pool. The assignment is done via semi-automatic matching, bidding, or a combination thereof. This step aims to maximize topical overlap and diversity of the reviewer set for each manuscript, while avoiding conflicts of interest.</p><p>Then, reviewing begins. The reviewers independently evaluate their assigned manuscripts and each of them writes a review report: a short semi-structured essay evaluating the work, often accompanied by a range of scores (e.g., overall score, confidence, soundness, novelty) and checklists (e.g., adherence to ethics guidelines or data availability). The reviewing workload varies, typically 3-6 submissions per reviewer. The evaluation is often followed by discussion, which consists of two parts. In the author-response phase (also known as rebuttal ) the authors see their papers' reviews and can respond to reviewers' questions and concerns. This is followed by the reviewers discussing the manuscript among themselves to arrive at the final version of their review reports. Once the reviewing is done, the review reports, author responses and discussions serve as input to the meta-reviewers who can additionally provide their own judgement of the work. Meta-reviewers may request further reviews in case of missing or low-quality reports, or if there are ethical concerns about the manuscript. Following that, meta-reviewers write a meta-review report-a brief text that summarizes reviewers' feedback, and provides their own evaluation of the work-typically 10-20 manuscripts per meta-reviewer.</p><p>At large conferences, based on the manuscript, its reviews and its meta-reviews, as well as any considerations of target acceptance rates, senior meta-reviewers then give a recommendation on whether or not to accept the paper to be published in the conference proceedings-around 20-50 manuscripts per senior meta-reviewer. Finally, all papers, meta-reviews and recommendations are passed to the program chairs-2-4 very senior members of the community responsible for the conference program. They make the final acceptance/rejection decisions for all manuscripts submitted to the conference. If a manuscript is accepted, the authors prepare a camera-ready revision of their submission that includes the necessary revisions and is later published. If the manuscript is rejected, the authors often resubmit their work to another venue, ideally incorporating reviewers' feedback into the new version of the manuscript. The revision is oftentimes accompanied by amendment notes that summarize the changes. Once the process is complete, conference organizers can analyze the rich data resulting from the reviewing campaign to inform future review organization and to provide insights to the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scope of this Paper</head><p>As we have discussed, peer review serves as a crucial quality control mechanism in modern science. It is a complex process that involves many independent actors. As science accelerates, peer review faces new challenges. Which of these challenges are solvable, and to what extent can AI help? We note that texts are central to the peer-reviewing process: submitted manuscripts largely consist of text, and so do the reviewing guidelines, organizers' communications, peer review reports, rebuttals, discussion comments, meta-reviews, and the resulting publications along with their amendment notes (Figure <ref type="figure" target="#fig_1">1b</ref>). To a large extent, reviewing work is text work, and some parts of it bear resemblance to annotation work <ref type="bibr" target="#b120">(Rogers and Augenstein, 2020)</ref>. Thus, NLP techniques could be leveraged to support peer review. As we shall see, many of the challenges in NLP for peer review are special cases of general NLP challenges familiar from other domains. Peer review provides both a new testing ground for approaches that emerge elsewhere, and can serve as a unique source of new tasks, data, and methods that can benefit other application areas. The goal of this paper is to help consolidate the efforts in this space and to provide an entry point for researchers and practitioners interested in using natural language processing to help peer review.</p><p>The rest of the paper is structured as follows. We start with a detailed discussion of NLP support for peer review, organized around three temporal stages of a peer-reviewing campaign (Figure <ref type="figure" target="#fig_1">1a</ref>). Before review, NLP can provide assistance with preparing the submission (Section 4.1), reviewer-paper scoring (Section 4.2) and matching (Section 4.3). During review, NLP can assist in evaluation (Section 5.1), review writing (Section 5.2) and during discussion (Section 5.3). After review, NLP can help perform meta-reviewing (Section 6.1), assist decision making by the PCs (Section 6.2), support manuscript revision (Section 6.3) and facilitate post-review analysis (Section 6.4). While we do not systematically survey all prior research in this space, we outline core application points and challenges in NLP for peer review and illustrate the discussion by existing works. We then turn to overarching challenges in NLP for peer review, including collecting and securing data (Section 7), measurement and experimental methodology (Section 8), and ethical issues that accompany peer-reviewing applications of NLP (Section 9). We conclude with an explicit call for action for the authors, reviewers, NLP and AI researchers, policymakers, and funding organizations (Section 10).</p><p>We note that while this work proposes many possible applications of NLP for peer review, each of them hinges on open research questions; while we do not argue that the proposed solutions are yet practical or even possible, we do believe that they are worthy of exploration. We also note that while our text is organized by stages of the review process, many of the discussed solutions afford multiple uses: for example, a reviewer can use screening tools (Section 4.1.3) at evaluation stage (Section 5.1) to check a manuscript for misconduct; a meta-reviewer (Section 6.1) can use review and discussion analysis tools (Section 5.3) to detect low-quality reviews; and any analytical tool can be used for aggregate statistics after a reviewing campaign is over (Section 6.4). Thus, even a single NLP assistance approach at a particular stage can benefit a large number of people and the process as a whole. Finally, while we discuss ethics-and data-related questions since they directly influence the NLP practice, our paper does not focus on policies, incentives, or other organizational measures to improve peer review, which constitute an important topic of their own.</p><p>4.1 Preparing the Submission 4.2 Scoring Potential Matches 4.3 Review-Paper Matching Writing assistance Metadata and presentation Screening Text similarity Keywords Bidding Conflicts of interest Strategic behavior Start Ready for review Aggregating scores Good reviewers Manual assignment support </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Assistance Before Review</head><p>The preparation phase is crucial for a successful reviewing campaign, and we start our overview by discussing the key steps that precede the actual reviewing: preparing the manuscript for submission, identifying potential reviewer candidates for the submission from the reviewer pool, and making the reviewing assignments. We note that this list is not exhaustive and much more needs to be done before reviewing begins-like recruiting the initial pool of reviewers or issuing a call for papers. Here we focus on the tasks internal to the reviewing process that we believe to be the most promising targets for NLP assistance (Figure <ref type="figure" target="#fig_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preparing the Submission</head><p>Efficient and robust assessment of scientific work starts with a clearly written and appropriately filed manuscript. Clarity issues can make manuscripts harder to read and evaluate, steering the precious resource of peer review away from the main scientific content of the submission. Even with stellar scientific content, a poorly composed paper is more likely to be rejected and later resubmitted <ref type="bibr" target="#b25">(Church, 2020)</ref>, incurring additional work for both authors and future reviewers. The goal of NLP assistance at this stage is to help make submissions easy to review, while reducing preparation effort for the authors. In addition, NLP can provide assistance during revision and resubmission of the manuscripts: this case is separately addressed in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Writing assistance</head><p>Tools to support academic writing are abundant, although currently there exist no ready-to-use solutions that can do it all <ref type="bibr" target="#b3">(Altmäe et al., 2023)</ref>. While an in-depth discussion of academic writing assistance is orthogonal to the topic of peer review and lies beyond our scope, we outline the general directions of writing support compatible with the current norms on use of generative AI in paper writing (e.g., <ref type="bibr" target="#b122">Rogers et al., 2023a)</ref>. Generally, technologies that introduce new intellectual content to the paper are not allowed, since they bear risk of plagiarizing ideas or parts of text from published sources. However, using NLP systems to improve writing and clarity without altering the intellectual contribution of the manuscript is allowed. This includes performing basic surface-level analysis, fixing typos, correcting grammar, flagging issues with verbosity, readability, or repetitions. Such functionality is already offered by many commercial tools such as Grammarly (<ref type="url" target="https://www.grammarly.com">https://www.grammarly.com</ref>) and WordTune (<ref type="url" target="https://www.wordtune.com">https://www.wordtune.com</ref>), as well as specialized academic writing support systems like Curie (<ref type="url" target="https://www.aje.com/curie">https://www.aje.com/curie</ref>), PaperPal (<ref type="url" target="https://paperpal.com">https://paperpal.com</ref>), and Writefull (<ref type="url" target="https://www.writefull.com">https://www.writefull.com</ref>). Many such systems also offer a paraphrasing functionality, which currently focuses on applying scientific writing style to the text rather than improving the structure and organization of a draft.</p><p>In the future, NLP systems could be used to detect common structural issues within technical papers, such as using a symbol or abbreviation before it is defined, or symbol overloading. They could suggest additional forward or backward references (e.g., to figures, definitions, and sections) that aid clarity. Further along the complexity spectrum, and closer to the area of scholarly document processing <ref type="bibr" target="#b21">(Chandrasekaran et al., 2020;</ref><ref type="bibr" target="#b6">Beltagy et al., 2021;</ref><ref type="bibr" target="#b26">Cohan et al., 2022)</ref>, NLP tools could recommend related papers that should be cited in a given manuscript <ref type="bibr" target="#b2">(Ali et al., 2020;</ref><ref type="bibr" target="#b43">Färber and Jatowt, 2020;</ref><ref type="bibr" target="#b102">Narimatsu et al., 2021)</ref>. Of course, these recommendations will need to be carefully vetted by the authors. An even more complex task is scientific claim verification <ref type="bibr" target="#b153">(Wadden et al., 2020</ref><ref type="bibr" target="#b154">(Wadden et al., , 2022))</ref>: support for checking the statements in the manuscript against provided or cited evidence. After the NLP tool identifies the unsupported statements, the authors would be suggested to either strengthen the evidence or weaken the claim. Such tools would help the authors and arguably result in stronger submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Metadata and presentation</head><p>Along with the manuscript, the authors are often required to provide submission metadata such as keywords, track, and contribution type. This metadata is important: keywords are often used in reviewer-paper matching (Section 4.2) and later in conference program navigation; the track sets the context both for the review and for the later presentation of the accepted manuscripts; and the contribution type helps reviewers with evaluation. Yet, deciding on some of this metadata can be challenging. For example, the ACL-2023 conference featured 26 distinct tracks, and AAAI-2024 used 14 primary and 282 secondary keywords as metadata. Often, a submission would not directly fit any of these tracks and keywords. While some venues allow the authors to add arbitrary keywords to their submissions, these need to be concise yet descriptive, and distilling a paper to a good set of keywords requires thought and knowledge of the field. NLP-assisted keyword generation <ref type="bibr" target="#b19">(Caragea et al., 2014;</ref><ref type="bibr" target="#b18">Çano and Bojar, 2020)</ref> and automatic track suggestion could make the submission process and the subsequent review more efficient.</p><p>Additionally, the submission stage could feature assistance for reformatting the submission for alternative presentation modes. Examples include TL;DRs (informal, one-sentence summaries, see <ref type="bibr" target="#b145">Syed et al., 2018;</ref><ref type="bibr" target="#b17">Cachola et al., 2020;</ref><ref type="bibr" target="#b98">Mao et al., 2022)</ref>, graphical abstracts (e.g., <ref type="bibr" target="#b162">Yamamoto et al., 2021)</ref>, and video previews<ref type="foot" target="#foot_2">foot_2</ref> . Such materials are used to make the final conference program easier to navigate, are shown in the user interfaces for reviewers and chairs, and are published as part of a centralized effort to promote conference submissions via conference social media accounts. Assistance with adjusting a given work to other presentation modes can additionally speed up the submission process for the authors. Research problems of interest include generating first-draft posters <ref type="bibr" target="#b161">(Xu and Wan, 2022)</ref>, presentations <ref type="bibr" target="#b60">(Hu and Wan, 2013;</ref><ref type="bibr" target="#b47">Fu et al., 2022)</ref>, and accompanying videos given a manuscript. Furthermore, AI could be used to generate an overarching figure for the paper (e.g., based on methods like <ref type="bibr" target="#b5">Belouadi et al. 2023)</ref>, which would help reviewers and future readers to quickly get a gist of the paper. Such generative applications may significantly aid the authors in writing strong papers and subsequently presenting their work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Screening</head><p>Before a submission enters the review process, it undergoes a range of additional checks to make sure it adheres to the standards and requirements of the venue. Failing to pass these checks can lead to desk rejection without review. Authors could use screening tools pre-submission to ensure that the manuscript complies with formal requirements, and to minimize the overhead for the reviewers and program chairs.</p><p>At a bare minimum, screening should ensure correct manuscript formatting. This includes basic parameters such as page count, font size, layout and margins, correct display of figures, and formatting of references. Tools like ACL pubcheck<ref type="foot" target="#foot_3">foot_3</ref> offer some of this functionality and can be extended. Multimodal tools for surface-level analysis of manuscripts such as detecting image manipulation (e.g., <ref type="url" target="https://www.proofig.com">https://www.proofig.com</ref>) can be further deployed, but fall outside of our NLP-centered scope.</p><p>Next, a submission can be evaluated with respect to the content-level editorial requirements. Several AI conferences have recently introduced compulsory discussions of limitations and broader ethical impact of the submissions;<ref type="foot" target="#foot_4">foot_4</ref> simple NLP automation would be sufficient to detect the presence of these sections in the submission. In a similar vein, some checklists<ref type="foot" target="#foot_5">foot_5</ref> ask authors to indicate the sections of their papers in which certain issues are discussed, e.g., reproducibility efforts, compensation to human participants, adherence to data licenses <ref type="bibr" target="#b34">(Dodge et al., 2019;</ref><ref type="bibr">Beygelzimer et al., 2021a;</ref><ref type="bibr" target="#b121">Rogers et al., 2021)</ref>. Automated checks for whether the checklist answers correspond to the indicated paper sections could help authors avoid accidental errors, speed up checklist completion and facilitate reviewing <ref type="bibr">(Liu and Shah, 2023)</ref>.</p><p>Finally, the screening stage should detect violations of general reviewing and publishing rules and policies. This includes checking submissions for plagiarism and self-plagiarism, detecting anonymity violations ("as shown in our prior work"), dual submission (to different venues) and duplicate submission (to the same venue), detecting papers from fake paper mills <ref type="bibr" target="#b41">(Else and Van Noorden, 2021)</ref>, as well as detecting potentially machine-generated text and enforcing compliance with the editorial guidelines on the use of AI for writing assistance. Initial manuscript quality plays a large role in the subsequent process, motivating the deployment of advanced assessment tools already at the screening stage. For example, NLP assistance could help analyze the submissions for coherence, citation coverage, and readability (see Section 5.1). Such analysis would help program chairs and meta-reviewers identify submissions that could be manually reviewed as candidates for desk rejection and mentoring programs, as well as help balance the reviewing load and study the effects of manuscript quality on the subsequent process even if desk rejection does not take place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scoring Potential Reviewer-Paper Matches</head><p>Once a manuscript is submitted and has passed the screening stage, it needs to be assigned to reviewers. Reviewer-paper matching is a crucial step: it is at this stage that both the qualification gap and strategic behavior are best mitigated. In this section, we discuss a key precursor to matching that offers considerable scope for improvement via NLP techniques: determining how well and in what ways a given reviewer is qualified to review a given submission. Then, Section 4.3 discusses considerations that arise in the identification of optimal matches given additional constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Text-similarity scoring</head><p>NLP-based approaches to reviewer-paper scoring typically focus on deriving a similarity score between a text describing a submission (title, abstract, or full paper) and the publication history of a candidate reviewer. Publication history is commonly extracted from third-party aggregators like Semantic Scholar (<ref type="url" target="https://www.semanticscholar.org">https://www.semanticscholar.org</ref>), Google Scholar (<ref type="url" target="https://scholar.google.com">https://scholar.google.com</ref>), or DBLP (<ref type="url" target="https://dblp.org">https:  //dblp.org</ref>). One well-known example of a system employing a text-similarity-based approach is the Toronto Paper Matching System <ref type="bibr" target="#b22">(Charlin and Zemel, 2013)</ref>, which originally used similarity based on word counts and a latent Dirichlet allocation topic model. Recent ACL conferences have employed another system based on an encoder trained on Semantic Scholar abstracts <ref type="bibr" target="#b158">(Wieting et al., 2019;</ref><ref type="bibr" target="#b103">Neubig et al., 2021)</ref>. An alternative approach is to derive similarity scores via general-purpose paper representations <ref type="bibr" target="#b27">(Cohan et al., 2020)</ref>. However, evaluations of current similarity-scoring algorithms have found significant room for improvement <ref type="bibr" target="#b142">(Stelmakh et al., 2023b)</ref>. Indeed, while intuitive, similarity-based scoring faces several challenges. First, the publication history of a potential reviewer does not necessarily reflect their expertise or their current interests-and at a large conference it is virtually guaranteed that some reviewers will have incomplete and inaccurate profiles. Second, a text similarity score might capture criteria that are irrelevant for reviewer assignment, e.g., choice of notation, discussion of basic related work, phrasing in social impact sections, or stylistic elements. Finally, most modern semantic similarity measures are based on dense representations, and thus lack interpretability <ref type="bibr" target="#b72">(Kim et al., 2023)</ref>. These factors lead even the NLP community itself to accord low trust to similarity scores as an assignment criterion <ref type="bibr" target="#b148">(Thorn Jakobsen and Rogers, 2022)</ref>. Addressing these issues within the text similarity paradigm leaves much room for improvement in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Keywords</head><p>A second way of scoring potential reviewer-paper matches draws on manually provided keywords. For example, ACL-2023 experimented with simple matching by keywords selected by authors and reviewers from a pre-defined list, to permit matching according to area, contribution type, and language studied in the paper. Subsequent analysis <ref type="bibr" target="#b123">(Rogers et al., 2023b)</ref> showed that this approach led to higher than average acceptance rates for papers with "minority" contribution types, which would likely otherwise be assigned to people not interested in this type of contribution. Keywords are also more interpretable than text-based similarity scores, and can serve as explanations for the matches. Yet, a poor selection of keywords will fail to adequately differentiate large groups of submissions and reviewers, which in turn will impact the quality of the subsequent matching. Assisting conference organizers in preparing effective keyword lists is a promising target for NLP assistance-for example, one could use lists of session titles assigned in the program at the previous iterations of a conference as a starting point. Keyword suggestion faces additional challenges due to the need to disambiguate unrelated keywords and connect the related keywords: for example, the authors might chose "human factors : ethics" as metadata for their submission, while the reviewer may choose "philosophical foundations : human subject experiments". This challenge may be mitigated by imputing fractional keyword scores to each non-chosen keyword, e.g., based on co-occurrence data between keywords chosen by other papers and authors <ref type="bibr" target="#b85">(Leyton-Brown et al., 2024)</ref>. Further improvements to such techniques can come from using more advanced NLP approaches that directly incorporate keywords' underlying semantics and allow free-form text in place of pre-selected keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Bidding</head><p>At some AI conferences, reviewers can actively "bid" by indicating their interest and expertise (or lack thereof) in reviewing specific submissions. Bidding should be understood as a measure of reviewer interest rather than expertise; for example, bidders might be more likely to bid on submissions that they believe will be high quality, justifying the effort of reading them carefully. They also may bid more actively in areas where they would like to develop expertise, rather than in areas where they already have the expertise but no longer actively work. Furthermore, as the number of submissions increases, bidding faces limitations: there are many papers to bid on, and the reviewers need to enter many bids. These problems can be mitigated if bidding is combined with keyword-based and text-similarity-based approaches, as reviewers can be offered the opportunity to bid only for papers for which they would otherwise be judged to have expertise. This approach could further be augmented to leverage information about the number of bids each paper has already received, aiming to ensure that each paper receives an adequate number of bids <ref type="bibr" target="#b44">(Fiez et al., 2020;</ref><ref type="bibr" target="#b99">Meir et al., 2021)</ref>.</p><p>A further, significant problem with bidding is that it increases opportunities for strategic behavior. Reviewers can bid in order to favorably or unfavorably review work to which they are methodologically aligned or opposed. Reviewers can further use bids to favorably or unfavorably review work they suspect was authored by individuals they count as friends or competitors. Finally, reviewers can engage in explicit collusion rings, wherein each member of the ring bids for papers from the other ring members regardless of expertise, ultimately submitting positive reviews regardless of submission quality. Collusion rings have become a topic of grave concern because of recent reports of their increasing prevalence at major computer science conferences <ref type="bibr" target="#b152">(Vijaykumar, 2020;</ref><ref type="bibr" target="#b91">Littman, 2021)</ref>. Collusive bidding can be mitigated by constraining the ways that reviewers can bid; e.g., each reviewer could be required to bid positively on at least 20 papers, under the assumption that the minimum required bid size is larger than the number of the colluding papers. Collusion can also be curtailed by constraining the matches; see Section 4.3. As for detecting collusions, algorithms based on bidding data alone fail to perform well <ref type="bibr" target="#b63">(Jecmen et al., 2022</ref><ref type="bibr" target="#b62">(Jecmen et al., , 2024))</ref>. Exploring the use of NLP techniques to analyze review texts and associated metadata, alongside bid analysis, presents a valuable research direction for detecting collusion rings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Aggregating scores</head><p>Text similarity assessment, keyword matching, and bidding can complement each other. However, this raises the additional challenge of how they should be combined. AI conferences take different approaches in this regard. ACL-2023 mainly relied on keywords, falling back on similarity scores when keywords were unavailable, and then on manual adjustments by the PCs where necessary <ref type="bibr" target="#b123">(Rogers et al., 2023b)</ref>. NeurIPS-2016 employed a fixed formula <ref type="bibr" target="#b134">(Shah et al., 2018)</ref>. AAAI-2021 aggregated scores from two similarity-based systems and keywords augmented with imputed values, using a manually configured function to derive an aggregate score that takes the bidding choices into account <ref type="bibr" target="#b85">(Leyton-Brown et al., 2024)</ref>. In general, the choice of the best method for combining the individual scores is not clear, and PCs commonly use heuristics to decide on the combination method. A more principled approach to choosing the assignment method is to use post-hoc evaluations of counterfactuals <ref type="bibr" target="#b128">(Saveski et al., 2023)</ref>. Specifically, one can evaluate the potential quality of counterfactual methods-methods which were not used-based on the data from the reviews obtained.</p><p>A key question herein is the measurement variable. While current post-hoc evaluations rely on reviewers' self-reported expertise or meta-reviewer ratings of reviews, a much more nuanced assessment could be enabled by merging NLP-based evaluation of reviews with causal inference techniques <ref type="bibr" target="#b128">(Saveski et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reviewer-Paper Matching</head><p>Given scores for every reviewer-paper pair, the next step is to determine the optimal matching that respects the load preferences of the reviewers. This problem can be addressed with discrete optimization techniques <ref type="bibr" target="#b22">(Charlin and Zemel, 2013;</ref><ref type="bibr" target="#b75">Kobren et al., 2019;</ref><ref type="bibr" target="#b108">Payan and Zick, 2021;</ref><ref type="bibr" target="#b138">Stelmakh et al., 2021a;</ref><ref type="bibr" target="#b123">Rogers et al., 2023b;</ref><ref type="bibr" target="#b85">Leyton-Brown et al., 2024)</ref>, given a set of constraints and optionally an objective function to score the matches. It is a critical step, and it can materially improve matching quality and consequently the quality of the overall peer review process. The role of NLP techniques is to assist with collecting the information that serves as input for discrete optimization. Such information includes scores, as well as constraints like conflicts of interest, reviewer history, and venue-and community-specific constraints that might need to be derived from the submission data.<ref type="foot" target="#foot_6">foot_6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Identifying conflicts of interest</head><p>One major class of matching constraints are the conflicts of interest (COI), which prohibit a given reviewer from being assigned to a given paper. Most conferences ask reviewers to specify their COI manually, but this approach can fail if reviewers are forgetful, have too many conflicts, or strategically choose to under-report their COIs. It is thus desirable to infer conflicts automatically. Conflicts arising from co-authorship can be inferred if reviewers are required to specify their DBLP profile, Semantic Scholar profile, or ORCID (<ref type="url" target="https://orcid.org">https://orcid.org</ref>). AAAI-2021 applied a system of this kind and found at least one unreported conflict for 78.8% of submissions-over 96,000 unreported conflicts in total <ref type="bibr">(Leyton-Brown et al., 2024, Section 5.1.4</ref>). Making such systems more powerful and more robust would require advanced NLP techniques. First, conflicts arising from supervisory relationships, co-holding grants, working at the same institution, being students of the same supervisor, etc. are hard to infer, as there exists no single, comprehensive and reliable source of such data. A second, interlocking problem is the difficulty of name disambiguation. There exist various web services<ref type="foot" target="#foot_7">foot_7</ref> that provide information on co-authorship, joint-grant, and academic advising relationships. However, mapping information from such external databases to the reviewer and author database in a conference is a challenge since the same natural person may be associated with different name variations, multiple and changing affiliations, etc. (Amado <ref type="bibr" target="#b4">Olivo and Kerzendorf, 2023)</ref>. Development of better natural language processing techniques to address this important issue can help improve the checking of conflicts in peer review as well as various other processes like correct attribution of citations, etc. Finally, it may be desirable to add constraints between reviewers who are in direct competition with authors' work-though this issue must clearly be approached with care, as competitors can also be the most knowledgeable reviewers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Reducing incentives for strategic behavior</head><p>The matching problem can be further adjusted to reduce the effectiveness of strategic behaviors such as collusion rings (Section 4.2.3). The AAAI-2021 conference addressed this through various constraints, based on the principle that being overly aggressive about forbidding potentially problematic matches (false positives) was better than allowing collusive behavior to succeed (false negatives), particularly given the large number of potential reviewers for each paper <ref type="bibr" target="#b85">(Leyton-Brown et al., 2024)</ref>. Specifically, this conference penalized matches in which multiple reviewers came from the same geographic area, in which any pair of reviewers had coauthored papers together (even if neither was in conflict with the submission), and in which any pair of reviewers had both bid positively on each other's papers and were both assigned these papers to review (because such "2-cycles" create opportunities for reciprocity). Other AI/ML conferences have further addressed incentives for strategic behavior by adding randomness to reviewer-paper matching scores to make the formation of collusion rings more difficult <ref type="bibr" target="#b65">(Jecmen et al., 2020</ref>). Yet, using only quantitative data like bidding, it is hard to detect collusion rings <ref type="bibr" target="#b63">(Jecmen et al., 2022</ref><ref type="bibr" target="#b62">(Jecmen et al., , 2024))</ref>-and combining this data with NLP analysis bears promise. For example, NLP can be used to re-check the expertise of the assigned reviewer and spot the outliers who may have manipulated bidding. Alternatively, NLP can be used to analyze whether a reviewer engaged in an unusual amount of discussion (Section 5.3) to get a suspected COI paper accepted. The potential of NLP for alleviating the problem is yet to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Prioritizing good reviewers</head><p>As conferences grow, curating sets of reviewers becomes increasingly difficult for the program chairs. Leveraging data about reviewers' past performance can help assemble reviewer sets by giving well-performing reviewers priority in subsequent matches. The most straightforward approach restricts itself to less sensitive metadata about reviewing: whether past reviews were submitted on time, how long reviews were, whether reviewers read author rebuttals and participated in discussions, etc. However, even using review metadata may create privacy risks, as prior work has shown that information about timing of reviews alone can aid in de-anonymizing the reviewers <ref type="bibr">(Goldberg et al., 2023a)</ref>. Natural language processing bears great potential in helping to determine review quality automatically, with caveats as detailed in Section 8. Yet, re-using the data from prior conferences to determine a reviewer's past performance by the current program chairs meets even greater confidentiality risks, and calls for novel applications of privacy-preserving techniques such as anonymization and differential privacy (Section 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Manual assignment support</head><p>At many AI conferences automated reviewer-paper assignment is only the first step, followed by a manual adjustment of the results. This manual adjustment can be a laborious process that can benefit from NLP assistance. For example, NLP could be used to help identify areas of expertise that are required to thoroughly evaluate the submission, but are not offered by any of the currently assigned reviewers. This assistance scenario would be particularly valuable for emerging research areas, for which there are few experts, and for interdisciplinary submissions that can benefit from reviewing expertise outside of the default reviewer pool. For example, at ACL-2023, keyword frequency analysis was used to identify the papers with 'rare' topics, languages or contribution types, and the senior meta-reviewers were asked to consider adjusting the assignments for these papers <ref type="bibr" target="#b123">(Rogers et al., 2023b)</ref>. Ideally, NLP-based tools would aid in identifying such papers more precisely, and could further be expanded to suggest appropriate reviewers and provide PCs with a simple way of inviting these reviewers, incl. explaining why their specific expertise is needed. Such NLP assistance could also be of use for reviewing systems where all reviewer assignments are manual, as done in many journals.</p><p>5.1 Evaluating the Manuscript 5.2 Writing the Review 5.3 Discussion Augmented reading Literature assistance Detecting manuscript flaws Tone, clarity, and substantiation Composition and pragmatics Writing the author response Contextualizing and monitoring the discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ready for review</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ready for decision</head><p>Scoring and calibration Increasing participation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Assistance During Review</head><p>Once the submissions are distributed among reviewers, the reviewing begins. In this section we continue our discussion by outlining core challenges and opportunities for NLP assistance during manuscript evaluation, review-writing and the subsequent discussion between authors, reviewers, and meta-reviewers (Figure <ref type="figure" target="#fig_4">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluating the Manuscript</head><p>The review phase starts with the reviewers reading and evaluating the manuscript at hand. Reading for peer review is a hard expert task that often requires reviewers to build a deep understanding of the work and draw upon their background knowledge and analytical skills. The goal of NLP assistance at this stage is to reduce reviewer effort while promoting grounded and thorough assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Augmented reading</head><p>Recent years have seen substantial advances in developing AI-powered reading interfaces for scientific manuscripts. Such tools can be used to support peer review. For example, SCIM <ref type="bibr" target="#b45">(Fok et al., 2023)</ref> provides multi-class highlights on the manuscript text that can guide the reviewer through the general topic and important aspects of the work, such as contributions, experimental setup and findings. ScholarPhi <ref type="bibr" target="#b58">(Head et al., 2022)</ref> augments the reading application with symbol definitions for parts of mathematical formulae. Guided reviewing interfaces are another subclass of reading assistance for review: since novice reviewers would be unfamiliar with the process of writing a review, a tool that guides them through the process could improve review quality <ref type="bibr" target="#b141">(Stelmakh et al., 2021d)</ref>. For example, the recently introduced ReviewFlow <ref type="bibr" target="#b144">(Sun et al., 2024)</ref> directly approaches the problem of AI-based scaffolding to support novice peer reviewers. General-purpose frameworks for developing AI-augmented reading applications <ref type="bibr" target="#b164">(Zyska et al., 2023;</ref><ref type="bibr" target="#b94">Lo et al., 2023)</ref> facilitate further development of guided reading interfaces for peer review, along with question-answering tools to help manuscript navigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Literature assistance</head><p>Reviewing requires in-depth understanding of prior work. Due to the fast pace and increasing topical diversity of modern research, such in-depth understanding might be lacking even for experienced reviewers. NLP has potential to assist literature work in several ways. Citation recommendation tools <ref type="bibr" target="#b12">(Bhagavatula et al., 2018)</ref> can be integrated into the reviewing process to help reviewers detect relevant work not mentioned in the manuscript, potentially guiding the judgement of novelty of the work <ref type="bibr" target="#b157">(Wang et al., 2024)</ref> and helping address the common problem of unjustified lack-of-novelty criticisms <ref type="bibr" target="#b120">(Rogers and Augenstein, 2020)</ref>. Such assistance would help reviewers suggest additional comparative discussions when needed, in the light of conflicting results in related work or missing baselines. Systems for citation recommendation and fact-checking <ref type="bibr" target="#b154">(Wadden et al., 2022)</ref> may be adapted to help the reviewer assess whether the references that are present in the manuscript are cited appropriately, detecting factually wrong claims and misrepresented prior results, and potentially guiding the judgement of the work's soundness and motivation. As part of assessing the paper, reviewers often use search engines to find related literature-and might encounter public versions of the manuscript under review, which can inadvertently expose the authors' identities and affect the evaluation <ref type="bibr" target="#b117">(Rastogi et al., 2022)</ref>. Custom search engines that hide the manuscript under review from the search results obtained as part of the reviewing process could help mitigate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Detecting manuscript flaws</head><p>Finally, NLP has the potential to help reviewers with more in-depth analysis of the manuscript. Recent work has shown that generative AI models could be used to identify errors such as mathematical mistakes, conceptual fallacies, or problems in experimental design <ref type="bibr" target="#b106">(Nuijten and Polanin, 2020;</ref><ref type="bibr">Liu and Shah, 2023)</ref>, but might overemphasize certain aspects such as implications of the research and adding more experiments, compared to humans <ref type="bibr" target="#b90">(Liang et al., 2023)</ref>. Multimodal applications of NLP to analyze the correspondence between numerical results (figures and tables) and their textual interpretation in the manuscript are another promising research direction <ref type="bibr" target="#b15">(Blecher et al., 2023)</ref>. While it is questionable whether the process of manuscript assessment itself can or should be fully automated, it might be possible to develop tools that nudge reviewers to consider typical pitfalls. For example, NLP assistance can be used to check if the manuscript complies with specific experimental standards of a research community, such as reporting of computational costs, statistical testing, study participant details, etc.-as outlined in reproducibility checklists widely employed in the NLP and AI community <ref type="bibr" target="#b34">(Dodge et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Writing the Review</head><p>During paper evaluation, reviewers often take notes and draft the initial review. This draft then needs to be transformed into an official document addressed to the authors, meta-reviewers and program chairs, that constructively evaluates the paper, informs the meta-reviewers about the benefits and flaws of the submission, and instructs the authors on potential improvements. Low-quality reviews are frustrating and commonplace, indicating the potential for review writing assistance. The goal of NLP at this stage is to help the reviewer craft a useful review report and mitigate common issues prior to submitting it into the system. The issues detected at this stage can be taken into account by the reviewer for self-improvement, or by the meta-reviewers in case the issue persists (Section 6.1). Feedback to reviewers could be combined with concrete consequences after repeated violations of the established reviewing standards. Effective NLP tools to evaluate the quality of reviews may be directly useful in building incentives that encourage reviewers to provide high-quality work <ref type="bibr" target="#b54">(Goldberg et al., 2023b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Tone, clarity, and substantiation</head><p>Reviewer anonymity aims to elicit objective reviews and protect the reviewers from potential backlash-yet anonymity also reduces accountability on the reviewer side. As a result, rude, vague and unsubstantiated reviews are not uncommon. Computational approaches to politeness analysis <ref type="bibr" target="#b13">(Bharti et al., 2023;</ref><ref type="bibr" target="#b113">Priya et al., 2024)</ref> can help mitigate communication issues. To address vagueness in peer reviews, specificity analysis <ref type="bibr" target="#b87">(Li and Nenkova, 2015)</ref> can be deployed; <ref type="bibr" target="#b50">Ghosal et al. (2022)</ref> investigate hedging and uncertainty expressions in peer review. To improve review substantiation, claims that are inconsistent with the content of the paper or not supported by related work may be detected <ref type="bibr" target="#b57">(Guo et al., 2023)</ref>. Matching reviewer comments to paper content can help detect the overall lack of anchoring to the submission text, detect misaligned comments, and point reviewers to specific sentences or passages in the paper with contrary evidence <ref type="bibr" target="#b78">(Kuznetsov et al., 2022;</ref><ref type="bibr" target="#b29">D'Arcy et al., 2023)</ref>. Another connection may be drawn between review substantiation and a broad line of work on scientific fact checking <ref type="bibr" target="#b52">(Glockner et al., 2024)</ref>, where claims made by reviewers would need to be validated against wider bodies of scientific knowledge.</p><p>A special case of substantiation analysis pertains to heuristics and strategic behavior. Peer review is a hard task prone to heuristics, such as "results are not surprising", "results do not surpass state of the art", "the method is too simple", "the authors should have done [X] instead", and others.<ref type="foot" target="#foot_8">foot_8</ref> Due to the competitive environment, reviewers might behave strategically, for example, by enforcing citations of their own work or by writing "torpedo reviews" aimed to scuttle certain topics or competing groups. Heuristics and strategic behavior might be observed in review text, and thus could be automatically detected by NLP methods, helping the reviewers check the thoroughness and impartiality of their argumentation. However, the aforementioned issues count as such only if a particular evaluation or statement is not substantiated or faces contrary evidence. For example, simply rejecting a work for not delivering a state of the art score is a heuristic. Yet, if results do not surpass state of the art while the manuscript incorrectly claims otherwise, it is a legitimate concern. Similarly, a reviewer suggesting highly relevant work, which happens to be their own, is not necessarily behaving strategically. The problem of detecting heuristics and strategic behavior thus goes beyond surface-level analysis, and constitutes an exciting direction for future NLP research. Finally, as with manuscripts, plagiarized and fake reviews are also an issue <ref type="bibr" target="#b110">(Piniewski et al., 2024)</ref> that NLP tools can help detect and prevent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Composition and pragmatics</head><p>Peer review is an argumentative text that pursues a range of communicative goals. Yet, unlike research papers that generally follow an established discourse structure, the standards of peer review writing are less codified. NLP assistance can be thus deployed to enforce the adherence of peer reviews to the composition standards accepted in a given community. One potential line of work contributing to this is the discourse analysis of peer reviews: pragmatic tagging <ref type="bibr" target="#b78">(Kuznetsov et al., 2022;</ref><ref type="bibr" target="#b40">Dycke et al., 2023b)</ref> aims to label each review sentence according to its role (such as strength, weakness, todo or summary), while argument-mining approaches <ref type="bibr" target="#b61">(Hua et al., 2019)</ref> discover the argumentative structure of reviews, splitting them into facts, requests, and evaluations. This can be coupled with the argumentative analysis of the submission itself <ref type="bibr" target="#b80">(Lauscher et al., 2018</ref><ref type="bibr" target="#b81">(Lauscher et al., , 2022))</ref>.</p><p>A traditional way to enforce composition of peer review reports is to deploy structured and semi-structured reviewing forms-which has been shown to improve inter-rater agreement of decision recommendations <ref type="bibr" target="#b96">(Malički and Mehmani, 2024)</ref>. Structured forms open new opportunities for NLP assistance in peer review. On one hand, NLP can be used to help reviewers pre-fill structured reviewing forms based on their drafts and reviewing notes, and to detect inconsistencies (e.g., a substantial weakness listed as a question). On the other hand, data from structured reviews can be used to improve review quality in non-structured reviewing-for example, <ref type="bibr" target="#b40">Dycke et al. (2023b)</ref> use structured review forms as weak supervision for analyzing essay-style reviews. This analysis could be applied to provide feedback to reviewers during writing and be particularly useful for novice reviewers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Scoring and calibration</head><p>To facilitate decision making at the later stages of the process, reviewers are often asked to accompany their reports with numerical scores. An overall score and a confidence score are commonly used; some conferences employ additional "criteria scores", like novelty, soundness, impact, or excitement. While convenient, scoring meets multiple challenges. Given the high effort associated with reviewing, each reviewer normally evaluates only a small subset of submissions. This naturally leads to calibration issues. One kind of miscalibration arises when reviewers have different interpretations of the scores or thresholds for acceptance, with some reviewers being stricter, some lenient, and so on <ref type="bibr" target="#b155">(Wang and Shah, 2019)</ref>. Several algorithms have been developed to address this issue <ref type="bibr" target="#b124">(Roos et al., 2012;</ref><ref type="bibr" target="#b49">Ge et al., 2013)</ref> but have not performed well in practice (see <ref type="bibr" target="#b132">Shah, 2022</ref>, Section 5 for more details on this miscalibration). These algorithms rely solely on the numerical scores provided by reviewers, and can be complemented by NLP analysis. Score prediction <ref type="bibr" target="#b70">(Kang et al., 2018;</ref><ref type="bibr">Li et al., 2020, and others)</ref> can be used to detect inconsistencies between review text and the overall score, and confidence score prediction can similarly be used to detect both over-confident and under-confident reviews. This information can help the reviewers adjust their scores prior to submitting the review, and can be especially useful for junior reviewers and researchers who review for multiple conferences which employ different scoring schemata and score semantics. At an overarching level, NLP tools that can access the text of reviews across the conference (or even multiple conferences) can help calibrate reviews across the entire reviewer pool. Another calibration issue occurs when reviewers employ different ways of combining individual criteria (such as novelty, clarity, etc.) into their overall recommendation score-often referred to as "commensuration bias" <ref type="bibr" target="#b83">(Lee, 2015)</ref>. Existing solutions to commensuration bias such as <ref type="bibr" target="#b105">(Noothigattu et al., 2021)</ref> are limited due to their reliance on numeric scores alone, and NLP can substantially contribute to this line of research by helping extract individual preferences from reviewers' aggregate review reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>After the initial round of reviews is completed, peer review enters the discussion phase. Here, the authors communicate with their reviewers for the first time, and can address reviewers' questions and concerns, and ensure that the key contributions of the paper are properly understood. This is followed by a discussion among the reviewers-often prompted by the meta-reviewer. Here, reviewers can edit their review reports and adjust the scores to reflect their final assessment of the manuscript. This discussion creates a complex dialogue grounded in the context of the paper, reviews, and the conversation so far, and offers ample space for NLP assistance. The goal at this stage is to ensure that the discussion is efficient and effective, and results in a more objective evaluation of the submission, which later informs the meta-review (Section 6.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Writing the author response</head><p>The authors are presented with a set of reviews discussing the paper and given a short period of time to respond. Like reviews, author responses can be done in ways that are more or less productive, and similarly to review writing support (Section 5.2), authors can be provided with feedback about the composition and tone of their author responses, including argument mining, pragmatic tagging, politeness, and specificity analysis <ref type="bibr" target="#b48">(Gao et al., 2019)</ref>. Recent work has explored helping authors argue for their submission via attitude-and theme-guided rebuttal generation <ref type="bibr" target="#b115">(Purkayastha et al., 2023)</ref>. NLP tools can further help the authors extract key questions and concerns from the peer reviews, e.g., by extracting individual action items, aggregating points of concern, and helping the authors devise a plan for the author response, as outlined by the widely used tutorial on rebuttal-writing in the ML community.<ref type="foot" target="#foot_9">foot_9</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Contextualizing and monitoring the discussion</head><p>Due to the author response period, the discussion among reviewers can take place several weeks after the initial reviewing. Because of this time lag reviewers may struggle to interpret parts of the discussion pertaining to the submitted manuscript, other reviews, and author responses. To help guide the discussion, cross-document analysis can be applied to identify and give the reviewers clear pointers to the parts of the manuscript under discussion <ref type="bibr" target="#b78">(Kuznetsov et al., 2022;</ref><ref type="bibr" target="#b29">D'Arcy et al., 2023)</ref>. NLP assistance can provide direct mappings between the author responses and review reports <ref type="bibr" target="#b23">(Cheng et al., 2020)</ref>, and help analyze the discourse structure of the author responses, e.g., by highlighting parts of the response that address reviewers' criticisms and reporting on the changes in the submission text in response to the review <ref type="bibr" target="#b71">(Kennard et al., 2022)</ref>.</p><p>The organization of the discussion phase depends on the conference management system and the venue. For example, in the author response process at a typical AI conference, all participants need to engage in the discussion and track updates that apply to them. Most current CMS can either send notifications for all updates, which can be overwhelming, notifications of explicit replies, which may be insufficient, or no notifications, which puts the onus on the participants to track discussion. Basic natural language processing could assist by detecting updates to the conversation that concern each participant and notifying them accordingly. Similarly, NLP tools can be used to notify meta-reviewers in cases their intervention is mandatory-such as open conflict between reviewers and authors, related to recent work on dispute tactics in Wikipedia (De Kock and Vlachos, 2022). The utility of the discussion phase is often debated: while some studies find that the discussion phase did little to alter reviewers' opinion <ref type="bibr" target="#b30">(Daumé III, 2013;</ref><ref type="bibr" target="#b134">Shah et al., 2018)</ref>, others reveal favorable outcomes <ref type="bibr" target="#b107">(Parno et al., 2017;</ref><ref type="bibr" target="#b46">Frachtenberg and Koster, 2020)</ref>. Past studies also hypothesize and experiment with anchoring and herding effects among reviewers, wherein a reviewer might be (disproportionally) influenced by other reviewers <ref type="bibr" target="#b146">(Teplitskiy et al., 2019)</ref> or might follow the reviewer who initiates the discussion <ref type="bibr">(Stelmakh et al., 2023a)</ref>. Often such anchoring and herding effects are undesirable, and NLP techniques may assist in tracking and flagging the influence of reviewers towards each other by analyzing the discussion and the subsequent changes in the review reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Increasing participation</head><p>Reviewing takes substantial time and effort, and is rarely the reviewers' primary professional occupation. Low reviewer participation in the discussion phase is commonplace <ref type="bibr">(Shah et al., 2018, Section 3.5)</ref>. Two possible approaches to promote discussion among reviewers are generic reminders-which are often ineffective and ignored-and personalized reminders-which are generally known to be more effective <ref type="bibr" target="#b151">(Vervloet et al., 2012)</ref>, but require effort by the meta-reviewers. Given the capabilities of modern generative AI for text, NLP tools may assist meta-reviewers in nudging reviewers to further engage in the process, from customizing and pre-filling reminder templates to generating personalized reminder emails that can be finalized by the meta-reviewers and sent to participants. A deeper level of NLP assistance can be deployed to detect reviewer statements and author responses that go unanswered or are not sufficiently followed through. A prototypical and familiar case is when a reviewer points out a concern which leads them to assign the work a low overall score. In their response, the authors fully address the concern. In the discussion phase, reviewer thanks the authors for their response, but does not adjust the score. As result, the authors notify the meta-reviewer about potential unfair reviewing, or, alternatively, simply accept their fate and hope that the issue is noticed during meta-review. Automatically detecting such cases during the discussion when the reviewers are still present and engaged has a potential to improve the quality of the process and reduce meta-reviewers' workload at later stages of peer review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Assistance After Review</head><p>Once reviewing is finished, the reviewing campaign enters its final phase. Following the terminology introduced in Section 2, the review reports from the reviewers are aggregated by the meta-reviewers (area chairs); these are later used by senior meta-reviewers (senior area chairs) to make accept/reject recommendations, which inform the program chairs (PC) in the final decision-making phase. We now discuss the core challenges and opportunities in applying NLP to support meta-review writing, decision making, camera-ready revision preparation, and post-analysis of the reviewing campaign (Figure <ref type="figure" target="#fig_5">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Meta-review</head><p>Meta-reviews are the first step in the decision-making process. A meta-reviewer's duty is to aggregate input from individual reviews, detect low-quality reviews, find emergency reviewers if needed, and oftentimes to read the manuscript themselves and provide their own grounded opinion. The result of this work is a meta-review-a brief report to the PCs that summarizes individual peer reviews and discussions, often including a meta-reviewer's own evaluation. Meta-review also summarizes feedback and required changes for the authors and provides an initial acceptance recommendation. Preparing a high-quality meta-review is hard: the meta-reviewer must read each review, rebuttal, and discussion thread, aggregate the information, take a stance on controversial points, and assess whether the authors' responses sufficiently address the reviewers' concerns. NLP assistance at this stage aims to facilitate this complex multi-document analysis task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Review and discussion aggregation</head><p>The meta-reviewer's task of aggregating information from the prior peer-reviewing data meets three core challenges: the amount of information that needs to be processed, the varying quality of this information, and the inherently argumentative nature of the underlying texts. To help meta-reviewers navigate and interpret the reviews, same NLP approaches that can help reviewers compose their reports (Section 5.2.2) can be adopted to separate peer reviews into statements and categorize these statements according to their argumentative or pragmatic function. This way, a meta-reviewer can quickly aggregate information from different peer reviews, e.g., get an overview of the manuscripts' strengths, weaknesses, and improvements suggested by multiple reviewers. We note that peer reviews afford analysis across multiple dimensions, which can all support meta-reviewers in their work. In particular, aspect-based analysis of peer reviews <ref type="bibr" target="#b20">(Chakraborty et al., 2020;</ref><ref type="bibr" target="#b71">Kennard et al., 2022)</ref> can help identify aspects of the manuscript that were insufficiently or vaguely covered by the reviewers. For example, lack of evaluation in the "technical soundness" aspect can prompt the meta-reviewer to investigate the manuscript more carefully, and ultimately help prevent accidental acceptance of technically flawed work. We note that existing works on aspect-based analysis of peer reviews operate with fairly coarse-grained aspect labels. A finer granularity dependent on the contribution type might bring further benefits: for example, a dataset paper would not require rigorous discussion of the experimental setup, but would instead prompt more detailed scrutiny into questions such as licensing, participant recruiting and compensation, guidelines, etc. To help meta-reviewers navigate the discussion around the manuscript, approaches building upon discussion assistance (Section 5.3.2) can be re-used at this stage, e.g., by providing explicit links between the manuscript, the reviews, author's responses and discussions.</p><p>A further level of assistance can be provided via meta-review generation-automatic summarization of peer reviews, discussions and author responses. Such summaries can include a list of contributions, weaknesses, and a summary of the discussion, ideally taking into account reviewer confidence and expertise. This task is challenging due to the need to process diverse, interconnected texts which contain differing, often antagonistic opinions. Authors want their manuscript to be accepted: they will argue in favor of their paper, possibly trying to minimize issues identified by the reviewers or to provide easy solutions to reviewers' concerns. Reviewers want the manuscript critically assessed: they will focus on the perceived weaknesses and might disagree with each other. Meta-review generation is related to aspect-based sentiment analysis and multi-document summarization, and has recently received increased attention. <ref type="bibr" target="#b163">Zeng et al. (2023)</ref> cast the task as scientific opinion summarization. They propose a checklist-based approach to prompt an LLM to write summaries that fulfill meta-review requirements of self-consistency, faithfulness, and active engagement. Li et al. ( <ref type="formula">2023</ref>) explore a summarization model that incorporates inter-document relationships between reviews, author responses, and discussions, finding that the model struggles with recognizing and resolving conflicts. <ref type="bibr" target="#b127">Santu et al. (2024)</ref> conduct in-depth experiments with prompting LLMs to perform the task. <ref type="bibr" target="#b135">Shen et al. (2022)</ref> and <ref type="bibr" target="#b76">Kumar et al. (2021)</ref> explore meta-review generation in a controlled generation setting conditioned on the meta-review score. These initial approaches bear promise that NLP will eventually be able to support meta-reviewers in navigating the complex reviewing discourse. Yet, given the importance of meta-reviews for decision making, we caution against the use of NLP to fully automatically compose meta-review reports and take decisions on meta-reviewers' behalf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Writing assistance</head><p>While meta-reviewers are typically more experienced than reviewer pool average, they work under stricter time constraints and experience higher workload than regular reviewers. Many aspects of review-writing also apply for meta-review, and meta-reviewers can benefit from similar NLP assistance. Similar to review reports, tools can help ensure that the meta-reviews are clear and well-substantiated (Section 5.2.1) and include key information necessary for decision making by the PCs such as manuscript strengths, weaknesses and required improvements (Section 5.2.2). NLP assistance can further help calibrate meta-review scores and acceptance recommendations (Section 5.2.3). It is important to note, however, that meta-reviews and review reports serve different purposes, and many of the reviewing assistance techniques would need adjustment to help meta-reviewers. While the main purpose of a review report is to evaluate the work as thoroughly as possible, a meta-review aims to facilitate decision making by the program chairs, and the most helpful meta-reviews are the ones where the meta-reviewer unequivocally takes a stance in favor or against a paper. Approaches to hedging and uncertainty detection <ref type="bibr" target="#b50">(Ghosal et al., 2022)</ref> thus gain in importance in meta-review writing assistance. Similarly, while reviewers might give authors improvement suggestions which the authors are free to adopt or ignore, meta-reviewers can condition manuscript acceptance on fulfilling some new criteria, marking the need for not only detecting, but also prioritizing required changes. While reviews must be substantiated based on the manuscript and related work, meta-reviews must additionally take into account the content of author responses and discussions, etc.</p><p>Overall, the high stakes at this stage of peer review motivate the development of assistance tools to ensure the highest possible quality of meta-reviewing. At the same time, given the high stakes, increased care needs to be taken as not to bias the meta-reviewers and distort their decisions. Thus, full automation of this stage of peer review is ill-advised at the current state of NLP technology, and possibly on general ethical grounds (Section 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Final Decisions</head><p>Once meta-reviews are completed, program chairs (PCs) are presented with meta-reviews for each manuscript, the original reviews, author responses and discussion, and the paper itself, and make the final accept/reject decision for each paper. Peer review is an imperfect process, and falsely accepted and rejected submissions are not uncommon. Given access to the fullest information about the peer-reviewing process up to this stage, PCs can use NLP assistance to address issues with the reviewing process and detect deficiencies in the meta-reviews. Yet, similar to our stance on meta-review assistance, we advice against a fully automated approach to assessing the value of scientific papers. The goal of NLP assistance at this stage is to provide additional input and aggregate existing information to increase it's utility for decision making, and never to replace expert human judgement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Improving consistency</head><p>and meta-reviews. Often the decision-making process requires a calibration step prior to ranking papers on any aspect of the meta-reviews. Similar to reviewer score calibration (Section 5.2.3), NLP tools can be used to compare meta-review scores and comments, and allow a possible scaling of meta-reviews that can reduce unintended and unfair assessments. At a minimum, PCs can be given a type of "strictness score' for the the meta-reviewers that can be used by PCs to better calibrate and rank papers.</p><p>Finally, NLP tools can assist PCs in detecting cases where the meta-review appears to be taking a significantly different stance on the overall assessment of the paper compared to the reviewers. Cases found to have large disagreements in this sense might point to situations that need careful consideration as they may indicate unethical meta-reviewing. For example, when a meta-reviewer defends a paper, but reviewers have found the that paper has major flaws that cannot be addressed by the camera-ready deadline, PCs need to intervene and ensure that the meta-reviewer is not colluded with authors; in the case where the meta-review is overly negative and the reviews are not, PCs need to evaluate whether the meta-reviewer is biased or is acting in an unprofessional manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Supporting editorial priorities</head><p>While manuscript evaluation by the (meta-)reviewers often has clearly defined criteria and is supported by extensive guidelines, final decisions by PCs are typically less structured for the larger set of borderline papers. Some PCs might want to rank papers on certain topics higher than papers on other topics in the same borderline batch, based on their editorial priorities for a given venue. Current CMS and peer review tools allow specifying formulae based on numerical scores in the reviews-yet this is insufficient for the PC's decision-making process, which often involves the text of the reviews and meta-reviews, the discussion, etc. Creating an automated tool that can adjust on the fly to the ranking specified by the PCs would be a potentially useful application to explore. As an example, PCs could explicitly define their ranking criteria in natural language: "rank all papers in the borderline group based on the largest disagreement in overall assessment, and then based on their relevance to the special theme". This definition could then serve as input to NLP assistance systems that would make use of the combined information in the manuscript text and review reports to select and prioritize the relevant subset of submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">Best paper award nominations</head><p>The technical programs in conferences typically include (best) paper awards. Given the large submission numbers, candidate submissions are usually determined either by explicitly asking reviewers to nominate the papers they are reviewing, or by considering manuscripts that have received a very high score among reviewers.<ref type="foot" target="#foot_11">foot_11</ref> Both approaches face challenges. Reviewer's nominations are often scarce: for example, at the ACL-2023 conference, almost all submissions considered by the best paper committee had only one reviewer nomination. Because of this, many equally deserving submissions get overlooked because they never get nominated. Relying on the scores, in turn, is prone to calibration issues (Section 5.2.3). One approach to increasing fairness of best paper selection would be to augment existing mechanisms with automated tools that take review texts into account. Given a set of high-scoring or nominated submissions, such tools could detect papers that have received similar comments about their novelty, soundness and contributions. Approaches to detecting innovative work as is done by <ref type="bibr" target="#b59">Hofstra et al. (2020)</ref> could be used here to identify papers that are unique and worthy of adding to the pool of best paper candidates. This automatically augmented pool of papers can be then passed on to the best paper award committee for ranking and selection of best papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Revision and Camera-ready</head><p>Whether accepted or rejected, the manuscript is revised at least once as a result of the peer-reviewing process. If the manuscript is accepted, the authors must incorporate reviewer suggestions into the camera-ready version ("Please provide p-values for Table X"), as well as follow up on the commitments made in the rebuttal ("We will add this to the limitations"). Similarly, if the manuscript is rejected, the authors are expected to use the reviewers' feedback to make the manuscript stronger before resubmitting it to another venue. Revision is often accompanied by an amendment note where the authors describe the changes made, submitted along with the revised manuscript. NLP has the potential to assist the participants of the peer-reviewing process in two core ways: by helping authors revise the manuscript based on feedback, and by helping reviewers, meta-reviewers and program chairs analyze the resulting changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Revision assistance</head><p>Ideally, the revision should incorporate reviewer and meta-reviewer feedback as well as the authors' commitments made during discussion. This can result in a large number of interconnected edits of different localization and complexity that need to be prioritized and executed while adhering to the formal requirements like page limitations. Recent works in NLP pave the path towards automatic edit localization by connecting reviews and rebuttals to the manuscript text <ref type="bibr" target="#b78">(Kuznetsov et al., 2022;</ref><ref type="bibr" target="#b29">D'Arcy et al., 2023)</ref>; suggesting and executing some of the revisions has been also explored and proven a challenging task <ref type="bibr" target="#b29">(D'Arcy et al., 2023)</ref>. Once reliable NLP technology is available, user-facing applications that build upon it would need to overcome the variation in reviewing and publishing workflows between different research communities and venues, as well as technical challenges related to processing complex, richly formatted long documents (Section 7).</p><p>Besides, the space of potential manuscript revisions is very diverse, from correcting a single-word typo to including new experimental sections. Not every such edit operation can or should be automated. Edit prioritization and routing of the revisions between NLP assistants and human authors is an exciting avenue for future research. Automatic edit summary generation to help the authors compose amendment notes is yet another promising research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Revision analysis</head><p>The revision step raises new questions for reviewers, meta-reviewers and program chairs as well. Does the revision address all the points raised by the reviewers and follow up on the promises made during the rebuttal? Currently, nothing technically prevents the authors from entirely ignoring some of the reviewer suggestions once the manuscript is accepted. Does the revision feature new material that has not been reviewed and that significantly alters the substance of the manuscript? Again, at the moment, nothing stands in the way of this, and manually re-analyzing the changes for each submitted manuscript is similarly not feasible. These use cases motivate automatic analysis of manuscript revisions. While basic diff-like functionality is commonplace in conference management systems like OpenReview, more advanced processing of revisions has been approached for arXiv pre-prints <ref type="bibr" target="#b66">(Jiang et al., 2022;</ref><ref type="bibr" target="#b36">Du et al., 2022)</ref> and ACL publications <ref type="bibr" target="#b100">(Mita et al., 2022)</ref>, and in the context of peer review <ref type="bibr" target="#b78">(Kuznetsov et al., 2022;</ref><ref type="bibr" target="#b29">D'Arcy et al., 2023)</ref>. Existing NLP approaches help align manuscript revisions on sentence and paragraph level, detect new and changed content, and can automatically label the edits by their intent, e.g., grammar, clarity or factual adjustment. These pilot studies can already support both manuscript-level analysis to aid the reviewers and meta-reviewers, and collection-level analysis that can provide general, high-level insights about the revision process across different research communities or intervention groups ("Does the new policy result in more thorough revisions?"). Potential directions for follow-up work in this area include multimodal processing to incorporate tables, formulas and figures into the analysis; designing and evaluating user-facing applications to help reviewers and the editorial team to check the work; and meta-studies on the general revision behavior across research communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Post Peer-Review Analysis</head><p>Peer review generates large and complex empirical data that can help us analyze and improve the process. Examples of such analysis include post-conference reports <ref type="bibr" target="#b134">(Shah et al., 2018;</ref><ref type="bibr" target="#b123">Rogers et al., 2023b</ref>) that cover various aspects and changes to the review process for a given conference, or evaluations of interventions in the peer-review process, such as the reviewer training experiment of <ref type="bibr" target="#b141">Stelmakh et al., 2021d</ref>. Post peer-review analysis typically amounts to reporting minimal summary statistics, that are presented at the conference and recorded in the front matter of the proceedings. Further analysis is labor-intensive and requires additional effort from the few participants with full access to the reviewing data (program chairs and technical staff), who are typically exhausted at the end of the reviewing campaign. This motivates the development of automated tools for post-peer review analysis.</p><p>NLP assistance could open up a range of avenues for analysis beyond the standard statistics compiled today. First, it could help us learn more about our reviewing. We could identify reviewer trends and biases, for example, identifying which requests from reviewers correlate with acceptance decisions. Those requests may be legitimate, and sharing them may help future authors avoid common pitfalls, or they may be problematic, indicating areas where additional guidance is needed for reviewers. Aggregate analysis of reviews, discussions, meta-reviews and revisions can give further insights, e.g., on what forms of discussion influence the meta-review the most, and for what reasons people adjust their review reports. Initial results in NLP-supported aggregated analysis of reviewing and editing behavior in peer review show great promise: among others, <ref type="bibr" target="#b78">Kuznetsov et al. (2022)</ref> investigate the connections between review comments and manuscript edits, while <ref type="bibr" target="#b61">Hua et al. (2019)</ref> use argumentative structure to compare review composition in different computer science venues and investigate the relationships between review argumentation and scores. Yet, existing results are preliminary and motivate future work.</p><p>Post peer-review analysis opens new opportunities to evaluate the reviewing process as a whole. Is our peer review able to accurately predict the impact of the work and reliably evaluate its soundness? To answer this, one might elicit impact assessments from the reviewers and relate them to some metric of future impact of the work. One simple-yet imperfect-metric of impact is the number of citations that a manuscript accumulates <ref type="bibr" target="#b111">(Plank and van Dalen, 2019)</ref>, potentially refined by citation type or class, such as background citations and method citations <ref type="bibr" target="#b147">(Teufel et al., 2006;</ref><ref type="bibr" target="#b0">AbuRa'ed et al., 2017;</ref><ref type="bibr" target="#b67">Jurgens et al., 2018;</ref><ref type="bibr" target="#b112">Pride and Knoth, 2020;</ref><ref type="bibr" target="#b81">Lauscher et al., 2022)</ref>. The insights from this kind of automatic analysis would allow us to evaluate the effects of reviewing policies, gain a better empirical understanding of how peer review works, and hopefully make the process as a whole more transparent and trustworthy.</p><p>Finally, post-review analysis can help us learn more about researchers. Conducting reviewer surveys is commonplace at AI conferences <ref type="bibr" target="#b104">(Névéol et al., 2017;</ref><ref type="bibr" target="#b117">Rastogi et al., 2022;</ref><ref type="bibr" target="#b133">Shah, 2023)</ref>. The results of these surveys can impact policies at future conferences. Typically, the surveys include structured and free-text fields. While structured fields can be easily aggregated and analyzed, free text and open-ended questions require careful reading and aggregation by the conference organizers. NLP tools for the analysis of free-text responses (e.g., by clustering or extracting common topics) could help reveal new trends and patterns in peer review and efficiently elicit feedback on new policies and common issues encountered by the reviewers. Assuming that the peer-reviewing data is made available (Section 7), one could study trends in the scientific process over time, and compare reviewing practices across communities and disciplines. What questions do reviewers at NLP conferences ask today compared to five years ago, and how does that reflect the shifting norms in the field? Are these questions the same at other AI conferences, or are there differences? How does reviewing at AI conferences differ from human-computer interaction or computational social science venues? As research becomes increasingly multi-disciplinary, these insights might be highly valuable for scientists both while evaluating unfamiliar contribution types, and while submitting their own work to new communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Data, Privacy, and Copyright</head><p>Developing NLP assistance for peer review requires data. Several unique features of peer-reviewing data set it apart from more traditional and well-covered application domains in NLP like Wikipedia, newswire, social media, and even the closely related domain of scientific publications. In the following we focus on the data that directly results from the peer-reviewing process, using AI conferences as a representative example for conference-style peer review. While we touch upon legal aspects of peer-reviewing data collection, we stress that this discussion does not constitute legal advice. We particularly recommend consulting legal experts when collecting reviewing data from a new venue or community. Figure <ref type="figure">5</ref>: Peer-reviewing data includes many complex document types that are semantically and structurally interrelated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Data Composition</head><p>Peer-reviewing data is complex and diverse (Figure <ref type="figure">5</ref>). It covers a range of document types: the manuscript itself, its review reports, rebuttals, discussion threads, meta-reviews, and-potentially-a camera-ready revision. Many of these documents are multimodal in a broad sense, combining written language with categorical metadata, structure (e.g., manuscript layout), non-linguistic elements (e.g., figures, tables, formulae, formatting), and numerical scores (e.g., overall and confidence scores). In addition, peer-reviewing data is highly interconnected, where later documents are informed by the earlier documents: the review does not exist in isolation, but discusses and actively refers to the manuscript; rebuttals are not standalone texts but act as replies to the reviews, and meta-reviews emerge as weighted summaries of individual (potentially revised) reviews and the subsequent discussion. This complexity brings new supervision and evaluation signal for NLP systems that would be hard to obtain otherwise. For example, if review reports are associated with numerical scores, one can directly use this information to learn to predict score from text, which in turn can aid with score calibration (Section 5.2.3) and outlier detection. At the same time, the complexity and inter-connectedness of the reviewing data bring new challenges. NLP has traditionally focused on the processing of isolated texts reduced to plain written language. While structured representations for scientific texts are available <ref type="bibr" target="#b95">(Lo et al., 2020;</ref><ref type="bibr" target="#b78">Kuznetsov et al., 2022;</ref><ref type="bibr" target="#b15">Blecher et al., 2023)</ref>, this information is rarely preserved when creating peer-reviewing data, limiting its further use; general-purpose formalisms for representing cross-document relations in peer review are equally scarce. The complexity of the data also affects task design, as it introduces new, potentially crucial contextual and multimodal information. Thus, when collecting peer-reviewing datasets, we deem it important to preserve as much information as possible, including non-textual elements of review forms, manuscripts in the original submission format, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Data Availability</head><p>Peer-reviewing data is abundant, yet open peer-reviewing data is scarce, and NLP for peer review suffers from the lack of domain diversity. For a concrete example, the ACL-2023 conference alone generated over 12,000 reviews for over 4,500 full-text manuscripts <ref type="bibr" target="#b123">(Rogers et al., 2023b)</ref>, together with the accompanying meta-reviews, discussions, and revisions. Yet, apart from the camera-ready manuscripts for the accepted papers, none of this data is public. While some other communities and publishers make the reviews openly available e.g., F1000Research (<ref type="url" target="https://f1000research.com">https://f1000research.com</ref>), PeerJ (<ref type="url" target="https://peerj.com">https://peerj.com</ref>), ICLR (<ref type="url" target="https://iclr.cc">https://iclr.cc</ref>), despite its growing popularity <ref type="bibr" target="#b159">(Wolfram et al., 2020)</ref>, open review is an exception rather than a rule. Three strategies for dealing with the lack of open peer-reviewing data emerge. First, one can develop NLP systems and conduct experiments using closed data-as it has been done in earlier meta-scientific studies of peer review <ref type="bibr" target="#b16">(Bornmann, 2011;</ref><ref type="bibr">Tomkins et al., 2017, etc.)</ref>. Alternatively, the data can be distributed under special conditions via a waiver-similar to the common practice in clinical NLP and other fields dealing with sensitive data. Yet, closed data is problematic in terms of reproducibility, makes follow-up research and deployment of derivative NLP systems challenging, and brings new administrative and technical challenges of redistributing the data and tracking its use.</p><p>Second, one might resort to open data from few select venues and communities. Datasets based on open reviewing constitute the majority of peer-reviewing data available to NLP. Yet, challenges remain here as well. Since open reviewing data comes from only few communities, it might not be representative of the diverse reviewing practices in a new community or discipline. The sole fact that reviews are made publicly available (sometimes with reviewer identity disclosed, such as at F1000Research) might affect the tone and composition of the review reports, and some publishers only release peer reviews for accepted papers. Moreover, the data being openly accessible does not mean that the data can be used for research purposes and openly redistributed. While some publishers are explicit about their licensing and terms of use (e.g., F1000Research), in other cases the legal status of the publicly available reviews and manuscripts is unclear. The third option for handling peer-reviewing data is donation-based data collection, where the data from a previously closed reviewing process is made openly available. The advantage of data collection on a donation basis is that it theoretically can be applied to any community and any venue. The disadvantage is that it takes time and effort, and needs to take into account the confidentiality, personal data regulations, and licensing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Considerations for Data Collection</head><p>Peer-reviewing data in a closed-review system is confidential. Making this data open via donation-based data collection requires explicit permission from the participants. Such permissions can take the form of terms of service and policy made known to the participants before the start of the reviewing campaign, with or without an option to opt out, like it is done at F1000Research and-since at least September 2023-at OpenReview.<ref type="foot" target="#foot_12">foot_12</ref> Alternatively, the permission can be given by the participants on a case-by-case basis in an opt-in fashion. An additional challenge stems from the fact that peer-reviewing data is interconnected. In particular, any data associated with an unpublished manuscript (reviews, discussions) can leak information and results that can be misused by malicious parties (e.g., by scooping research ideas) and lead to resubmission bias, where knowledge about a paper's previous rejection at a conference can negatively influence subsequent reviews <ref type="bibr" target="#b140">(Stelmakh et al., 2021c)</ref>. Potential counter-measures include only collecting the data associated with accepted papers, or making the data public with a substantial time lag that would allow the authors to disseminate their work in the meantime, even if rejected. A direct consequence of all these strategies is that with each step the data becomes less representative of the full data of a reviewing campaign. <ref type="foot" target="#foot_13">13</ref> One solution to this issue is to quantify the difference between closed full data and partial open data by using non-confidential aggregate statistics like score distribution, vocabulary overlap, etc., and report the comparative statistics between the open sample and the full population upon data release. Some of the peer-reviewing data relates to natural persons (authors, reviewers, meta-reviewers, program chairs), and thus constitutes personal data under GDPR (<ref type="url" target="https://gdpr.eu">https://gdpr.eu</ref>), which regulates the use of personal data of EU subjects anywhere in the world <ref type="bibr" target="#b121">(Rogers et al., 2021)</ref>, and is widely considered as a best-practice in personal data management overall. While GDPR is more permissive to research use than commercial use <ref type="bibr" target="#b68">(Kamocki et al., 2018)</ref>, it still imposes requirements on the collection of reviewing data, including informed consent, a clear reflection on what exact data is collected and for what purpose, how the data is stored and managed, whether the data can be withdrawn and consent revoked, etc.</p><p>Finally, peer-reviewing data constitutes intellectual property of its respective authors. Thus, in order to make peer-reviewing data openly available for follow-up research, it must be associated with a license that regulates its downstream use. Peer-reviewing data includes diverse document types authored by multiple parties, and many peer-reviewing data types fall into the grey zone of the common licensing practice. Some publishers make manuscripts openly available under a permissive license-but what about the initial submitted drafts and peer review reports? Who should be allowed to use the data and in which way? Who should be attributed as the data owner and who should stay confidential? Who should manage and distribute the licensed data, and how should a data repository be selected for secure long-term storage? These questions invite additional reflection and consideration while designing a data collection campaign; for a broader discussion see <ref type="bibr" target="#b9">(Bender and Friedman, 2018;</ref><ref type="bibr" target="#b69">Kamocki and Witt, 2020;</ref><ref type="bibr" target="#b121">Rogers et al., 2021)</ref>. A special case that deserves a mention is the use of third-party services to process peer-reviewing data, which became increasingly common with the advent of closed-source LLMs accessible via APIs. We stress that processing peer-reviewing data off-site requires careful consideration of the license, consent, and confidential status of the data. While experimenting with openly licensed datasets may not prohibit such use, deploying an off-site LLM to process closed (or not-yet-open) peer-reviewing data raises ethical and legal concerns.</p><p>Example: Data collection at ACL Rolling Review. One example of a peer-reviewing data collection process that takes confidentiality, personal data, and licensing into account is the "Yes-Yes-Yes" workflow used at ACL Conferences through the ACL Rolling Review system. This opt-in workflow was developed following an in-depth community discussion and legal advice, and consists of three steps (1-3). Before reviewing starts, the reviewers are (1) asked to give informed consent for all their peer reviews in a given reviewing campaign, and to transfer a Creative Commons Non-Commercial license<ref type="foot" target="#foot_14">foot_14</ref> for their review reports to an external copyright holder (the Association for Computational Linguistics). The reviewers are given an option to request attribution: in this case, their name appears on the copyright notice, by which the reviewers get credit for their work, but effectively become deanonymized. This constitutes the "Yes" from the reviewers, and if reviewers do not give the permission for data use, the data collection for the given review stops. After this, the reviewing campaign proceeds, the decisions are taken, and some papers are (2) accepted. This constitutes the "Yes" from the PCs: an accepted paper with its intellectual content will appear in proceedings. For those accepted papers, the authors are then asked to (3) consent to the data associated with their manuscript being collected and made public. This is the "Yes" from the authors. In the end, only the data for accepted papers where both reviewers and authors have explicitly consented to its public research use is made open. This highly selective procedure is automated and integrated into the OpenReview CMS used by ACL Rolling Review. It is supplemented by license agreements and consent forms. In the reviewing campaigns of September to November 2021, over 1,900 reviewers and the authors of 235 submissions contributed their data, later published as part of the NLPeer corpus <ref type="bibr" target="#b39">(Dycke et al., 2023a)</ref>. The workflow is deployed in the peer-reviewing system of ACL Rolling Review, collecting new data on a bi-monthly basis, which is later released to the public. The procedure itself is further developed to include additional data types and to increase author and reviewer participation. The above workflow can serve as a starting point for future data collection efforts in other communities; for further details see <ref type="bibr" target="#b38">(Dycke et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Measurement and Experimentation</head><p>How do we know whether our tools to support peer review work well? Empirical study of NLP assistance for peer review requires systematic experimentation, which amounts to measuring the intrinsic performance of NLP systems (e.g., "does my tool reliably detect unsubstantiated claims in the manuscript?"), as well as measuring the downstream effects of the NLP assistance on the process (e.g., "does this tool, once deployed, reduce the time and increase the quality of peer reviews?"). Yet, many operations and assessments in peer review are hard to formalize, which translates into the challenge of converting these operations into robust NLP tasks and evaluation criteria. This poses a major problem for NLP and AI, whose methodology largely depends on objective and measurable performance. Even with measurements well-defined, peer review remains a complex process which involves many factors and has many interacting parts. This introduces the challenge of designing experimental setups that allow measuring the effects of interest while avoiding confounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Defining and Measuring the Variables</head><p>In our discussion of NLP-based peer-reviewing assistance so far, we have often assumed that the phenomenon to be modeled-or the outcome to be improved-is well-defined and easily measurable. Yet, this is often not the case, especially for complex, subjective phenomena like "peer-review quality", "manuscript quality" or "reviewer expertise". While one might be tempted to focus on easily accessible signals like "review helpfulness score" for review quality or "confidence score" for reviewer expertise, it is important to remember that these signals only serve as imperfect proxies of the complex processes that we aim to study and improve <ref type="bibr" target="#b156">(Wang et al., 2021;</ref><ref type="bibr" target="#b54">Goldberg et al., 2023b)</ref>. Lack of clarity on what is measured and how it is measured can be problematic. It can affect both the direction of subsequent research ("manuscript quality prediction is solved"), and the perception of the results by the public and the policymakers ("we can use AI to predict manuscript quality"). Here we warn against hastily defined and poorly measured variables in NLP research for peer review, and outline a few key considerations that we deem important. Our discussion below is broadly applicable, in line with the trend towards more reliable operationalization and measurement in NLP in general <ref type="bibr" target="#b7">(Belz, 2022;</ref><ref type="bibr" target="#b8">Belz et al., 2023;</ref><ref type="bibr" target="#b130">Schuff et al., 2023;</ref><ref type="bibr" target="#b160">Xiao et al., 2023)</ref>. The ideas expressed in these works are, in turn, inspired by the practices in other disciplines that face similar challenges, such as psychology, educational science, and sociology, highlighting the increasing need for collaboration between NLP and other disciplines.</p><p>Many variables in NLP for peer review are not hard objective properties. Instead, they are constructshypothetical variables that we can't observe directly. Review quality, manuscript quality, and expertise are constructs-similar to "language proficiency", "music taste" or "political preferences". Since constructs cannot be observed directly, they need to be operationalized : we must clearly define them, propose measurements to make quantitative statements about them, and assess whether these measurements are adequate. Like language proficiency tests serve as one imperfect measure for the actual "language proficiency," any evaluation of "reviewer expertise"-be it a confidence score, the ability to detect technical flaws, or something else-is similarly just a proxy. This needs to be taken into account and clearly communicated when reporting any scientific results in high-stakes application domains like peer-reviewing assistance.</p><p>To be able to clearly communicate about NLP and AI for peer review, we first require precise and explicit definitions of our constructs. For example, many of the approaches that we discussed in this paper aim to improve peer review quality. But do we agree on what exactly peer review quality is? Is it reviewers' ability to detect manuscript flaws and recognize the potential impact of the work? Is it the well-formedness of the resulting review report? Is it the utility of the review for the decision making? Or is it a combination of these? Clearly defining the variable can help design the measurements to estimate it, evaluate these measurements, inform downstream applications that make use of it, and foster shared understanding of the object of study among researchers.</p><p>Once defined, variables in peer review afford a wide range of measurements-including evaluation against a gold standard, participant surveys, time-related measurements, etc. Generalizing from the measurementtheoretical perspective on natural language generation by <ref type="bibr" target="#b160">Xiao et al. (2023)</ref>, the two core desiderata for any measurement are reliability and validity. A reliable measurement is robust to random error. This includes measurement stability (e.g., obtaining same value on repeated measurement) and measurement consistency (e.g., across raters or data points). A valid measurement, in turn, is robust to systematic error. This includes concurrent validity (the obtained measurements correlate with other measurements for the same construct) and construct validity (measurements behave in a way that reasonably reflects the construct, and does not reflect something else). We note that to judge measurement validity, we need multiple alternative ways to measure the variable of interest and other related variables. Designing accurate, valid, and reliable measurement methods is hard, and provides an important avenue for the future empirical study of peer review in the context of NLP and AI assistance, and in general.</p><p>Example: Review quality. In the rest of this subsection, we illustrate the aforementioned discussion by an example of review quality. We limit our scope to the quality of the review report. The review report has two purposes: to inform the meta-reviewers about the content, merits, and issues with the work, and to ask the authors for clarifications and provide them with actionable improvement suggestions. Based on this, one can construct the quality of a review report as follows. We assume that to be helpful for the meta-reviewers, the review report should summarize the work and clearly outline its strengths and weaknesses. We assume that to be helpful for the authors, the review report should clearly outline the potential directions in which the work can be improved. We assume that a high-quality review should be clear and concise, written in a professional tone, well-substantiated, and that the reviewer should avoid heuristics and strategic behavior. Like manuscripts that typically fall into "clear accept", "clear reject", and "borderline" categories, peer review reports form clusters with respect to their quality. Thus, while we can expect agreement on review reports that are clearly high-and low-quality, we can also expect a broad class of borderline cases, where our indicators of review quality would be less reliable.</p><p>Following this notion of review report quality, we can discuss the measurements. We can measure structural well-formedness by manually or automatically labeling parts of a review according to their pragmatics, e.g., strengths, weaknesses or feedback to authors (Section 5.2.2)-and downscoring the review if one of the pragmatic categories is missing. We can apply manual analysis or automated tools to determine clarity, conciseness, and politeness of a review report. We can attempt to detect heuristics and strategic behavior in peer review text, along with substantiation cues like references to manuscript text and related work-and downscore reviews that are poorly substantiated and potentially biased (Section 5.2.1). As a proxy of report utility, we can elicit numerical review quality scores from the authors and meta-reviewers, using either a single Likert scale, or a structured questionnaire like Review Quality Instrument <ref type="bibr" target="#b150">(van Rooyen et al., 1999)</ref>.</p><p>We then need to empirically estimate the reliability and validity of each of these measurements. We take review quality rating as example. If we ask the authors to rate the reviews they receive, would their rating be stable, i.e., would they give the same rating when asked at a later date? Would the ratings given by the authors and meta-reviewers be consistent? Do review quality ratings relate in a reasonable way to other measurements of review quality, like well-formedness and substantiation? Do they correlate with some variables unrelated to review quality? For example, are longer reviews perceived as higher quality, and do unfavourable review scores make the authors judge reviews as lower-quality <ref type="bibr" target="#b54">(Goldberg et al., 2023b)</ref>? Reliable and valid measurements can help us (1) design NLP assistance tools and policies that target specific issues with review report quality, (2) evaluate the effect of these tools on the quality of the review reports, and (3) evaluate the effect of reviewing policies by measuring aggregate quality of review reports in a given reviewing campaign. We note that each new measurement allows us not only to quantify a variable of interest, but also to validate the already existing measurements. Inconsistencies might indicate that our definition of peer review quality is incomplete or that our measurements need further refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Experiment Design</head><p>As variables and measurements in peer review are diverse, so are the available ways to obtain these measurements. As a complex process, peer review affords many experimentation models, many of which go beyond the standard NLP practice. We briefly outline the core experimental setups that can be used to measure the performance and the effects of NLP assistance in peer review.</p><p>Human-subject evaluation: A natural method of evaluating the output of any NLP model is to ask people with suitable expertise to evaluate it. For instance, to assess a model that generates reviews of papers (or meta-reviews from individual reviews), researchers may be asked to evaluate the quality of these reviews <ref type="bibr" target="#b90">(Liang et al., 2023;</ref><ref type="bibr" target="#b28">D'Arcy et al., 2024;</ref><ref type="bibr" target="#b127">Santu et al., 2024)</ref>. In addition to the challenge of defining review quality, such human evaluation is prone to bias: for instance, <ref type="bibr" target="#b54">Goldberg et al. (2023b)</ref> find that authors of the papers are biased by the positivity of reviews, while third-party evaluators are biased by the length, etc. These biases must be taken into account when interpreting the results. A further drawback of human evaluations in NLP for peer review is that they require effort and expertise, and might be challenging to reproduce <ref type="bibr" target="#b8">(Belz et al., 2023)</ref>.</p><p>Gold-standard evaluation: An alternative approach is to carefully create "gold standard" ground truth. For example, to assess the capabilities of large language models in terms of evaluating papers, Liu and Shah (2023) (i) create a set of 14 papers with deliberately inserted errors, and assess if the LLMs can find these errors; (ii) create a set of 10 pairs of abstracts such that one has strictly stronger contents than the other in each pair, and assess if the LLMs can identify the stronger abstract; (iii) manually label the author checklists for a subset of papers from the NeurIPS conference, and assess if the LLM can verify them. A second example pertains to evaluating models that compute expertise of reviewers for papers. Here, <ref type="bibr" target="#b142">Stelmakh et al. (2023b)</ref> construct a "gold standard" dataset by asking researchers to rank and rate papers they have read in terms of their own expertise. A third example is that of a 'Chimera' test from <ref type="bibr" target="#b132">Shah (2022)</ref>, which constructs a mashup of multiple papers to create a nonsensical paper. This paper is then used to test if proposed 'automated reviewer systems' can detect its nonsensical nature. Objective evaluation and measurement of peer review variables meets two core challenges: not every variable can be objectively defined to yield a single ground truth; and even if it is defined, constructing an extensive gold dataset often requires substantial effort, thereby prohibiting large-scale experimentation.</p><p>Laboratory experiments: In some scenarios, simply asking participants to perform evaluations of the artifacts may not suffice, but instead the evaluation may warrant a closer replication of some aspects of an actual peer-review environment. If one cannot conduct the experiment in an actual peer-review process, one may replicate a part of the review process with either individual participants (e.g., as done for rebuttals by <ref type="bibr">Liu et al., 2023)</ref> or groups of participants (e.g., as done for strategic behavior by <ref type="bibr" target="#b139">Stelmakh et al., 2021b;</ref><ref type="bibr" target="#b64">Jecmen et al., 2023)</ref>. Such a setting is particularly well-suited for measuring time-related variables, as well as evaluating the performance of real-time reviewing assistance tools <ref type="bibr" target="#b164">(Zyska et al., 2023;</ref><ref type="bibr" target="#b144">Sun et al., 2024)</ref>. A careful experimental design under this approach may also allow capturing parts of the review process that are concerned with the evaluation task and eliminate those that are not, thereby allowing to rigorously establish causal effects.</p><p>Observational studies on open data: Publishers and platforms that make reviewing data open (Section 7) make it possible to conduct large-scale observational studies <ref type="bibr" target="#b97">(Manzoor and Shah, 2021;</ref><ref type="bibr" target="#b78">Kuznetsov et al., 2022)</ref>. For example, one can learn how reviewers write peer reviews by collecting openly available data and using off-the-shelf discourse analysis tools (Section 5.2), or study the relationships between review texts and scores purely based on observation. The core challenge associated with such studies is the lack of influence over the experimental setup, which makes it more challenging to control for confounds and make causal claims. Further, even if well-suited for extracting high-level insights about reviewing processes in a given community, the data might be not representative of closed-reviewing scenarios and carry potential biases that influence the observed outcomes. Moreover, observational studies on open data are limited to the data types that are made open, and provides limited insight into the parts of the peer review not commonly recorded and archived, such as the processes of review writing, manuscript evaluation or final decision making.</p><p>Natural experiments: The policies or policy changes in peer review process can result in natural experiments. For example, several conferences in recent times have inserted some randomness in the reviewer assignment process <ref type="bibr" target="#b65">(Jecmen et al., 2020)</ref> in order to mitigate the issue of collusion rings. This randomization can now be exploited to understand various counterfactuals such as the effects of alternative assignment algorithms <ref type="bibr" target="#b128">(Saveski et al., 2023)</ref>. A second example pertains to the ICLR conference which switched from single-blind to double-blind reviewing in 2018. This change in policy is used to investigate biases with respect to author identities in single-blind reviewing <ref type="bibr" target="#b97">(Manzoor and Shah, 2021)</ref>. Similar strategies can be applied if an NLP assistance tool is deployed in the reviewing system-taking into account the potential risks and harms of such application, as discussed below (Section 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Controlled experiments:</head><p>The highest standard of scientific experimentation is offered by controlled experiments, where the researcher has a high degree of influence over the full experimental setting and can ensure minimal interference between the external factors and the target observations. An example of a controlled experiment is separating the participants in two groups corresponding to different reviewing conditions (e.g., with or without a certain NLP assistance tool) and measuring the effect of the condition on some variable of interest (e.g., reviewing time). Controlled experiments are increasingly used in machine learning research on peer review to understand different aspects of the process <ref type="bibr" target="#b82">(Lawrence and Cortes, 2014;</ref><ref type="bibr" target="#b149">Tomkins et al., 2017;</ref><ref type="bibr" target="#b11">Beygelzimer et al., 2021b;</ref><ref type="bibr">Stelmakh et al., 2023a;</ref><ref type="bibr" target="#b116">Rastogi et al., 2024)</ref>. These works can serve as a valuable source of inspiration for controlled experiments in the context of NLP assistance. A controlled experiment can yield crucial insights into the process, but one must also simultaneously ensure fairness and efficiency of the review process, for example, in consideration of the different treatment effects in randomized control trials.</p><p>The choice of the experimental setup will ultimately depend on the variables that need to be measured. NLP assistance tools can both help obtain these measurements (e.g., deriving a politeness score based on the text), and be the target of the evaluation. In both cases, we stress the importance of studying the effects of domain shift on NLP assistance performance. For the results to be robust, the findings need to be validated across domains, scientific fields and communities, as well as across time (e.g., due to changes in topics and composition of the research community) and across reviewing procedures (e.g., the use of particular reviewing forms or discussion formats). While such comprehensive evaluation is hardly feasible, this highlights the need for carefully documenting the above aspects when performing experiments on peer review, including the overall procedure used, the community involved, as well as descriptive statistics and examples of the reviewing data. Finally, if the experiment involves human subjects, the researcher should obtain an approval from an Institutional Review Board (IRB) or an equivalent ethics board before conducting the experiment. Researchers are also encouraged to pre-register their experiments, via platforms like OSF (<ref type="url" target="https://osf.io">https://osf.io</ref>) and AsPredicted (<ref type="url" target="https://aspredicted.org">https://aspredicted.org</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Ethics of NLP-Assisted Peer Review</head><p>Peer review is a hard, time-consuming task prone to error. The goal of NLP assistance for peer review is to make parts of the process more efficient and effective. Ideally, using NLP for reviewing assistance would reduce the number and severity of existing ethical issues in peer review, such as human biases. However, NLP and AI tools also have the potential to create new risks and introduce new biases. Therefore, it is essential to discuss the potential risks of using NLP for peer review, and how these risks can be mitigated through means taken by the application designers and by the community.</p><p>One central issue that accompanies the use of AI tools for decision support is the often implicit assumption that a more automated process is fairer, safer, or less biased than a manual process. Yet, the performance of AI tools can be misleading, and automated systems that seem to work well in the majority of cases can induce complacency, bringing new risks in terms of bias propagation, systematic error, outcome disparity, and safety. <ref type="foot" target="#foot_15">15</ref> As of today, we possess neither the technology capable of performing peer review in a satisfactory manner, nor the methodology to evaluate its effects on the affected community and the scientific process. While NLP tools can support peer review on the level of individual tasks, it is necessary to define the types of biases that these tools can exhibit, and to systematically monitor and quantitatively evaluate NLP assistance tools with respect to these biases-in addition to the tools' performance. <ref type="foot" target="#foot_16">16</ref>Novel tools and policies can bring additional risks and raise new issues, but some of these risks are not new and already exist in the system. For example, algorithms may introduce biases, but even without algorithms, people already have biases encoded in their evaluation and opinions. Similarly, in terms of transparency, new algorithms require transparency, but the human process of decision making also requires transparency, which can be improved in existing processes as well. Therefore, the development of NLP assistance and the subsequent policy decisions should be made in consideration of the trade-off between risks, advantages, and benefits in the existing systems, and the risks, advantages, and benefits introduced by the new systems. We outline several areas of concern in NLP for peer review from an ethics standpoint.</p><p>Bias: Deploying NLP assistance for peer review bears the risk of reinforcing and augmenting existing biases in the process <ref type="bibr" target="#b131">(Shah et al., 2020)</ref>. Unfair decisions will not magically go away, and authors will always disagree with rejections <ref type="bibr">(Goldberg et al., 2023b, Section 4.2)</ref>. Groups of people or areas of research might feel excluded and this might lead to allocational harm (i.e., high-stakes impact of unfair decisions). NLP assistance models can pick up on idiolects of people who frequently published in the past. Thus, in the process of designing new systems, special attention needs to be paid to measuring and mitigating potential biases, old and new.</p><p>Transparency: Given the importance of peer review outcomes for scientific process and research careers, a high level of transparency is desired.</p><p>Transparency of machine-assisted peer review can be increased by explicitly stating what types of NLP assistance are deployed in a given reviewing campaign, publishing detailed descriptions of the models used (e.g., on blogs) and model cards. Useful information includes: how the tools are built, deployed, what the tools are capable of-what they can and cannot do, how are the decisions made whether to use these tools and to what extent, and how is data privacy addressed.</p><p>Using open-source models promotes scientific openness and reproducibility-yet increases the risks of malicious behavior by adversarial parties who can exploit vulnerabilities in these tools. Related to transparency is explainability: while explainable NLP is a major research direction, there are hardly any works that explicitly target explainability in NLP for peer reviewing <ref type="bibr" target="#b72">(Kim et al., 2023)</ref>-an important area for future study.</p><p>Agency: With NLP in the loop, who is responsible for the outcome? If an NLP assistance module produces a faulty prediction, is it the fault of the developers who created it, the conference organizers who deployed it, or the users who failed to check the predictions? Given the ever-growing capabilities of NLP systems, reviewers, authors, and editors alike may be inclined to outsource their responsibility on the review to the "machine", e.g., by submitting texts and assessments automatically generated by LLMs with little further involvement. Such cases need to be explicitly addressed by the community policy-for example, it can be made clear that the user is responsible for the ultimate content and quality of the work.</p><p>Parallel to that, it must be ensured that the mechanisms for reporting and recourse can adequately account for the consequences of NLP-assisted submission, matching, reviewing, discussion, etc.</p><p>Privacy: The sharing of peer-reviewing data and the use of models to assist in the peer review process could potentially be abused to infer personal information about the reviewers, threatening the integrity of the process. Following the example by <ref type="bibr" target="#b65">Jecmen et al. (2020)</ref>, suppose the models used to assign reviewers to papers, the data pertaining to publication history of each reviewer, and all submissions are public. If the models and optimization procedure are deterministic, anyone can re-run these models to reconstruct the assignment of reviewers to papers, thereby compromising the anonymity of the review process. <ref type="bibr" target="#b33">Ding et al. (2020</ref><ref type="bibr" target="#b32">Ding et al. ( , 2022) )</ref> provide further examples. Additional concerns include reidentification (also called deanonymization) by analyzing the writing patterns from publicly available reviews and matching them to the publicly available papers. In addition to the careful legal treatment of reviewing data and its derivatives (Section 7), privacy-preserving techniques such as differential privacy <ref type="bibr" target="#b37">(Dwork and Roth, 2014;</ref><ref type="bibr" target="#b74">Klymenko et al., 2022)</ref> and anonymization <ref type="bibr" target="#b89">(Li et al., 2007)</ref> should be explored to minimize risks to the participants of the peer-reviewing process when sharing data. A disclaimer prohibiting the use of data and models for author profiling and reidentification should be included with the release of the respective resources.</p><p>We stress that it is not possible to foresee all potential consequences of NLP assistance for peer review. Given the high-stakes nature of the process, once NLP systems are deployed, the users will likely adjust their behavior over time to gain further benefits from the system-intentionally or not. We note that this is not unique to NLP automation; deployment of any new policy can have a similar effect. This highlights the importance of closely monitoring the changes in the reviewing process in response to new policies and tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion: A Call for Action</head><p>Peer review is a critical part of modern science. As science accelerates, peer review faces new challenges related to logistics, cost, bias, low-quality reviewing and strategic behavior. In this white paper, we have discussed the potential ways in which natural language processing can help make the peer-reviewing process more efficient and robust, while taking into account potential legal, methodological, and ethical challenges. We believe that this research area has great potential to contribute to the study and advancement of the scientific process in the age of AI. The natural language processing and machine learning communities are uniquely positioned to study AI for peer review, as we practice peer review ourselves (and thus can reap the benefits of AI assistance), establish our own reviewing policies, and have the capability to develop tools that can alleviate some of the pressing issues with the process. We thus finish the paper with a call for action-the steps that individual readers of this work can take to help advance NLP for peer review.</p><p>In their role as scientists, we invite the readers to participate in the discussion. The goal of machineassisted peer review is to make the lives of researchers better-and this can not be done without taking the voice of the community into account. Openly available reviewing data is scarce. If you can, consider donating your data for research use-naturally, given that the data is collected according to adequate standards of licensing, personal data and privacy protection. NLP assistance for peer review seeks to help experts. Experts are hard to recruit on crowdworking platforms and to capture in public polls. When possible, participate in the controlled experiments, respond to surveys, and give feedback on the deployed assistance tools.</p><p>We invite NLP and AI researchers to contribute work. Peer review offers a plethora of applications for any area of NLP-from summarization to ethics to applied user studies. To help researchers get started, our companion repository (<ref type="url" target="https://github.com/OAfzal/nlp-for-peer-review">https://github.com/OAfzal/nlp-for-peer-review</ref>) compiles a set of relevant datasets pertaining to peer review. It will be regularly updated and welcomes new contributions. We call NLP and AI researchers working in the peer-reviewing domain to observe good scientific practice. This includes responsible handling of peer-reviewing data, methodological clarity and rigor, attention to the ethical implications of the work, and honesty in communicating the results and outlining the risks. Finally, help build the community. NLP for peer review is an emerging topic that attracts interest both within and outside NLP. Promote relevant existing work, participate in related shared tasks, join non-AI meetings on related topics,<ref type="foot" target="#foot_17">foot_17</ref> and consider organizing new meetings that will help grow and consolidate the community.</p><p>We call on policymakers for peer-reviewed venues to consider the opportunities of NLP assistance in peer review, but also to account for the risks. Fostering a culture of responsible tool use requires new policies. We believe four aspects to be of particular importance. First, machine-assisted quality control in science calls for transparency. It must be clear who decides to use what tool and for which purpose, how these tools are developed, and what risks they bear. Second, it is important for the community to have trust in the tools they are using. Thus, explainable methods for reviewing support should be given preference. Next, deploying NLP assistance in live reviewing systems calls for a high degree of quality control: new features should be rigorously tested, and processes must be in place to revert the use of new tools. This calls for careful, incremental deployment of NLP assistance for peer review: selecting new features carefully, observing their impact and the way they affect the community, and evaluating them before introducing the next bit of technology. Finally, deployments of NLP assistance (as well as any other systematic change to the process) call for careful monitoring, and all participants of peer review-authors, reviewers, meta-reviewers and program chairs-should be given clear feedback channels that will help inform further policy changes.</p><p>Finally, we invite the readers affiliated with research-sponsoring organizations or serving as their reviewers to prioritize research questions and investments that will enable progress in NLP and AI research for scholarly peer review. As science grows, peer review in many disciplines and research communities faces challenges. Inefficient peer review diverts the public resources from producing new research and training the next generation of scientists. Unreliable peer review, in turn, threatens the integrity of the scientific process and undermines public trust in scientific work. We believe that the NLP and AI research communities should be at the forefront of developing and using tools to support peer review, and should be among the beneficiaries of the recent progress in AI that they have enabled.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Peer review as a process, along with the resulting artifacts. Stages of peer review color-coded. (A) -authors, (R) -reviewers, (M) -meta-reviewers.</figDesc><graphic coords="5,148.70,200.65,51.49,51.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Areas of assistance before peer review.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Areas of assistance during peer review.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Areas of assistance after peer review.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>Fig. 1 Score</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Such as https://clarivate.com/web-of-science-academy or https://github.com/reviewingNLP/ACL2020T3material</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>At some conferences the author identities are known to the PCs and meta-reviewers. Pre-printing and advertising the work on social media are not uncommon either , potentially making the authors non-anonymous<ref type="bibr" target="#b117">(Rastogi et al., 2022)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For example, https://uist.acm.org/2024/cfp/#papers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/acl-org/aclpubcheck</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://aclrollingreview.org/cfp</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://aclrollingreview.org/responsibleNLPresearch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>For example, the target language of analysis is an important variable in NLP. At ACL-2023 most submissions focused on English, and relatively few focused on other languages. ACL-2023 aimed to ensure that such submissions had the first choice of reviewers speaking those languages, so as to provide such work with more fair and higher-quality reviews. This had a positive effect-subsequent analysis showed that reviews from reviewers matched by (non-English) language were 1.29 times less likely to be flagged by the authors for review issues(Rogers et al., 2023b, p. lxiii).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Besides DBLP and ORCID, further examples include National Science Foundation (https://nsf.gov), and academic genealogy tree databases such as https://genealogy.math.ndsu.nodak.edu and https://academictree.org.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>See https://aclrollingreview.org/reviewertutorial for more examples.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>See https://deviparikh.medium.com/how-we-write-rebuttals-dc84742fece1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_10"><p>Reviewed manuscripts typically fall into three categories: easy accept papers, easy reject papers and borderline papers that require careful consideration. At major AI conferences, the borderline paper pool is very large, from hundreds to thousands of papers. Automatic tools can be developed to help PCs in ranking borderline papers according to a combination of PC-defined criteria. NLP tools can assist decision making by grouping borderline papers in terms of their topic, contribution type, as well as strengths and weaknesses as identified by the reviewers and meta-reviewers. A dedicated tool can then keep track of decisions made for similar papers across different topics or generate clusters of decisions to facilitate comparison and minimize cases where a paper is accepted, and another comparable paper is rejected. For example, in clusters from the same track, PCs might want to compare the assertiveness of the meta-review comments, and rank papers based on this aspect: prior work on detecting uncertainty and hesitation<ref type="bibr" target="#b50">(Ghosal et al., 2022)</ref> could be extended for these cases.Another potential direction for NLP assistance at this stage is to facilitate pairwise paper and review comparison in case of ties, e.g., by aggregating individual contributions of the papers based on review reports</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_11"><p>See https://www.aclweb.org/adminwiki/index.php/ACL_Conference_Awards_Policy orShah et al. (2018, Section 2.3).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_12"><p>https://openreview.net/legal/terms</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_13"><p>Which, in fact, can be rarely represented in full, as some variables like reviewer identities or their bids are very likely to never be made public.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_14"><p>https://creativecommons.org/licenses/by-nc/4.0</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_15"><p>For example, to aid criminologists in predicting whether a criminal is likely to commit more crimes, AI-based tools have been developed, which were subsequently themselves shown to exhibit systematic biases<ref type="bibr" target="#b114">(ProPublica, 2016)</ref>, resulting in a discussion about various definitions of bias.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_16"><p>See<ref type="bibr" target="#b24">Chouldechova (2017)</ref> and<ref type="bibr" target="#b73">Kleinberg et al. (2016)</ref> for a discussion on trade-offs between biases in automatic systems and fairness criteria.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_17"><p>Such as the International Congress on Peer Review and Scientific Publication, https://peerreviewcongress.org.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The work of <rs type="person">Nihar B. Shah</rs> and <rs type="person">Alexander Goldberg</rs> was supported by <rs type="funder">NSF</rs> <rs type="grantNumber">1942124</rs> and <rs type="grantNumber">ONR N000142212181</rs>. The work of <rs type="person">Ilia Kuznetsov</rs>, <rs type="person">Nils Dycke</rs>, <rs type="person">Sheng Lu</rs> and <rs type="person">Iryna Gurevych</rs> was supported by the <rs type="grantName">LOEWE Distinguished Chair "Ubiquitous Knowledge Processing</rs>" (<rs type="affiliation">LOEWE initiative, Hesse, Germany</rs>) and co-funded by the <rs type="funder">European Union</rs> (<rs type="affiliation">ERC, InterText, 101054961</rs>), the <rs type="funder">German Research Foundation (DFG)</rs> as part of the <rs type="projectName">PEER</rs> project (grant <rs type="grantNumber">GU 798/28-1</rs>) and by the <rs type="funder">German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science</rs> and the Arts within their joint support of the <rs type="funder">National Research Center for Applied Cybersecurity ATHENE</rs>. The work of <rs type="person">Kevin Leyton-Brown</rs> was funded by an <rs type="funder">NSERC</rs> <rs type="grantName">Discovery Grant</rs> and a <rs type="funder">CIFAR Canada AI Research Chair (Alberta Machine Intelligence Institute</rs>). The work of <rs type="person">Anne Lauscher</rs> is funded under the <rs type="funder">Excellence Strategy of the German Federal Government</rs> and the <rs type="funder">States</rs>. Mausam was funded by <rs type="funder">IBM-IITD AI Horizons network</rs>, grants from <rs type="funder">Google</rs>, and a <rs type="funder">Jai Gupta chair fellowship</rs>. The authors would like to thank <rs type="person">Bahar Mehmani</rs> and <rs type="person">Dennis Zyska</rs> for their valuable input and feedback. Finally, the authors would like to thank <rs type="person">David Kunz</rs>, because "why wouldn't they? Nobody is complaining."</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GGYHNVy">
					<idno type="grant-number">1942124</idno>
				</org>
				<org type="funding" xml:id="_NbgNfvw">
					<idno type="grant-number">ONR N000142212181</idno>
					<orgName type="grant-name">LOEWE Distinguished Chair &quot;Ubiquitous Knowledge Processing</orgName>
				</org>
				<org type="funded-project" xml:id="_yp8EDkX">
					<idno type="grant-number">GU 798/28-1</idno>
					<orgName type="project" subtype="full">PEER</orgName>
				</org>
				<org type="funding" xml:id="_SwN3P2V">
					<orgName type="grant-name">Discovery Grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What sentence are you referring to and why? Identifying cited sentences in scientific literature</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abura'ed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chiruzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Mitkov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Angelova</surname></persName>
		</editor>
		<meeting>the International Conference Recent Advances in Natural Language Processing<address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>INCOMA Ltd</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Peer review: How we found 15 million hours of lost time</title>
		<author>
			<persName><surname>Aje</surname></persName>
		</author>
		<ptr target="https://www.aje.com/arc/peer-review-process-15-million-hours-lost-time/.Online" />
		<imprint>
			<date type="published" when="2013-03">2013. March-2024</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning in citation recommendation models survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kefalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Imran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="page">162</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Artificial intelligence in scientific writing: A friend or a foe?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Altmäe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sola-Leyva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salumets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reproductive BioMedicine Online</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Development of a global registry for peer review in astrophysics</title>
		<author>
			<persName><forename type="first">Amado</forename><surname>Olivo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kerzendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Peer Review Under Review</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">AutomaTikZ: Text-guided synthesis of scientific vector graphics with TikZ</title>
		<author>
			<persName><forename type="first">J</forename><surname>Belouadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feigenblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Herrmannova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shmueli-Scheuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Waard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Proceedings of the Second Workshop on Scholarly Document Processing</title>
		<meeting>the Second Workshop on Scholarly Document Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A metrological perspective on reproducibility in NLP</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1125" to="1135" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-repeatable experiments and non-reproducible results: The reproducibility crisis in human evaluation in NLP</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2023</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3676" to="3687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data statements for natural language processing: Toward mitigating system bias and enabling better science</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="587" to="604" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">a)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wortman Vaughan</surname></persName>
		</author>
		<ptr target="https://neuripsconf.medium.com/introducing-the-neurips-2021-paper-checklist-3220" />
		<imprint>
			<date type="published" when="2021-04">2021. April-2024</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
	<note>Introducing the NeurIPS 2021 Paper Checklist d6df500b. Online</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wortman Vaughan</surname></persName>
		</author>
		<ptr target="https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/.On-line" />
		<title level="m">The NeurIPS 2021 consistency experiment</title>
		<imprint>
			<date type="published" when="2021-04">2021b. April-2024</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Content-based citation recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PolitePEER: does peer review hurt? A dataset to gauge politeness intensity in the peer reviews</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Navlakha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ChatGPT and the future of journal reviews: A feasibility study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dobaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Yale Journal of Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="415" to="420" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nougat: Neural optical understanding for academic documents</title>
		<author>
			<persName><forename type="first">L</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stojnic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scientific peer review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bornmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="197" to="245" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TLDR: Extreme summarization of scientific documents</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cachola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4766" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two huge title and keyword generation corpora of research articles</title>
		<author>
			<persName><forename type="first">E</forename><surname>Çano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Calzolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Béchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mazo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Moreno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Piperidis</surname></persName>
		</editor>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6663" to="6671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Citation-enhanced keyphrase extraction from research papers: A supervised approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Caragea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Bulgarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Godea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das Gollapalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1435" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment analysis of scientific reviews</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020, JCDL &apos;20</title>
		<meeting>the ACM/IEEE Joint Conference on Digital Libraries in 2020, JCDL &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Waard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feigenblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Konopnicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shmueli-Scheuer</surname></persName>
		</author>
		<title level="m">Proceedings of the First Workshop on Scholarly Document Processing</title>
		<meeting>the First Workshop on Scholarly Document Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Toronto Paper Matching System: An automated paper-reviewer assignment system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research Workshop and Conference Proceedings</title>
		<meeting><address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 30th International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">APE: Argument pair extraction from peer review and rebuttal via multi-task learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7000" to="7011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chouldechova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Emerging trends: Reviewing the reviewers (again)</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="257" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feigenblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Herrmannova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shmueli-Scheuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Waard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Proceedings of the Third Workshop on Scholarly Document Processing</title>
		<meeting>the Third Workshop on Scholarly Document Processing<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Specter: Document-level representation learning using citation-informed transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2270" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">MARG: Multi-agent review generation for scientific papers</title>
		<author>
			<persName><forename type="first">M</forename><surname>D'arcy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04259</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ARIES: A corpus of scientific paper edits made in response to peer reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>D'arcy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bransom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.12587</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Some NAACL 2013 statistics on author response, review quality</title>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<ptr target="https://nlpers.blogspot.com/2015/06/some-naacl-2013-statistics-on-author.html.Online" />
		<imprint>
			<date type="published" when="2013-04">2013. April-2024</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">How to disagree well: Investigating the dispute tactics used on Wikipedia</title>
		<author>
			<persName><forename type="first">C</forename><surname>De Kock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3824" to="3837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Calibration with privacy in peer review</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISIT</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1635" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the privacy-utility tradeoff in peer-review data analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Privacy-Preserving Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>PPAI-21 workshop</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Show Your Work: Improved reporting of experimental results</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2185" to="2194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human-in-the-loop AI reviewing: Feasibility, opportunities, and risks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Te'eni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="109" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Understanding iterative revision from human-written text</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raheja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Nakov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Villavicencio</surname></persName>
		</editor>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3573" to="3590" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Yes-yes-yes: Proactive data collection for ACL rolling review and beyond</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dycke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="300" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">NLPeer: A unified resource for the computational study of peer review</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dycke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5049" to="5073" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Overview of PragTag-2023: Low-resource multi-domain pragmatic tagging of peer reviews</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dycke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Workshop on Argument Mining</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Alshomary</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</editor>
		<meeting>the 10th Workshop on Argument Mining<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023b</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The fight against fake-paper factories that churn out sham science</title>
		<author>
			<persName><forename type="first">H</forename><surname>Else</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Noorden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">591</biblScope>
			<biblScope unit="issue">7851</biblScope>
			<biblScope unit="page" from="516" to="519" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Chauvinism</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kienbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">352</biblScope>
			<biblScope unit="issue">6336</biblScope>
			<biblScope unit="page" from="560" to="560" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Citation recommendation: Approaches and datasets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Färber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="375" to="405" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A SUPER* algorithm to optimize paper bidding in peer review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fiez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratliff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="580" to="589" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scim: Intelligent skimming support for scientific papers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kambhamettu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Intelligent User Interfaces</title>
		<meeting>the 28th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="476" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A survey of accepted authors in computer systems conferences</title>
		<author>
			<persName><forename type="first">E</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DOC2PPT: Automatic presentation slides generation from scientific documents</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-02-22">2022. February 22 -March 1, 2022</date>
			<biblScope unit="page" from="634" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Does my rebuttal matter? Insights from a major NLP conference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1274" to="1290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A Bayesian model for calibrating conference review scores</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="http://mlg.eng.cam.ac.uk/hong/unpublished/nips-review-model.pdf" />
		<imprint>
			<date type="published" when="2013-04-04">2013. April 4, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">HedgePeer: A dataset for uncertainty detection in peer reviews</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kordoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries</title>
		<meeting>the 22nd ACM/IEEE Joint Conference on Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Ginther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Masimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Haak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Race, ethnicity, and NIH research awards</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="1015" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ambifc: Factchecking ambiguous claims with evidence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glockner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Staliūnaitė</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vallejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Batching of tasks by users of pseudonymous forums: Anonymity compromise and protection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Measurement and Analysis of Computing Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.09497</idno>
		<title level="m">Peer reviews of peer reviews: A randomized controlled trial and other experiments</title>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Are women prejudiced against women?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans-action</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="28" to="30" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Global State of Peer Review</title>
		<author>
			<persName><surname>Gspr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>Publons</publisher>
			<pubPlace>Wellington</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automatic analysis of substantiation in scientific peer reviews</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rennard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clavel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bali</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10198" to="10216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Math augmentation: How authors enhance the readability of formulas using novel visual design practices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Head</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The diversity-innovation paradox in science</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hofstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="9284" to="9291" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Ppsgen: learning to generate presentation slides for academic papers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Third International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2099" to="2105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Argument mining for understanding peer reviews</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Badugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2131" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">On the detection of reviewer-author collusion rings from paper bidding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jecmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.07860</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Jecmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.11315</idno>
		<title level="m">Tradeoffs in preventing manipulation in paper bidding for reviewer assignment</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A dataset on malicious paper bidding in peer review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jecmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2023</title>
		<meeting>the ACM Web Conference 2023</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3816" to="3826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Mitigating manipulation in peer review via randomized reviewer assignments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jecmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12533" to="12545" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">arXivEdits: Understanding the human revision process in scientific writing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9420" to="9435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Measuring the evolution of a scientific field through citation frames</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Data management plan (DMP) for language data under the new general data protection regulation (GDPR)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kamocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mapelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K ;</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Calzolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Odijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piperidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tokunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="135" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Privacy by design and language resources</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kamocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Calzolari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Béchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Isahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mazo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Moreno</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Odijk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Piperidis</surname></persName>
		</editor>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3423" to="3427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A dataset of peer reviews (PeerRead): Collection, insights and NLP applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kohlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Walker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Stent</surname></persName>
		</editor>
		<meeting>the 2018 Conference of the North American Chapter<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1647" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">DISAPERE: A dataset for discourse structure in peer review discussions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kennard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>O'gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Yelugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Meza Ruiz</surname></persName>
		</editor>
		<meeting>the 2022 Conference of the North American Chapter<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1234" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Assisting human decisions in document matching</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Inherent trade-offs in the fair determination of risk scores</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghavan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05807</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Differential privacy in natural language processing the story so far</title>
		<author>
			<persName><forename type="first">O</forename><surname>Klymenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Meisenbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matthes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Feyisetan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghanavati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mireshghallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Privacy in Natural Language Processing</title>
		<meeting>the Fourth Workshop on Privacy in Natural Language Processing<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Paper matching with local fairness constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kobren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1247" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">A deep neural architecture for decision-aware meta-review generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE Joint Conference on Digital Libraries (JCDL)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="222" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">I Do Not Have Time» -is this the end of peer review in public health sciences?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Künzli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Czabanowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madarasova Geckova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mantwill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Knesebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public Health Reviews</title>
		<imprint>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Revise and Resubmit: An intertextual model of text-based collaboration in peer review</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buchmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="949" to="986" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Scientific literature: Information overload</title>
		<author>
			<persName><forename type="first">E</forename><surname>Landhuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">535</biblScope>
			<biblScope unit="issue">7612</biblScope>
			<biblScope unit="page" from="457" to="458" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Investigating the role of argumentation in the rhetorical analysis of scientific publications with neural multi-task learning models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3326" to="3338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Meza Ruiz</surname></persName>
		</editor>
		<meeting>the 2022 Conference of the North American Chapter<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1875" to="1889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://inverseprobability.com/2014/12/16/the-nips-experiment.Online" />
		<title level="m">The NIPS Experiment</title>
		<imprint>
			<date type="published" when="2014-04">2014. April-2024</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Commensuration bias in peer review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1272" to="1283" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Bias in peer review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cronin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="17" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Matching papers and reviewers at large conferences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nandwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zarkoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raghu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page">104119</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Multi-task peer-review score prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Waard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feigenblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Konopnicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shmueli-Scheuer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Scholarly Document Processing</title>
		<meeting>the First Workshop on Scholarly Document Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="121" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Fast and accurate prediction of sentence specificity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2281" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Summarizing multiple documents with conversational structure for meta-review generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2023</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bali</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7089" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">t-Closeness: Privacy beyond k-anonymity and l-diversity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Data Engineering</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Chirkova</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Dogac</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Sellis</surname></persName>
		</editor>
		<meeting>the 23rd International Conference on Data Engineering</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="106" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vodrahalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01783</idno>
		<title level="m">Can large language models provide useful feedback on research papers? A large-scale empirical analysis</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Collusion rings threaten the integrity of computer science research</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="43" to="44" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jecmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Conitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.05443</idno>
		<title level="m">Testing for reviewer anchoring in peer review: A randomized controlled trial</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">ReviewerGPT? An exploratory study on using large language models for paper reviewing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.00622</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">PaperMage: A unified toolkit for processing, representing, and manipulating visually-rich scientific documents</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Authur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bransom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Candra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zamarron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soldaini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Lefever</surname></persName>
		</editor>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="495" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Structured peer review: Pilot results from 23 Elsevier journals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Malički</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mehmani</surname></persName>
		</author>
		<idno type="DOI">bioRxiv:10.1101/2024.02.01.578440</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">bioRxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Uncovering latent biases in text: Method and application to peer review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4767" to="4775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">CiteSum: Citation text-guided scientific extreme summarization and domain adaptation with limited supervision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10922" to="10935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A market-inspired bidding scheme for peer review paper assignment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lesca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mattei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4776" to="4784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11484</idno>
		<title level="m">Towards automated document revision: Grammatical error correction, fluency edits, and beyond</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Bias in research grant evaluation has dire consequences for small universities</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lavoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Macisaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Villard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">155876</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Task definition and integration for scientific-document writing support</title>
		<author>
			<persName><forename type="first">H</forename><surname>Narimatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dohsaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Higashinaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Minami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feigenblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Herrmannova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shmueli-Scheuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Waard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Scholarly Document Processing</title>
		<meeting>the Second Workshop on Scholarly Document Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<ptr target="https://github.com/acl-org/reviewer-paper-matching.Online" />
		<title level="m">ACL reviewer matching code</title>
		<imprint>
			<date type="published" when="2021-04">2021. April-2024</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Névéol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hwa</surname></persName>
		</author>
		<ptr target="https://hal.science/hal-01660886/file/EMNLPReviewerSurvey2017.pdf.Online" />
		<title level="m">Report on EMNLP Reviewer Survey</title>
		<imprint>
			<date type="published" when="2017-04">2017. April-2024</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Loss functions, axioms, and peer review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Noothigattu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Procaccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1481" to="1515" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Automatically detect statistical reporting inconsistencies to increase reproducibility of meta-analyses</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Nuijten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Polanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research synthesis methods</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="574" to="579" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Parno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Enck</surname></persName>
		</author>
		<ptr target="https://www.ieee-security.org/TC/Reports/2017/SP2017-PCChairReport.pdf.Online" />
		<title level="m">Report on the IEEE S&amp;P 2017 submission and review process and its experiments</title>
		<imprint>
			<date type="published" when="2017-04">2017. April-2024</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Payan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02126</idno>
		<title level="m">I will have order! Optimizing orders for fair reviewer assignment</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Peer-review practices of psychological journals: The fate of published articles, submitted again</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Ceci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="195" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Emerging plagiarism in peer-review evaluation reports: a tip of the iceberg?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Piniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jarić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutsoyiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Kundzewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">CiteTracked: A longitudinal dataset of peer reviews and citations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Dalen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019)</title>
		<meeting>the 4th Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019) co-located with the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="116" to="122" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">An authoritative approach to citation classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pride</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knoth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020, JCDL &apos;20</title>
		<meeting>the ACM/IEEE Joint Conference on Digital Libraries in 2020, JCDL &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Computational politeness in natural language processing: A survey</title>
		<author>
			<persName><forename type="first">P</forename><surname>Priya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Firdaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ekbal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
			<publisher>ACM Computing Surveys</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<author>
			<persName><surname>Propublica</surname></persName>
		</author>
		<ptr target="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" />
		<title level="m">Machine bias</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2024" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Exploring Jiu-Jitsu argumentation for writing peer review rebuttals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Purkayastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="14479" to="14495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01015</idno>
		<title level="m">A randomized controlled trial on anonymizing reviewers to each other in peer review discussions</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Echenique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.17259</idno>
		<title level="m">To ArXiv or not to ArXiv: A study quantifying pros and cons of posting preprints online</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Committees with implicit biases promote fewer women when they do not believe gender bias exists</title>
		<author>
			<persName><forename type="first">I</forename><surname>Régner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thinus-Blanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Netter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schmader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huguet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1171" to="1179" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<title level="m" type="main">GPT-4 is slightly helpful for peer-review assistance: A pilot study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.05492</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">What can we do to improve peer review in NLP</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Augenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1256" to="1262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">A checklist for responsible data use in NLP</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<editor>
			<persName><forename type="first">M.-F</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</editor>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4821" to="4833" />
		</imprint>
	</monogr>
	<note>Just what do you think you&apos;re doing, Dave?</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<ptr target="https://2023.aclweb.org/blog/ACL-2023-policy.Online" />
		<title level="m">ACL 2023 policy on AI writing assistance</title>
		<imprint>
			<date type="published" when="2023-04">2023a. April-2024</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Program chairs&apos; report on peer review at ACL 2023</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karpinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="40" to="75" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">A statistical approach to calibrating the scores of biased reviewers: The linear vs. the nonlinear model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scheuermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multidisciplinary Workshop on Advances in Preference Handling</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Effect of Blinded Peer Review on Abstract Acceptance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Hachinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Krumholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Association</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1675" to="1680" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">What is open peer review? A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ross-Hellauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">F1000Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">588</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>version 2; peer review: 4 approved</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K K</forename><surname>Santu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knipper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guttikonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Akter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freestone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C W</forename><surname>Jr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.15589</idno>
		<title level="m">Prompting LLMs to compose meta-review drafts from peer-review narratives of scholarly manuscripts</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Counterfactual evaluation of peer-review assignment policies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saveski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jecmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Neumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="58765" to="58786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<monogr>
		<title level="m" type="main">A critical examination of the ethics of AI-mediated peer review</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Schintler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Mcneely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Witte</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.12356</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">How to do human evaluation: A brief introduction to user studies in NLP</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vanderlyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1199" to="1222" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Predictive biases in naural language processing models: A conceptual framework and overview</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5248" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Challenges, experiments, and computational solutions in peer review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="76" to="87" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">The role of author identities in peer review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS One</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">286206</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Design and analysis of the NIPS 2016 review process</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tabibian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">MReD: A meta-review dataset for structure-controllable text generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Nakov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Villavicencio</surname></persName>
		</editor>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2521" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Classical peer review: An empty gun</title>
		<author>
			<persName><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Breast Cancer Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">A large scale randomized controlled trial on herding in peer-review discussions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS One</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">287443</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">PeerReview4All: Fair and accurate reviewer assignment in peer review</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">163</biblScope>
			<biblScope unit="page" from="1" to="66" />
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Catch me if I can: Detecting strategic behaviour in peer assessment</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4794" to="4802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Prior and prejudice: The novice reviewers&apos; bias against resubmissions in conference peer review</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2021">2021c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">A novice-reviewer experiment to address scarcity of qualified reviewers in large conferences</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021d</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4785" to="4793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16750</idno>
		<title level="m">A gold standard dataset for the reviewer assignment problem</title>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Racism and censorship in the editorial and peer review process</title>
		<author>
			<persName><forename type="first">D</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gran-Ruaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Faber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Dow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03530</idno>
		<title level="m">ReviewFlow: Intelligent scaffolding to support academic peer reviewing</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Task proposal: The TL;DR challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Völske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gatt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Goudbeek</surname></persName>
		</editor>
		<meeting>the 11th International Conference on Natural Language Generation<address><addrLine>Tilburg University, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="318" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">Social influence among experts: Field experimental evidence from peer review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Teplitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ranub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Grayb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meniettid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Guinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Lakhani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Automatic classification of citation function</title>
		<author>
			<persName><forename type="first">S</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siddharthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tidhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</editor>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">What factors should paper-reviewer assignments rely on? community perspectives on issues and ideals in conference peer-review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Thorn Jakobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Meza Ruiz</surname></persName>
		</editor>
		<meeting>the 2022 Conference of the North American Chapter<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4810" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Reviewer bias in single-versus double-blind peer review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Heavlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">48</biblScope>
			<biblScope unit="page" from="12708" to="12713" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Development of the review quality instrument (RQI) for assessing peer reviews of manuscripts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Godlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Epidemiology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="625" to="629" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">The effectiveness of interventions using electronic reminders to improve adherence to chronic medication: a systematic review of the literature</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vervloet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Linn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Weert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>De Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Bouvy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Dijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="696" to="704" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Potential organized fraud in ACM</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<ptr target="https://medium.com/@tnvijayk/potential-organized-fraud-in-acm-ieee-computer-architecture-conferences-ccd61169370d.Online" />
	</analytic>
	<monogr>
		<title level="j">/IEEE computer architecture conferences</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2020-04">2020. April-2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Fact or fiction: Verifying scientific claims</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Webber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7534" to="7550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">SciFact-Open: Towards open-domain scientific claim verification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4719" to="4734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Your 2 is my 1, your 3 is my 9: Handling arbitrary miscalibrations in ratings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems</title>
		<meeting>the 18th International Conference on Autonomous Agents and MultiAgent Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="864" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Debiasing evaluations that are biased by evaluations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stelmakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10120" to="10128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hope</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14259</idno>
		<title level="m">Scimon: Scientific inspiration machines optimized for novelty</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Simple and effective paraphrastic similarity from parallel translations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4602" to="4608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Open peer review: promoting transparency in open science</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wolfram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hembree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1033" to="1051" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Evaluating evaluation metrics: A framework for analyzing NLG evaluation metrics using measurement theory</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bouamor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Bali</surname></persName>
		</editor>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="10967" to="10982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">PosterBot: A system for generating posters of scientific papers with neural models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="13233" to="13235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Visual summary identification from scientific publications via self-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Glavaš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Research Metrics and Analytics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sidhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14647</idno>
		<title level="m">Meta-review generation with checklist-guided iterative introspection</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">CARE: Collaborative AI-assisted reading environment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zyska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dycke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buchmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Bollegala</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Ritter</surname></persName>
		</editor>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="291" to="303" />
		</imprint>
	</monogr>
	<note>System Demonstrations)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
