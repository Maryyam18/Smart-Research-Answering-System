<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty in Natural Language Processing: Sources, Quantification, and Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mengting</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shiwan</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bingzhe</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">Uncertainty in Natural Language Processing: Sources, Quantification, and Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">32110A06804CAE4489066D2040476A57</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-28T12:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural Language Processing</term>
					<term>Uncertainty Estimation</term>
					<term>Pre-trained Language Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a main field of artificial intelligence, natural language processing (NLP) has achieved remarkable success via deep neural networks. Plenty of NLP tasks have been addressed in a unified manner, with various tasks being associated with each other through sharing the same paradigm. However, neural networks are black boxes and rely on probability computation. Making mistakes is inevitable. Therefore, estimating the reliability and trustworthiness (in other words, uncertainty ) of neural networks becomes a key research direction, which plays a crucial role in reducing models' risks and making better decisions. Therefore, in this survey, we provide a comprehensive review of uncertainty-relevant works in the NLP field. Considering the data and paradigms characteristics, we first categorize the sources of uncertainty in natural language into three types, including input, system, and output. Then, we systemically review uncertainty quantification approaches and the main applications. Finally, we discuss the challenges of uncertainty estimation in NLP and discuss potential future directions, taking into account recent trends in the field. Though there have been a few surveys about uncertainty estimation, our work is the first to review uncertainty from the NLP perspective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>N ATURAL Language Processing (NLP) is a multidisci- plinary field that encompasses computer science, artificial intelligence, and linguistics. Its aim is to develop machines that understand natural language and allow humans to interact with it using natural language. Benefiting from the development of deep neural networks (DNNs), NLP technology has a wide range of applications, including sentiment analysis (SA), machine translation (MT), questionanswering (QA) systems, etc. With the development of pretrained language models (PLMs) <ref type="bibr" target="#b0">[1]</ref>, plenty of NLP tasks can be tackled in a similar manner by sharing the same paradigm. That is to say, various tasks can be simply formulated into a classification, regression, generation, etc., problem. A unified trend is being observed in the development of NLP field, which enables the efficient utilization of PLMs and promotes knowledge transfer across tasks.</p><p>Though NLP field has achieved great success, it relies heavily on neural networks. Such a technique is a black box and depends on probability computation. There must be probable to make mistakes, which is inevitable. Thus, estimating the uncertainty of neural networks becomes a crucial research direction, which measures the reliability and trustworthiness of models. Especially in safety-critical applications like autonomous systems or medical diagnosis, uncertainties in predictions can have severe consequences if not appropriately addressed. Here, we demonstrate the importance of uncertainty through an example in the medi-</p><p>• Mengting Hu and Zhen Zhang are with the College of Software, Nankai</p><p>University. E-mail: mthu@nankai.edu.cn, zhangz@mail.nankai.edu.cn • Shiwan Zhao is an independent researcher. E-mail: zhaosw@gmail.com • Minlie Huang is with the Department of Computer Science, Tsinghua</p><p>University. E-mail: aihuang@tsinghua.edu.cn • Bingzhe Wu is with Tencent AI Lab. E-mail: bingzhewu@tencent.com This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</p><p>User Input: Decision: Can I take Xanax for anxiety? Output Predetermined Answers Based Information: "I don't know, for symptoms and medication instructions it is important to consult a healthcare provider for appropriate recommendations." Generation Answer: Yes, you can take Xanax for anxiety. (Uncertainty: 90%)</p><p>Action: abstain answer/human decision Information: "Domain"(Medical, Uncertainty: 3%) ; "Xanax" (Medicine, Uncertainty: 5%); "Anxiety" (Symptom, Uncertainty: 13%) cal domain. As depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, a user inquires about the potential use of the addictive drug "Xanax" for anxiety, the system fails to differentiate the intensity of anxiety symptoms, resulting in a direct affirmation of its usability. This is clearly deemed inappropriate for patients, considering the grave side effects associated with the drug "Xanax". However, incorporating a probability of uncertainty would aid in determining whether to abstain from the response or defer to the expertise of professionals for a final decision. Some studies have shown that accurately quantifying uncertainty can help identify situations where the model is uncertain, thereby improving the reliability and interpretability of the output <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLP System with uncertainty</head><p>Considering the unified trends of NLP and the essentiality of uncertainty, a systematic review of uncertaintyrelevant NLP and the corresponding solutions is still lacking, which we aim to fill in this survey. Initially, we arise a research question: what are the sources of uncertainty in the NLP field? Here, we naturally connect uncertainty with multiple stages of an NLP system and summarize arXiv:2306.04459v1 [cs.CL] 5 Jun 2023 the sources into input, system, and output. More concretely, natural language input to NLP system is inherently ambiguous and context-dependent, making it difficult to achieve perfect performance and reliability in many NLP tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Then, the system, such as the network initialization, architecture, and interior computation, etc., could introduce randomness, leading to uncertainty. Lastly, the uncertainties of outputs are due to complicating factors, such as a lack of broader knowledge, unreasonable model parameters, or specification of task output.</p><p>In light of the in-depth analysis of uncertainty sources, we further review the literature about the uncertainty quantification. Uncertainty estimation is ubiquitous in DNNs, and a common example is represented by confidence of the network output. Specifically, the Softmax scores obtained from these networks provide a direct means of estimating confidence values that are easily converted to uncertainty scores, e.g. subtracting the confidence from 1. However, the predictions are typically represented as point estimates <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, and mistakes are inevitable. Guo et al. <ref type="bibr" target="#b9">[10]</ref> point out that DNNs tend to exhibit excessive confidence, making their confidence scores inaccurate. This is problematic because under-/over-confident in the network can lead to wrong decisions and actions based on overconfident predictions. To address this challenge, researchers have developed various uncertainty estimation methods for DNNs. This survey categorizes uncertainty estimation methods into three groups based on different modeling approaches: (1) calibration confidence-based methods, (2) sampling-based methods, and (3) distribution-based methods. These methods go beyond traditional confidence level measurements to quantify the uncertainty in model predictions, identify situations where NLP systems are uncertain about outputs, and offer insights into model behavior. Throughout the survey, we discuss the features and challenges associated with each of these estimation methods.</p><p>Recently, the applications of uncertainty estimation in NLP are diverse and increasing. In this survey, we mainly divide the applications into three categories, including <ref type="bibr" target="#b0">(1)</ref> data filtering and action guidance, which is to select the required data according to the uncertainty and take the next action. In typical active learning, filter the data through the uncertain estimation of the teacher model, and guide the student network to train; another example is the detection of out-of-distribution data. (2) Improve the performance and efficiency of the system based on uncertainty, (3) Output quality assessment, such as the quality assessment of machine translation and understanding whether the answer of the QA system is trustworthy.</p><p>The challenges of uncertainty estimation in applications depend on the problem setting, including the paradigm of the NLP model and the type of task (e.g., classification, generation, regression, etc.). When observing the challenges faced by the three applications mentioned above, we considered different paradigms for classification in deterministic estimation methods in this survey. Recently, with the emergence of large pre-trained models such as T5 <ref type="bibr" target="#b10">[11]</ref> and the GPT family <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, which can capture linguistic patterns and knowledge from massive text data. The capabilities of NLP models in various paradigms have greatly improved. However, the complexity of large models makes it difficult to understand the models themselves, which include highdimensional spaces, variable lengths of generated text, and expressions of uncertainty for interpretation of results. These will be described in detail in the survey <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Overall, the main purpose of this survey paper is to systematically review the advances and challenges of uncertainty in the NLP field. To the best of our knowledge, our paper is the first to summarize this topic. Concretely, considering recent trends and paradigms in NLP, we provide a categorization of uncertainty sources according to the stages of NLP systems. Besides, a comprehensive review of the uncertainty estimation (a.k.a. quantification) technique in recent years is further provided. We also summarize the applications of uncertainty in NLP. Finally, we conclude the challenges based on the characteristics of text data and the NLP paradigms, highlighting future challenges and potential trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Organization of The Survey</head><p>The rest of the survey is organized as follows: In Sec 2, we introduce two kinds of uncertainty backgrounds in deep learning and illustrate the main sources of uncertainty in NLP systems in Fig. <ref type="figure">2</ref>. In Sec 3, we classify uncertainty estimation techniques based on their modeling approach. We also abstract the insights behind each category of technology and provide a comparison of different technologies. Following this, we present commonly used uncertainty evaluation metrics in NLP. Then, in Sec 4, we summarize the application directions of uncertainty in NLP under the three major categories and sort out the detailed literature of specific applications under each category. We then discuss the current challenges of uncertainty estimation and potential future research directions in Sec 5 and conclude this survey in Sec 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">UNCERTAINTY SOURCES</head><p>In this section, we aim to analyze the uncertainty sources of NLP from the input, system, and output stages, respectively. In advance, we review the theoretical background of uncertainty definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Theory Background of Uncertainty</head><p>In the field of machine learning, uncertainty is commonly divided into aleatoric uncertainty and epistemic uncertainty ( <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>). In general, these two types of uncertainty can be summarized as follows:</p><p>• Aleatoric Uncertainty It is also known as data uncertainty, which refers to the uncertainty inherent in data due to its randomness or noise. This type of uncertainty is irreducible, meaning it cannot be eliminated through model improvements or tuning. It can arise from a variety of sources, such as noisy observations, overlapping classes, ground truth errors, inherent randomness, or other factors that are not entirely predictable.</p><p>• Epistemic Uncertainty It is also known as model uncertainty, which is reducible uncertainty that arises from a lack of knowledge or understanding about the NLP System Inter action Uncer tainty Sour ces Input System Unknown Query Language Intrinsic System Model Structure Parameter Scale Remar k Epistemic Uncertainty Aleatoric Uncertainty Combined Uncertainty Inconsistency Context Ambiguity OOV Distraction Noise Mapping Model Training Dataset Hyperparameters Annotation Preprocessing Bias Output Classification Paradigm Granularity Number of Categories Regression Paradigm Output Representation Sequence Labeling Paradigm Dependencies Boundary Relationship Generative Paradigm Language Quality Search Algorithm Semantic Pipeline End-to-End Architecture</p><p>Fig. <ref type="figure">2</ref>. Illustration of sources of uncertainty. The figure includes the sources of uncertainty in the interaction of the NLP system. We start from the three processes of Input, System, and Output to analyze the possible causes of each uncertainty. It is worth noting that these three parts are interrelated. As the query passes through the NLP system, due to the combination of neural networks and different task specifications in complex ways, the type of uncertainty in the output prediction becomes complicated, including both the aleatoric and epistemic uncertainty, which are hard to decompose. Thus, we refer to it as combined uncertainty.</p><p>model itself. This can include uncertainty about the model's structure choice, and parameters, which can be reduced by increasing the amount and quality of training data. Epistemic uncertainty can also result from out-of-distribution examples, such as new languages or domains, and may be caused by model structure errors or training procedure errors.</p><p>It is worth noting that there is no clear distinction between epistemic uncertainty and aleatoric uncertainty, and these types of uncertainty can even be mutual <ref type="bibr" target="#b16">[17]</ref>. In particular, it may not always be clear which type of uncertainty is dominant in a given NLP system, and the two types of uncertainty can interact in complex ways, thus making combinations complex and difficult to decouple. Nonetheless, it is important to understand the source and classification of uncertainty, e.g., in MT, decoupled uncertainty can be used to infer whether predictions originate from noisy and ambiguous references, or out-of-distribution examples or noisy annotations <ref type="bibr" target="#b17">[18]</ref>.</p><p>To better understand the sources of uncertainty in NLP systems, it is necessary to incorporate NLP usage scenarios. To this end, we consider human-machine interactions in NLP tasks to mine factors that may contribute to uncertainty. Ultimately, we have determined three main sources of uncertainty: (1) Input text entered by a user, (2) System composed of models, and (3) Output returned to the user. In the following sub-parts, we will discuss these three sources. Meanwhile, in Fig. <ref type="figure">2</ref>, we provide a detailed source overview. It's worth emphasizing that three sources tend to be interconnected, and addressing one may help to mitigate uncertainty in the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Uncertainty Source from Input</head><p>During the usage of an NLP system, a user would feed input to it, expecting to obtain processed results or interactive feedback. Due to the language's intrinsic ambiguity and unknown query, the input itself contains uncertainty. Formally, assuming that the model parameters W have been trained on a dataset D = {(x 1 , y i )} N i=1 , and there exists a correct and specific hypothesis space H, if an input text falls within the hypothesis space H, then the input adheres to the data distribution. However, if an input contains inherent uncertainty or noise of the language, it may fall on the boundary of the hypothesis space or even outsides <ref type="bibr" target="#b18">[19]</ref>. In such cases, the system is prone to making incorrect judgments. Consequently, it is necessary to capture and quantify this resulting uncertainty. As depicted in the upperleft of Fig. <ref type="figure">2</ref>, we distinguish common sources of uncertainty in inputs from two perspectives, including the language intrinsic and unknown query, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Language Intrinsic</head><p>Language inherently contains ambiguity that leads to uncertainty <ref type="bibr" target="#b19">[20]</ref>. That is to say, ambiguity is a natural feature of language that arises from inconsistencies and contextual factors in the text <ref type="bibr" target="#b20">[21]</ref>. For instance, the word "apple" has various meanings when appearing in different contexts, i.e. "I like to eat apple" and "Steven Jobs built up Apple". Inconsistencies can take the form of incomplete sentences or expressions <ref type="bibr" target="#b20">[21]</ref>, while contextual semantics encompass the completeness and accuracy of information expression, as well as social and cultural aspects and differences between the unique context of text production (including features of space, time, authorship, etc.) and multiple interpretation contexts <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Natural language noise contains out-of-vocabulary (OOV) and distraction. OOV indicates spelling errors or emerging words that are unseen tokens for a running NLP system. These would result in unsureness. Besides, distracting refers to the presence of some words in a sentence that are not related to the meaning of the sentence. This kind of noise is ubiquitous in informal texts <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. For example, Liu et al. <ref type="bibr" target="#b24">[25]</ref> in entity recognition find that a part of the original sentence retains enough words to express the relationship, and the rest has many irrelevant words that can be regarded as noise that may hinder the performance of the extractor. Kadavath et al. <ref type="bibr" target="#b25">[26]</ref> set different prompt templates, such as helpful, incorrect, and distracting hints, and find that distracting hints in natural language generation (NLG) also lead to deviations in the generated answers.</p><p>As mentioned above, natural language is inherently complex, ambiguous, and context-dependent. It can be expressed in different ways to convey the same ideas or concepts. Therefore, the mapping relationship in NLP is also a source of uncertainty <ref type="bibr" target="#b22">[23]</ref>. This complexity can result in multiple input-output mapping scenarios, such as one-toone and many-to-one mapping. In such cases, the model must select an output from several possible ones, which can be challenging and uncertain. For instance, in MT, the same source sentence can have multiple semantic equivalent translations, resulting in multiple nature of machine translation learning tasks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. Similarly, Text-to-SQL needs to deal with many-to-one cases, indicating that multiple input texts correspond to the same SQL query <ref type="bibr" target="#b27">[28]</ref>. This is also caused by the uncertainty of natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">System Unknown Query</head><p>In the application of the actual world, systems usually encounter an unknown query differently distributed from training data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>. Systems tend to produce unreliable or even catastrophic predictions, thereby harming the trust of users. The uncertainty of the field is derived from the model that the model cannot explain the out-of-distribution (OOD) sample due to the lack of external knowledge. The source of this uncertainty is the input data extracted with the unknown subspace. Although DNNs can extract the knowledge in the domain from the domain migration sample, they cannot extract the knowledge samples in the domain from the external sample <ref type="bibr" target="#b15">[16]</ref>. For example, QA systems deployed in search engines or individual assistants require elegantly processing OOD input, because users usually propose a problem that is not within the scope of system training distribution <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. Another example is to make entailment judgments for breaking news articles in search engines <ref type="bibr" target="#b30">[31]</ref>. If an input is not acceptable, the model needs to acknowledge its uncertainty and turn to humans or better (but more costly) models <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Uncertainty Source from System</head><p>The sources of uncertainty induced by NLP systems mainly originate from model blocks, which are usually introduced in the design and training of neural networks. The design of DNNs involves explicit modeling of the network architecture and its stochastic training process. The assumptions made about the problem structure, based on the network design and training, are referred to as inductive biases <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Model Structure</head><p>Since neural networks are manually designed, their structures are also a source of uncertainty. In other words, changing the structure of networks might alter decisionmaking, bringing randomness to model training and inference stages. Here we discuss from the views of architecture and parameter scale.</p><p>In an end-to-end model, many factors are relevant to model architecture, such as the dimension of hidden states, and interior neuron connections, directly affect the performance of the network and thus introduce uncertainty <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> When studying complex AI systems, it needs to take into account the predictions of several independent models in downstream tasks <ref type="bibr" target="#b34">[35]</ref> due to the effect of pipeline propagation. The cost and risk of any decision-making process that produces different errors need to be balanced. This creates uncertainty when confidence in one module's predictions is allowed to have an impact on the next module. For this reason, if the uncertainty and confidence scores can be reliably interpreted as probabilities, the rules of probabilistic calculus can be applied, allowing one system to abort the decision if its predictions are not confident enough <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. This is a useful property in many situations. For example, in the prediction of an information extraction system <ref type="bibr" target="#b5">[6]</ref>, there are locations where the information extraction system extracts useful information and uses confidence to check whether the extracted information is meaningful. After this, another model makes a final prediction of the extracted value of the field.</p><p>The parameter scale is a critical factor that contributes to uncertainty in a fixed model structure <ref type="bibr" target="#b38">[39]</ref>. The size of a model can have a significant impact on the level of uncertainty. In particular, larger models tend to possess a greater number of parameters and increased complexity. This enables them to better capture the intricacies of the training data, leading to reduced error on the training set. However, the larger parameter count also renders these models more susceptible to overfitting <ref type="bibr" target="#b9">[10]</ref>, potentially causing poor performance when presented with unseen data. On the other hand, smaller models have fewer parameters and lower complexity, making them more prone to underfitting the training data. Consequently, these models may exhibit higher error rates on the training set but have the potential to generalize better on previously unseen data, thereby potentially reducing uncertainty. It is important to recognize that the relationship between model size and uncertainty is multifaceted, and a comprehensive evaluation should consider various factors such as model complexity, dataset size and characteristics, and the regularization techniques employed during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Model Training</head><p>During the model training, we distinguish uncertainty sources into the dataset and hyperparameter views, respectively. Initially, to learn an intelligent system, high-quality training data is essential. Yet due to the different language abilities of annotators, texts in the dataset pose various challenges for them <ref type="bibr" target="#b39">[40]</ref>. As a result, this directly affects the correlation between samples and corresponding labels. Uncertainty arises whenever there are multiple possible interpretations of the data, but the knowledge or information to definitively choose one of them is not available <ref type="bibr" target="#b40">[41]</ref>. Then, uncertainty may be part of the pre-processing stage. For example, NLP tools used for pre-processing can introduce uncertainty <ref type="bibr" target="#b41">[42]</ref>. More concretely, Part-of-speech taggers are often trained on data from historical periods. Yet, recent data is often not available. Making results for recent data is possibly unreliable, leading to uncertainty, and this uncertainty is usually irreducible. Furthermore, Bender et al. <ref type="bibr" target="#b42">[43]</ref> discuss ethical issues raised by PLMs, including issues of bias and fairness. They argue that language models encode biases and assumptions in the training data, which perpetuate social inequalities. This highlights sources of uncertainty related to the ethical implications of PLMs, and the need to consider them carefully for transparency.</p><p>The setting of hyperparameters, such as batch size, learning rate, and epoch number, is also random, resulting in different local optimal solutions and different final models <ref type="bibr" target="#b15">[16]</ref>. Dodge et al. <ref type="bibr" target="#b43">[44]</ref> study the fine-tuning process of PLMs and find that hyperparameters and tuning such as weight initializations, data orders, and early stopping are critical to achieving high performance on downstream tasks. This highlights that under the large model, fine-tuning the model requires more strategies and hyperparameter requirements and adjustments, which will also introduce new uncertainty in the model framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Uncertainty Source from Output</head><p>Since making mistakes is inevitable for an NLP system, its output is not fully trusted, i.e. presenting uncertainty. Here we summarize the uncertainty of the output according to the paradigms and the nature of the task, namely classification, sequence labeling, generation, and regression paradigms. It is worth noting that although the task outputs are quite different, their sources of uncertainty are often combined and complex. That is to say, uncertainties sourced from the input text and model itself will affect the output. Therefore, in the right part of Fig. <ref type="figure">2</ref>, we do not specifically define whether the uncertainty of output is epistemic or aleatoric uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Classification Paradigm</head><p>The output of the classification paradigm is usually connected with the number and granularity of categories. Firstly, as the number of categories increases, the classification task becomes more difficult. There may be more errors or misclassification space, which enlarges uncertainty. This is because models may have difficulty distinguishing between similar categories, or be more error-prone due to the inherent ambiguity of the classification task. Secondly, the classifying granularity (e.g., classifying at words or sentences level, defining hierarchical or multi-label categories) may introduce uncertainty. More concretely, wordlevel classification may have variability. For example, a word like "Washington" could be classified as a person or place depending on the context, and this classification itself may be vague or indeterminate. Classification at a coarser level (e.g. classifying entire sentences or documents) depends on more factors (e.g. contribution of each token, contextual semantics, length of sentence/document, etc.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Sequence Labeling Paradigm</head><p>Sequence labeling aims to assign a tagger to each token in the given input text. It needs to consider contexts to determine the dependencies of label sequences. Specifically, for sequence dependencies, the labels assigned to elements can depend on the labels assigned to adjacent elements in the sequence. This can lead to non-determinism because (1) due to the inherent non-determinism of natural language itself, the label assigned to an element may be ambiguous, which needs to be determined according to the context and neighboring elements in the sequence <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>. <ref type="bibr" target="#b1">(2)</ref> The labels assigned to adjacent elements in the sequence should be consistent and follow a certain pattern. However, this can be challenging as the model may struggle to capture and model complex dependencies and relationships between adjacent elements <ref type="bibr" target="#b46">[47]</ref>.</p><p>In addition, the boundary, e.g. the start and end positions of entities in named entity recognition (NER) tasks, is also crucial. Correct entity boundaries are effective in mitigating error propagation in entities that are linked to knowledge bases <ref type="bibr" target="#b47">[48]</ref>. Some studies consider entity boundary detection as a subtask in NER <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>. In summary, analyzing the prediction uncertainty in sequence labeling can be roughly divided into boundary uncertainty and boundary entity classification uncertainty. Both of these are inseparable from boundary detection and are dependent on the sequence. This analysis increases the rationality and reliability of the output results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Generation Paradigm</head><p>A more challenging output is the generation paradigm, where the uncertainty of the prediction is affected by uncontrollable factors. The output space of generating natural language has O(|T | N ) dimensions. For example, MT models have hundreds of millions of parameters, and the search space is exponentially large, which poses great challenges for search algorithm. We usually observe only one reference for a given source sentence, leading to uncertainty in the final output. At present, tools and strategies can be borrowed and combined from machine learning and statistics <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>. Beam search is an effective search strategy, but some external uncertainty (such as the quality of training data) will lead to large beam performance degradation <ref type="bibr" target="#b22">[23]</ref>.</p><p>One of the goals of NLG is to ensure that the generated text conveys the intended semantic content of the sentence. However, in decision-making tasks that rely on NLG, e.g. QA system <ref type="bibr" target="#b3">[4]</ref>, it is crucial to ensure invariance to the output space, but this is often not explicitly specified in the model. The meaning of the generated text is especially important in terms of its beliefs. While a system may be reliable even when generating output using many different expressions, it may be less reliable when answering questions with inconsistent meanings.</p><p>Another key purpose of NLG is to ensure the language quality. which is typically evaluated based on several factors, including adequacy, fluency, readability, and variation <ref type="bibr" target="#b52">[53]</ref>. A recent example of an effective strategy is reinforcement learning from human feedback (RLHF) <ref type="bibr" target="#b53">[54]</ref>. But it has been found to be wrongly calibrated <ref type="bibr" target="#b25">[26]</ref>. This is not surprising because RL fine-tuning tends to collapse language model predictions to the behavior that yields the highest reward, which raises the inherent problem of whether the model's output is biased or unfair. Furthermore, the length of the input text can vary widely <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56]</ref>, leading to uncertainty in the output. This particularly exists for longer sequences, where the joint likelihood decreases due to the conditional independence of labeling probabilities <ref type="bibr" target="#b3">[4]</ref>. Rare words also contribute to this problem because they have small probabilities and therefore occur less frequently in the sequence <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b56">57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.4">Regression Paradigm</head><p>The uncertainty source in the classification output can be easily explained. A discrete probability distribution over the classifier's output is naturally provided through a softmax layer. However, in the regression paradigm, the output is a single numerical value in continuous target spaces <ref type="bibr" target="#b57">[58]</ref>. The representation of the output becomes important, specifically the range of the output. For example, FLORES <ref type="bibr" target="#b58">[59]</ref> divides 0-100 points into ten intervals, each representing different translation quality. In semantic text similarity assessment <ref type="bibr" target="#b59">[60]</ref>, the objective is to predict a similarity score for a sentence pair (S 1 , S 2 ) within the range [0, 5]. A score of 0 indicates complete dissimilarity, while a score of 5 signifies the sentences are equal in meaning. Calibrated regression <ref type="bibr" target="#b60">[61]</ref> extends the calibration method of classification to regression. It applies the obtained algorithm to the Bayesian deep learning model and calibrates the confidence score in the [0, 1] interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UNCERTAINTY ESTIMATION</head><p>Summarizing the sources of uncertainty in NLP systems can help reduce uncertainty, however, accurately distinguishing uncertainty types is challenging <ref type="bibr" target="#b15">[16]</ref>. In this section, We first introduce the background of uncertainty estimation techniques, Then, As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, according to the modeling characteristics, we propose a taxonomy of uncertainty estimation in NLP, and the specificity of linguistic uncertainty modeling. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Uncertainty Estimation Background</head><p>Modern neural networks are parameterized by a set of model weights W , providing a formalization of uncertainty in the BNN case, and these are sufficient statistics ω = (W i ) L i . For a given dataset D = {x i , y i } N i=1 with distribution p(x, y), for a classification model P (y * = ω c |x * , D) trained on D, the distribution on prediction y * is described as:</p><formula xml:id="formula_0">P (y * = ω c |x * , D) = P (y * = ω c |x * , θ) (a) Data p(θ|D) (b) Model dθ,<label>(1)</label></formula><p>where the aleatoric uncertainty is formalized as the posterior probability P (y * = ω c |x * , θ) of over class labels for a given parameter, while p(θ|D) is used as the posterior distribution of the model parameters, describing the uncertainty of the model parameters, given a dataset D.</p><p>However, p(θ|D) is intractable using Bayes' rule posterior distributions, and it is necessary to use variational inference methods to achieve p(θ|D) ≈ q(θ) <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>. In the uncertainty estimation method, three ways are mainly considered, which will be placed in the next subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Uncertainty Estimation Methods</head><p>After reviewing the uncertainty estimation methods commonly used in recent NLP tasks, we generally divide them into three types, namely, calibration confidence-based methods, sampling-based methods, and distribution-based methods. We provide the usage and hierarchical classification of these methods in Fig. <ref type="figure" target="#fig_2">4</ref>, and below we summarize from each of these three types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Calibration Confidence-based Methods</head><p>Calibration methods aim to correct the reliability of the uncertainty estimates provided by a model. The basic idea is to measure the accuracy of the predicted probabilities against the true probabilities. As shown in Fig. <ref type="figure" target="#fig_3">5</ref>, if the predicted probabilities are well calibrated, then they will accurately reflect probabilities, and the model will be considered reliable. It is worth noting that the probability distribution can be obtained by a single forward pass of the network, also including some post-processing methods. We introduce below two commonly used methods of expressing uncertainty, as well as calibration methods in NLP research.</p><p>Softmax Response. Confidence method is a widely used uncertainty estimation method. It is based on the idea that the predicted category with the highest probability is the most likely category, and the uncertainty can be represented by Softmax Response (SR). There are also methods based on the difference (SD) between the top two values of the Softmax output, with a slight difference indicating uncertainty and a significant difference indicating confidence. Given an input x (i) , the uncertainty describe as:</p><formula xml:id="formula_1">u SR (x (i) ) = 1 -max c∈C p(y (i) = c|x (i) ).</formula><p>(2)</p><formula xml:id="formula_2">u SD (x (i) ) = u SR (x (i) ) -max c2∈C c2̸ =c p(y (i) = c 2 |x (i) ).<label>(3)</label></formula><p>Entropy-based. Prediction entropy, which can be understood as the total uncertainty of the output distribution. When the certainty of the output distribution is higher, the entropy value is lower. Maximum entropy is reached when all outcomes have equal probability. In other words, entropy is highest value when the model is unable to distinguish between different outcomes. The predictive entropy at point x (i) is equal to the conditional entropy of the output random variable Y :</p><formula xml:id="formula_3">P E(x (i) ) = -p(y|x (i) ) ln p(y|x (i) )dy.<label>(4)</label></formula><p>In NLP research, the entropy value can represent a variety of uncertain information, such as the N-grams word entropy value as a heuristic feature to select domain adaptation data <ref type="bibr" target="#b64">[65]</ref>, the entropy value at the sentence level can represent semantic uncertainty <ref type="bibr" target="#b3">[4]</ref>, The composition of information entropy in NLP is diverse and complex, and these issues need to be considered when using entropy values.</p><p>Recently, the problem of overconfidence can arise in DNNs <ref type="bibr" target="#b9">[10]</ref>, often requiring calibration to obtain reliable confidence. A simple but effective calibration method is Temperature scaling (TS), which was originally introduced into the machine learning community as a tool for knowledge distillation <ref type="bibr" target="#b65">[66]</ref>. Specifically, TS helps introduces a temperature parameter T &gt; 0 and generates a calibrated prediction vector by mapping:</p><formula xml:id="formula_4">q (i) = σ SM ( z (i) T ),<label>(5)</label></formula><p>where σ SM (z) = e z / K k=1 e z k refers to the logits z pass the Softmax function. With T = 1, q (i) = p(i) , when T &gt; 1, the entropy of q (i) increases, which helps reduce confidence s (i) and combat overconfidence, and larger T will make the distribution flatter. Similarly, T &lt; 1 reduces entropy and increases confidence, making the distribution sharper to help with underconfidence predictions. The temperature parameter T is trained with NLL on the validation set. TS acts as a post-processing calibration method, treating each prediction independently without explicitly taking contextual information into account. Therefore, it's important to consider whether token-level or sequence-level calibration is more appropriate for the specific NLP task at hand. In addition, there are some implicit calibration methods to combat overconfidence, such as Label smoothing and Re-weighting techniques <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref>. Label smoothing is a regularization technique that replaces the one-hot target vector y (i) with a weighted combination of targets that have a uniform distribution. The amount of smoothing is controlled by an α parameter. In the K -class setting, the true class represented by the value 1 is replaced by 1 -α, and the other classes are assigned the value of α K-1 . Label smoothing has been shown to have several benefits for the model predictions. It implicitly calibrates model predictions similar to TS and does not require post-operation quantification. Similarly, reweighting techniques such as Focal loss can also reasonably reduce the sharpness, and thus improve the calibration effect and performance <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b69">70]</ref>. By increasing the entropy of the target distribution, label smoothing and Focal loss prevent the model from producing extremely lowentropy predictions.</p><p>Conformal Prediction (CP) <ref type="bibr" target="#b70">[71]</ref> is a framework for constructing calibrated predictive models that provide a measure of confidence for each prediction. When CP is formulated in terms of a set of predictions C(X n+1 ), it provides finite-sample, distribution-free guarantees for events in which C contains Y n+1 . CP is based on hypothesis testing, where for a given input x and possible output y, a statistical test is performed to accept or reject the null hypothesis that the pairing (x, y) is correct. It can be seen as a post-hoc calibration method, which means that it does not require access to the training data during the calibration process. Instead, it uses the model's predictions on a validation set or a hold-out set to construct a set of prediction regions or intervals.</p><p>Challenging: How to solve the trade-off between accuracy and calibration? As increasing the accuracy may result in overfitting and poor calibration, while increasing the calibration may result in underfitting and poor accuracy. Finding the optimal balance requires careful consideration of the model complexity and the application requirements. Furthermore, NLP tasks often involve making predictions at different levels of granularity, such as sentencelevel, document-level, or token-level predictions. Uncertainty quantification of texts with different granularities is also worth thinking about.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sampling-based Methods</head><p>Sampling methods provide a representation of the target random variable, which can be divided into parametric and predictive sampling, from which implicit conditional distributions of the posterior model parameters can be derived. Bayesian variational inference approximates the posterior distribution of model parameters. It optimizes a variational objective function. This method estimates uncertainty in model predictions by sampling from the approximate posterior distribution. For instance, variational inference methods approximate intractable posterior distributions <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b71">72]</ref> by optimizing a family of tractable distributions. A commonly used method is MC dropout <ref type="bibr" target="#b72">[73]</ref>, which involves randomly dropping out some neurons in the network during testing. This technique results in a different prediction for each dropout configuration. These methods are based on Markov Chain Monte Carlo and further extensions <ref type="bibr" target="#b73">[74]</ref>. Furthermore, sampling-based methods also include ensemble methods. These methods involve training multiple models with different initializations or architectures and combining their predictions to make a final prediction.</p><p>Monte Carlo dropout (MCD) is the typical sampling method. Specifically, the MCD-based method performs M stochastic forward passes with activated dropout, explicit ensembling to approximate integral Eq. 1:</p><formula xml:id="formula_5">P (y (i) |x (i) , D) ≈ 1 M M i=1 P (y (i) |x (i) , θ(i) ), θ(i) ∼ q(θ)<label>(6)</label></formula><p>Each P (y</p><formula xml:id="formula_6">(i) |x (i) , θ<label>(i)</label></formula><p>) sampled from the distribution q is a categorical distribution of class labels on a given input x. It visualizes as a point on the simplex <ref type="bibr" target="#b7">[8]</ref>.</p><p>Therefore, these methods construct an implicit conditional distribution simplex by sampling, and the characteristic is that whether a certain answer can be determined through the sharpness of the simplex, that is, the expression containing uncertainty. Given such a collection of distributions, it is expected that the entropy of the distribution P will indicate the uncertainty of the prediction. It should be noted that although the uncertainty expressed by entropy is effective, it cannot be directly distinguished as aleatoric uncertainty or epistemic uncertainty. To address this, measures such as mutual information can be used to estimate prediction uncertainty due to epistemic uncertainty <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76]</ref>.</p><p>In addition, Bayesian active learning by disagreement (BALD; <ref type="bibr" target="#b76">[77]</ref>) can be seen as a way to quantify uncertainty based on MCD:</p><formula xml:id="formula_7">u BALD = - K k=1 p c log p c + 1 M c,m p t c log p t c<label>(7)</label></formula><p>From a Bayesian perspective, MCD is a way of approximating Bayesian inference in neural networks <ref type="bibr" target="#b72">[73]</ref>. It can be understood as placing a prior distribution on the weights of the neural network, and then using dropout to sample from the posterior distribution.</p><p>Ensemble Methods. An alternative approach to Bayesian approximation involves creating ensembles of multiple independent deterministic neural networks. The technique involves training M neural network classifiers, with different models randomly initialized and optimized individually, and combining their networks outputs to form a single classification function f (x) : X → Y , For example, this can be achieved by simply averaging the predictions of the members:</p><formula xml:id="formula_8">f (x) := 1 M M i=j f j (x).<label>(8)</label></formula><p>The premise behind this is to leverage diverse perspectives to improve the generalization of the model, on the basis that a group of decision-makers is typically more effective than a solitary one. Ensembles have been found to be particularly effective in uncertainty estimation networks in neural systems. Moreover,to measure different sources of uncertainty, mutual information (MI; <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b76">77]</ref>) between ensemble predictions and their parameters is often used in NLP models <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b79">80]</ref>. Furthermore, MCD can be thought of as an ensemble technique that simulates different models using dropout <ref type="bibr" target="#b33">[34]</ref>, wherein a set of independently trained networks with their own weights are generated. By aggregating the predictions from these networks, we can obtain more accurate and robust predictions. In addition, recent work has achieved "better" uncertainty by combining the expressive power of ensemble methods with MCD <ref type="bibr" target="#b80">[81]</ref>. It has been demonstrated that sampling-based methods exhibit inherent calibration and robustness to unknown data representations <ref type="bibr" target="#b81">[82]</ref>.</p><p>Challenging: How to design methods to reduce time and computational cost? A set of models with varying initializations has the potential to explore multiple modes of the parameter space <ref type="bibr" target="#b82">[83]</ref>. Reducing sampling time and computational cost is a challenging task in sampling-based uncertainty estimation models. It is important to maintain the diversity of individual models while achieving these goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Distribution-based Methods.</head><p>As shown in Fig. <ref type="figure" target="#fig_1">3</ref> </p><formula xml:id="formula_9">Dir(p (i) |α (i) ) = 1 B(α (i) ) C c=1 p (α (i) c -1) c ,<label>(9)</label></formula><p>where B(α (i) ) is the C-dimensional multinomial beta function. Specifically, the P (y (i) |x (i) , θ) obtained by the neural network is a categorical distribution Cat(p (i) ), which is a point on the simplex, and the Dirichlet distribution is a prior distribution over categorical distribution, which is parameterized by its concentration parameters, and the epistemic uncertainty on x (i) can be expressed by q (i) = Dir(α (i) ), so it can be interpreted as the distribution of the categorical distribution.</p><formula xml:id="formula_10">p (i) c = α (i) c α (i) 0 , y (i) = arg max c∈C p (i) c .<label>(10)</label></formula><p>There are currently some studies using the Dirichlet distribution to quantitative uncertainty <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86]</ref>. The advantage is that the network can obtain uncertainty estimates once forward. In addition, this parameterization allows the calculation of closed-form classical uncertainty measures <ref type="bibr" target="#b8">[9]</ref>, such as the differential entropy of Dirichlet distribution m</p><formula xml:id="formula_11">(i) diffE = H(Dir(α (i) )) or mutual information m (i) MI = I(y (i) , (p (i) )) [8].</formula><p>Moreover, constructing distributions by modeling networks is also a deterministic approach. For example, modeling a Dirichlet distribution on class probabilities rather than the point estimate of a Softmax output <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85]</ref>.</p><p>Gaussian-based Uncertainty Models (GBU). Parameterizing uncertainty information using the mean and variance of a Gaussian distribution is a common approach used in various fields. Specifically, referring to µ(x) and σ(x) as functions parameterized by W , the output mean and standard deviation are computed for input x, assuming y ∼ N (µ(x), σ(x) 2 ) for the data generating process in the regression setting, and for the logits vector z in the classification setting samples are then converted to probabilities using a Softmax operation. This process can be described as The process is described as:</p><formula xml:id="formula_12">u ∼ N (µ(x), σ(x) 2 ), p = Softmax(u), y ∼ Categorical(p).<label>(11)</label></formula><p>This method models uncertainty as a Gaussian distribution, where the mean represents the most likely value or prediction, and the variance represents the level of uncertainty or variability. By embedding words as Gaussian distributional potential functions in an infinite dimensional function space, Vilnis et al. <ref type="bibr" target="#b86">[87]</ref> not only maps word types to vectors, but also to soft regions in space, allowing uncertainty modeling of meaning and metaphor and providing a rich geometry of the latent space.</p><p>In NLP systems, Gaussian distributions and Mahalanobis distance can be used together in uncertainty estimation techniques <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b88">89]</ref>. Gaussian distributions are used to model variables, while Mahalanobis distance are used to compare simulated distributions with actual observed data and quantify the distance between them. For example, measure the Mahalanobis distance between a test instance and the closest class conditional Gaussian distribution to estimate uncertainty:</p><formula xml:id="formula_13">u M D = min c∈C (h (i) -µ c ) T Σ -1 (h (i) -µ c ),<label>(12)</label></formula><p>where h (i) denotes the hidden representation of the i -th sample. Furthermore, the Bayesian neural network can approximate the distribution of the function by learning the parameter distribution of some models, and the Gaussian processes (GPs) non-parametric Bayesian model directly uses the function f to approximate the distribution of the function <ref type="bibr" target="#b89">[90]</ref>, which can measure the uncertainty of the model. Formally, a Gaussian process can be defined as a set of random variables in which any limited number of variables follow a joint Gaussian distribution. To fully specify GP, two functions are needed, the mean function m(x) and the covariance function k(x, x ′ ). If a function f is based on GPs distribution of these two functions, it can be defined as:</p><formula xml:id="formula_14">m(x) = E[f (x)] k(x, x ′ ) = E[(f (x) -m(x))(f (x ′ ) -m(x ′ ))] f (x) ∼ GP(m(x), k(x, x ′ )),<label>(13)</label></formula><p>where m(x) is the mean function, which is usually the 0 constant, and k(x, x ′ ) is the kernel or covariance function, which describes the covariance between values of f at the different locations of k(x and x ′ ).</p><p>GPs are an alternative kernel-based framework that provides competitive results for point estimation <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b92">93]</ref>, and they explicitly epistemic uncertainty in data and predictions. This makes GPs ideal when well-calibrated uncertainty estimates are required. A commonly used approach to uncertainty estimation using GPs is the posterior predictive distribution. The posterior predictive distribution is the distribution of the predicted value of the output variable at any given input point, given the observed data. The mean of the posterior predictive distribution gives the predicted value, while the variance provides a measure of uncertainty. The larger the variance, the more uncertain the forecast. However, the time complexity of GPs increases with the size of the data, which makes it intractable in many practical applications.</p><p>Challenging: How to make reasonable assumptions about different distributions in distribution-based uncertainty estimation modeling? Distribution-based uncertainty estimation modeling often requires making assumptions about the underlying distribution of the data, which can be challenging when dealing with complex or multi-modal data. Developing techniques for selecting appropriate distributions and assessing their validity is necessary to ensure the accuracy of the model and the reliability of the uncertainty estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Uncertainty Estimation Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Calibration Metrics</head><p>The calibration metric assesses the consistency of the classifier in producing reliable predictions. Fig. <ref type="figure" target="#fig_3">5</ref> can be intuitively visualized by (a) Calibration curves and (b) Reliability diagram. This part focuses on the following metrics for evaluating the calibration of predictions produced by deep learning classifiers using uncertainty estimation techniques.</p><p>Expected Calibration Error (ECE) <ref type="bibr" target="#b9">[10]</ref>. It denotes the expected calibration error, which aims to evaluate the expected difference between model prediction confidence and accuracy. The concrete formulation is as follows:</p><formula xml:id="formula_15">ECE = |B| i=1 N i N |acc(b i ) -conf(b i )|,<label>(14)</label></formula><p>where b i represents the i-th bin and |B| represents the total number of bins. N denotes the number of total samples. N i represents the number of samples in the i-th bin. acc(b i ) denotes the accuracy and conf(b i ) denotes the average of confidences in the i-th bin. Maximal Calibration Error (MCE) <ref type="bibr" target="#b9">[10]</ref>. MCE represents the maximum deviation between model accuracy and prediction confidence. In programs that require a reliable measure of confidence, it is desirable to minimize the worstcase deviation between confidence and accuracy. MCE is similar to ECE in that this approximation involves bins:</p><formula xml:id="formula_16">M CE = max m∈{1,...,M } |acc(b m ) -conf(b m )|. (<label>15</label></formula><formula xml:id="formula_17">)</formula><p>Brier Score (BS). The BS is a metric that evaluates the proximity of a model's predicted probability to the actual class probability, which is always 1. A desirable BS is near zero, as this indicates minimal deviation from the true class probabilities due to the squared differences <ref type="bibr" target="#b93">[94]</ref>. BS is often written as the mean square prediction error (MSE) in regression tasks, defined as:</p><formula xml:id="formula_18">BS = 1 N n i=1 (y (i) -ŷ(i) ) 2 . (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>Both BS and mean squared error (MSE) involve calculating the squared difference between predicted and actual values. BS is often used to evaluate probabilistic predictions, while MSE is more commonly used to evaluate regression models.</p><p>Negative Log-likelihood (NLL). The NLL is a widely accepted metric for evaluating the effectiveness of a probabilistic model <ref type="bibr" target="#b94">[95]</ref>, and in the realm of deep learning it is commonly referred to as cross entropy loss. When presented with a probabilistic model and a set of n samples, this measure can be used to gauge the model's calibration:</p><formula xml:id="formula_20">N LL = - n i=1 log(π(y (i) |x (i) )).<label>(17)</label></formula><p>Both NLL and Negative Log Predictive Density (NLPD) are calculated based on the negative logarithm of probability, and NLPD calculates the negative logarithm of the predicted density of new data given the training data and model.</p><p>Furthermore, some NLP research proposes to calibrate evaluation metrics to make them more suitable for evaluating specific tasks. For instance, Lin et al. <ref type="bibr" target="#b95">[96]</ref> proposes a new metric called Compositional Expected Calibration Error (CECE) in his Seq2Seq graph parsing work to measure the behavior of the model in predicting the structure of the combined graph, which evaluates the performance of the model on graph elements. This metric calibrates the case to better explain the behavior of the model in the combined graph structure and the distributional behavior of predicted graph structures under offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Uncertainty Indication</head><p>Apart from assessing confidence calibration, there are several other evaluation indicators for uncertainty. These metrics are primarily focused on constructing classification metrics by utilizing either uncertainty or confidence to determine the model's capability to differentiate between misclassified and outlier samples. The Area Under the Curve (AUC). AUC is a commonly used measure the area under the receiver-operator curve, the formulation is as follows:</p><formula xml:id="formula_21">AU C = t0∈D 0 t1∈D 1 1[g(t 0 ) &lt; g(t 1 )] |D 0 | • |D 1 | (<label>18</label></formula><formula xml:id="formula_22">)</formula><p>where D 0 is the set of negative examples, and D 1 is the set of positive examples. 1[g(t 0 ) &lt; g(t 1 )] denotes an indicator function which returns 1 if g(t 0 ) &lt; g(t 1 ) otherwise return 0. Area Under the Receiver-Operator Characteristic Curve (AUROC), and the AUROC metric measures the probability that a randomly selected accurate response has a greater uncertainty score than a randomly selected inaccurate response. Higher values indicate superior performance, with an ideal uncertainty score of 1 and a value of 0.5 for a random uncertainty measure. Therefore, AUROC scores high when the model is confident about correct predictions but uncertain about incorrect ones. Depending on the axes and decision information used, AUC can produce varying score indicators, such as AUCPR (Area Under the Precision-Recall Curve) can be seen as an extension of the AUC score. However, AUC is based on the True Positive Rate (TPR) and False Positive Rate (FPR), while AUCPR is based on Precision and Recall.</p><p>The Area Under the Risk Coverage Curve (AUC-RCC) <ref type="bibr" target="#b96">[97]</ref>. AUC-RCC evaluates the quality of uncertainty estimation in terms of its ability to reject predictions with high uncertainty and avoid misclassification. AUC-RCC is based on the risk coverage curve, which plots the cumulative risk (or loss) as a function of the uncertainty level used for rejecting predictions.</p><p>The Reversed Pair Proportion (RPP) <ref type="bibr" target="#b97">[98]</ref>. Given a labeled dataset D of size n, RPP is used to evaluate the distance of the uncertainty estimator ũ from the ideal value. In order to minimize the AUC of RCC, the selective classifier should be intuitive for the correct classification output ũ = 0 for the example of correct classification output, and ũ = 1 for the wrong example. Therefore, the formula is defined as follows: where n 2 in the denominator is used to normalize the values.</p><formula xml:id="formula_23">RP P = 1 n 2 n 1≤i,j≤n 1[ũ(x i ) &gt; ũ(x j ), l i &lt; l j ],<label>(19)</label></formula><p>RPP measures the proportion of pairs of examples with an inverse confidence-error relationship, and an ideal confidence estimator would have an RPP value of 0. In contrast to AUROC, which measures overall discrimination, the RPP incorporates consistency of ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATIONS OF UNCERTAINTY IN NLP</head><p>In this section, we review the applications of uncertainty estimation to NLP systems. As shown in Fig. <ref type="figure" target="#fig_5">6</ref>, the highlevel perspective is mainly divided into three dimensions: data, system, and assessment. The proportions of each type and its sub-types are further counted, depicted in Fig. <ref type="figure" target="#fig_6">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Filter and Action Guidance</head><p>In the data dimension, we summarize the application of uncertainty as filtering and guidance. It mainly involves three primary aspects, discussed separately below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Active Learning</head><p>Active learning (AL) is a machine learning technique that reduces the amount of labeled data required for training by selecting the most informative unlabeled data for annotation. This is based on the idea that not all data points are equally useful during learning. Uncertainty-based AL is a popular approach that assumes that data with higher uncertainty is more informative and likely to enhance model performance. As shown in Table <ref type="table" target="#tab_1">1</ref>, uncertainty-based AL has been applied to various NLP tasks, and performance metrics (e.g., Accuracy, F1) are mainly used for AL evaluation. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, AL mainly uses the MCD based on sampling, and the confidence and entropy method based on calibration, focusing more on the quality of data filtered by the teacher model and the overall performance of the student system.</p><p>Benefiting from the development of NLP's pre-training model, uncertainty-based AL can effectively reduce the labeling cost and improve model performance. For example, self-supervised language modeling and uncertainty filtering data can be used for AL in the cold-start setting, effectively reducing labeling costs and improving model performance <ref type="bibr" target="#b99">[100]</ref>. Ein-Dor et al. <ref type="bibr" target="#b100">[101]</ref> combine BERT with the uncertainty estimation in the study of text classification AL to achieve good results in data screening of minority classes. In addition, pre-trained models allow us to improve data filtering capabilities in few-shot learning. Mukherjee et al. <ref type="bibr" target="#b104">[105]</ref> consider fine-tuning DNNs with limited labeled data. This work develops two strategies for AL data filtering: filtering hard data with high uncertainty and filtering soft data with low uncertainty improves the performance of PLMs in fewshot learning scenarios. Combining pre-trained models with uncertainty-based active learning improves model performance by leveraging pre-existing knowledge and selecting informative examples for fine-tuning.</p><p>Moreover, data can be filtered using uncertainty and additional information. For example, ACTUNE <ref type="bibr" target="#b101">[102]</ref> proposes uncertainty-based data filtering and guidance, allowing switching between data filtering and model training. This approach chooses highly deterministic unlabeled samples as actively labeled, while low-uncertainty samples are selected for model self-training. AL usually needs to traverse all unlabeled data to find informative unlabeled samples, which are always close to the decision boundary with large uncertainty. AUSDS <ref type="bibr" target="#b103">[104]</ref> annotates unlabeled text samples with high uncertainty through adversarial attacks, which significantly compresses the search space and protects the decision boundary from drastic changes.</p><p>Since AL can select samples with large uncertainty, it is natural to imagine applying uncertainty-based AL to multiple domains in the NLP. For example, Lyu et al. <ref type="bibr" target="#b109">[110]</ref> identify language as a key factor in the difference between data inside and outside the distribution in QA. They further propose a data selection strategy for AL based on an uncertainty measure developed using deterministic sequence probability and MCD sequence probability. This approach was shown to be more effective in selecting uncertain data for multilingual QA. Information in real-world scenarios is often multi-domain and rich in unlabeled data. Siddhant et al. <ref type="bibr" target="#b1">[2]</ref> point out that classical uncertainty estimation can only account for arbitrary uncertainty (e.g., entropy or confidence). In contrast, using MCD+ BALD and Back-prop+BALD as two uncertainty estimation methods can achieve excellent results in the three NLP tasks of emotion classification, NER, and semantic role labeling.</p><p>However, there are also some challenges and limitations of uncertainty-based AL, such as how to measure uncertainty effectively, how to trade off exploration and exploitation, and how to deal with the class imbalance and data diversity. This section provides a comprehensive review of existing work on uncertainty-based AL in NLP, covering data selection strategies, NLP issues involved, and uncertainty estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">OOD/Outlier Detection</head><p>Out-of-distribution (OOD)/outlier detection is an important application of uncertainty estimation in NLP. It helps to distinguish in-distribution examples from OOD/outlier examples. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, detecting OOD data in NLP covers three paradigms of uncertainty estimation. The goal TC <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b100">101]</ref>; <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b104">105]</ref>; NLI <ref type="bibr" target="#b105">[106]</ref>; SA <ref type="bibr" target="#b102">[103]</ref>.</p><p>Confidence <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b101">102]</ref>; Entropy <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b103">104]</ref>; KL <ref type="bibr" target="#b102">[103]</ref>; MCD <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b104">105]</ref>; Ensemble <ref type="bibr" target="#b100">[101]</ref>; Evidence <ref type="bibr" target="#b105">[106]</ref>.</p><p>Accuracy <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b100">101]</ref>; <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b105">106]</ref>;</p><p>F1 <ref type="bibr" target="#b99">[100,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b105">106]</ref>.</p><p>Active Learning SeqLab NER <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b107">108]</ref>, <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b108">109]</ref>.</p><p>Confidence <ref type="bibr" target="#b108">[109]</ref>; Entropy <ref type="bibr" target="#b103">[104,</ref><ref type="bibr" target="#b107">108]</ref>; MCD <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b106">107]</ref>.</p><p>F1 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b107">108]</ref>; Accuracy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b108">109]</ref>.</p><p>Active Learning Generative QA <ref type="bibr" target="#b109">[110]</ref>; AS <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b110">111]</ref>.</p><p>Confidence <ref type="bibr" target="#b14">[15]</ref>; MCD <ref type="bibr" target="#b109">[110,</ref><ref type="bibr" target="#b110">111]</ref>.</p><p>ROUGE <ref type="bibr" target="#b14">[15]</ref>; BLEU Var <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b110">111]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OOD detection Class</head><p>SA <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b111">112]</ref>; SLU <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b112">113]</ref>; NLI <ref type="bibr" target="#b113">[114]</ref>; TC <ref type="bibr" target="#b113">[114,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b115">116]</ref>, <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b116">117]</ref>.</p><p>Confidence <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b114">115]</ref>; Entropy <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b112">113]</ref>; TS <ref type="bibr" target="#b112">[113]</ref>; KL <ref type="bibr" target="#b102">[103]</ref>; Mahalanobis <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b88">89]</ref>; Evidence <ref type="bibr" target="#b116">[117]</ref>; MCD <ref type="bibr" target="#b115">[116]</ref>; Ensemble <ref type="bibr" target="#b115">[116]</ref>; Gaussian <ref type="bibr" target="#b112">[113]</ref>.</p><p>Accuracy <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b113">114,</ref><ref type="bibr" target="#b115">116]</ref>; <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b112">113]</ref>; F1, <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b114">115,</ref><ref type="bibr" target="#b115">116]</ref>; ECE <ref type="bibr" target="#b113">[114]</ref>; AUROC <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b116">117]</ref>; AUPR <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b116">117]</ref>; FAR95 <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b111">112,</ref><ref type="bibr" target="#b112">113]</ref>; AUCRCC <ref type="bibr" target="#b88">[89]</ref>, FPR90 <ref type="bibr" target="#b116">[117]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OOD detection</head><p>SeqLab NER <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b88">89]</ref>. Entropy <ref type="bibr" target="#b75">[76]</ref>; Evidence <ref type="bibr" target="#b85">[86]</ref>.</p><p>F1 <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b88">89]</ref>; AUROC <ref type="bibr" target="#b85">[86]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OOD detection Generative</head><p>LM <ref type="bibr" target="#b75">[76]</ref>; MT <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b118">119]</ref>; QA <ref type="bibr" target="#b77">[78]</ref>.</p><p>MCD <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b118">119]</ref>; Entropy <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b112">113]</ref>; Ensemble <ref type="bibr" target="#b77">[78]</ref>.</p><p>Perplexity <ref type="bibr" target="#b75">[76]</ref>;</p><p>BLEU Var <ref type="bibr" target="#b117">[118,</ref><ref type="bibr" target="#b118">119]</ref>; Accuracy <ref type="bibr" target="#b77">[78]</ref>.</p><p>Selective prediction Generative QA <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b120">121]</ref>.</p><p>Confidence <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b120">121]</ref>; Smoothing <ref type="bibr" target="#b120">[121]</ref>; MCD <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b120">121]</ref>; AUROC <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b120">121]</ref>. Accuracy <ref type="bibr" target="#b29">[30]</ref>; Precision/Recall <ref type="bibr" target="#b119">[120]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selective prediction Class</head><p>Duplicate Detection <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b121">122]</ref>; NLI <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b121">122]</ref>; SA <ref type="bibr" target="#b122">[123]</ref>; TC <ref type="bibr" target="#b79">[80]</ref>.</p><p>Confidence <ref type="bibr" target="#b121">[122,</ref><ref type="bibr" target="#b122">123]</ref>; MCD <ref type="bibr" target="#b79">[80]</ref>; Ensemble <ref type="bibr" target="#b79">[80]</ref>; Bayes by Backprob <ref type="bibr" target="#b79">[80]</ref>.</p><p>Accuracy <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b121">122]</ref>; F1 <ref type="bibr" target="#b79">[80]</ref>; AUROC <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b121">122,</ref><ref type="bibr" target="#b122">123]</ref>; AUPR <ref type="bibr" target="#b122">[123]</ref>.</p><p>is to identify examples that are different from the training data and the model has not been seen before. Metrics used to detect system performance include task performance metrics and uncertainty classification metrics. The task performance metric evaluates the model's ability to correctly handle the corresponding task, while the uncertainty classification metric measures the uncertainty quality of the model's predictions. We summarize the types of detection data into three ones: unknown domain data, misinformation data, and misclassification data.</p><p>Unknown Domain Data Detection. Domain shift in NLP can be of different types such as background shift (same task, but style/domain change) and semantic shift (unseen labels) <ref type="bibr" target="#b123">[124]</ref>. For example, traditional data selection based on testing domain knowledge often fails in unknown domains such as patents and tweets. Thus, unknown domain data detection is an important field of outlier data, including multi-domain intersection, open domain, etc. Uncertainty-based detection techniques can help solve these problems to improve the reliability and usability of NLP models in real scenarios. The corresponding NLP tasks have applications. For instance, MULTIUAT <ref type="bibr" target="#b118">[119]</ref> addresses the challenge of learning multilingual and multi-domain translation models due to heterogeneous and imbalanced data, which makes the model converge inconsistently over different corpora. Li et al. <ref type="bibr" target="#b77">[78]</ref> perform multi-task-intensive retrieval in open-domain QA, exploiting epistemic uncertainty to deal with corpus inconsistencies. Yu et al. <ref type="bibr" target="#b75">[76]</ref> show that maximizing the uncertainty of training data using en-tropy can enhance prediction accuracy on unseen domains and outperform CNN, BERT, and Transformer baselines, even without knowledge of unknown domains.</p><p>Different domains have various distributions. Therefore, some previous works apply distribution-based uncertainty estimation for OOD detection. Zhou et al. <ref type="bibr" target="#b87">[88]</ref> propose an unsupervised unknown domain detection method using a contrastive learning framework. They fine-tune Transformers with a contrastive loss to improve representation compactness. They further use Mahalanobis distance and Gaussian distribution in the penultimate layer to accurately detect OOD instances. Hu et al. <ref type="bibr" target="#b116">[117]</ref> exploit uncertainty in OOD detection for text classification tasks using evidential neural networks based on Dirichlet distribution. They propose a framework that adopts auxiliary outliers and pseudo off-manifold samples to train the model with prior knowledge of a certain class, which has high vacuity for OOD samples. In addition, PLMs may affect OOD detection performance. Hendrycks et al. <ref type="bibr" target="#b111">[112]</ref> systematically study the OOD robustness of pre-trained Transformers on various models. To measure OOD detection performance, they use the negative prediction confidence as the outlier supervision score and show that the pre-trained Transformer performs better than previous models both in generalizing to OOD examples and in detecting OOD examples.</p><p>Misinformation Data Detection. The proliferation of misinformation on social media platforms is a major concern for society. One approach to developing uncertaintybased methods and preventing misinformation is to use uncertainty-based methods to detect and prevent misinformation and rumor spreading. To name a few, Zhang et al. <ref type="bibr" target="#b124">[125]</ref> conduct research on uncertainty estimation methods, specifically Bayesian deep learning and Evidence Lower Bound (ELBO) objective function for rumor detection. Kochkina et al. <ref type="bibr" target="#b125">[126]</ref> research error/outlier detection for automatic rumor verification. They propose two uncertaintybased instance rejection methods using aleatoric and epistemic uncertainty estimates obtained through confidencebased and MCD-based methods. This research aims to solve the problem of resolving rumors circulating online by prioritizing difficult instances for human fact-checkers and interpreting model performance during rumor unfolding. Furthermore, Feng et al. <ref type="bibr" target="#b126">[127]</ref> study "none of the above" (NOTA) detection tasks in dialogue systems, i.e. the case where no correct response (i.e. ground truth) exists in the candidate set. To this end, the paper utilizes MCD as an uncertainty estimation method to measure the ability to capture uncertainty for end-to-end retrieval models. Overall, the speed at which misinformation spreads on social media makes manual verification difficult, and these tools can help detect and mitigate the impact of misinformation, thereby reducing its potential negative impact on the public.</p><p>Misclassification Data Detection. To detect OOD, epistemic uncertainty is caused by limited training data or model structure. However, misclassification detection also requires modeling aleatoric uncertainty caused by noise and ambiguity in data <ref type="bibr" target="#b127">[128]</ref>. Hendrycks et al. <ref type="bibr" target="#b122">[123]</ref> study uncertainty estimation such as confidence-based and Softmax prediction probability for sentiment classification. The goal is to indicate when classifiers are likely to make mistakes to increase their adoption and prevent accidents. The authors provide a misclassification detection baseline for neural networks that outperforms MaxProb for selective prediction. Vazhentsev et al. <ref type="bibr" target="#b88">[89]</ref> investigate the usage of uncertainty estimation for Transformer-based NER and text misclassification, and add an experimental setup given OOD data. The research proposes two computationally efficient modifications, including Mahalanobis Distance with a spectralnormalized network, which approach or outperform computationally intensive approaches.</p><p>However, how to determine the OOD boundary? One of the main reasons for OOD boundary is the limited input information available to NLP models, as they typically rely on a limited set of features extracted from the input text. This can lead to many models that are well-calibrated or achieve excellent uncertainty estimation in IDs, but poor results in OOD samples <ref type="bibr" target="#b25">[26]</ref>. One option for finding the boundary between ID and OOD data is to train a supervised classifier on both ID and OOD data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b123">124]</ref>. Nevertheless, collecting a representative set of OOD data may not be practical due to the infinite compositionality of languages, and selecting an arbitrary subset may introduce selection bias and limit the generalizability of the model to unseen OOD data. Alternatively, Ryu et al. <ref type="bibr" target="#b128">[129,</ref><ref type="bibr" target="#b129">130]</ref> propose using generative models, such as autoencoders and GANs <ref type="bibr" target="#b130">[131]</ref>, to capture the ID data distribution and differentiate between ID and OOD data based on reconstruction error or likelihood. Other approaches, such as meta-learning for OOD detection and generating pseudo-OOD data <ref type="bibr" target="#b131">[132]</ref>, also provide decision boundary between ID and OOD <ref type="bibr" target="#b132">[133]</ref>, but all require additional data or training procedures beyond the task, which may result in significant data collection work or inference overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Selective Prediction</head><p>Selective prediction refers to the ability of AI systems to abstain from making predictions when faced with novel inputs that differ from their training data distribution. This is essential for improving the reliability of NLP systems in real-world safety-critical domains like biomedical and autonomous robots, where incorrect predictions can have serious consequences. Typically, c contains a prediction confidence estimator c and a threshold τ that controls the level of abstention:</p><formula xml:id="formula_24">c(x) = 1[c(x) &gt; τ ]<label>(20)</label></formula><p>For dataset D, the coverage of the threshold τ corresponds to the fraction of answered instances (where c(x) &gt; τ ), and the risk is the error of these answered instances.</p><p>Selective predictive systems make a trade-off between coverage and risk. As shown in Table <ref type="table" target="#tab_1">1</ref>, QA systems for medical domains require high precision and discard questions that will not be answered by the QA system. This presents a cost-saving opportunity <ref type="bibr" target="#b119">[120]</ref>. In addition, the example of a medical diagnosis model usually provides high-confidence classifications even when requires human intervention. The failure of classifiers indicates when they are likely mistaken, limiting their adoption or causing serious accidents. The application of selective prediction in NLP is crucial for enabling the reliable deployment of NLP systems in realworld applications.</p><p>Recently, selective prediction in NLP has received attention. Andersen et al. <ref type="bibr" target="#b79">[80]</ref> study MCD for selectivity prediction in text classification. A semi-automatic text classification framework that minimizes unreliable and error-prone classifications by explicitly modeling uncertainty. Varshney et al. <ref type="bibr" target="#b121">[122]</ref> handle NLI tasks based on ID and OOD datasets, allowing systems to avoid making predictions when they might go wrong, improving their reliability in safety-critical domains. Furthermore, Varshney et al. <ref type="bibr" target="#b120">[121]</ref> investigate selective prediction in NLP using 17 datasets covering NLI, duplicate detection, and QA tasks. In BERT-based experiments, the results show that these methods outperform the MaxProb method when comparing whether the MCD and label smoothing can improve the performance of selectivity prediction. Garg et al. <ref type="bibr" target="#b119">[120]</ref> conduct research on filtering out unanswered questions in medical QA systems. They focus on Transformer-based question models to improve answer models and estimate uncertainty. This research shows that confidence scores for answers can be approximated from question text alone without requiring answers. By filtering out error-prone problems, the improved QA model improves the reliability of real-world NLP applications. Kamath et al. <ref type="bibr" target="#b29">[30]</ref> explore selectivity prediction in QA tasks under domain shift. Here TS is used to calibrate confidence scores, which enables the model to discard answers when necessary to account for distributions different from the training data.</p><p>Selective prediction in NLP involves using uncertainty to determine which data to process, resulting in higher accuracy on the answered subset. However, determining  <ref type="bibr" target="#b133">[134,</ref><ref type="bibr" target="#b134">135]</ref>; SA <ref type="bibr" target="#b135">[136,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b137">138]</ref>; NLI <ref type="bibr" target="#b136">[137]</ref>; Fact verification <ref type="bibr" target="#b137">[138]</ref>; Topic classification <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b137">138]</ref>; Sentence classification <ref type="bibr" target="#b35">[36]</ref>; Shopping review <ref type="bibr" target="#b35">[36]</ref>; Sentences-matching <ref type="bibr" target="#b35">[36]</ref>.</p><p>Confidence <ref type="bibr" target="#b137">[138]</ref>;</p><p>Entropy <ref type="bibr" target="#b133">[134]</ref>;</p><p>TS <ref type="bibr" target="#b136">[137,</ref><ref type="bibr" target="#b137">138]</ref>;</p><p>CP <ref type="bibr" target="#b137">[138]</ref>.</p><p>Accuaracy <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b133">134]</ref>; <ref type="bibr" target="#b135">[136,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b137">138]</ref>; Consistency <ref type="bibr" target="#b137">[138]</ref>; Layers <ref type="bibr" target="#b137">[138]</ref>; Time% <ref type="bibr" target="#b133">[134]</ref>; Speedup <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b137">138]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficacy</head><p>Regression GLUE <ref type="bibr" target="#b135">[136]</ref>; Similarity <ref type="bibr" target="#b134">[135,</ref><ref type="bibr" target="#b137">138]</ref>.</p><p>Confidence <ref type="bibr" target="#b134">[135,</ref><ref type="bibr" target="#b137">138]</ref>; TS <ref type="bibr" target="#b137">[138]</ref>; CP <ref type="bibr" target="#b137">[138]</ref>.</p><p>Speedup <ref type="bibr" target="#b135">[136]</ref>;</p><p>Relative scores <ref type="bibr" target="#b134">[135]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficacy</head><p>Generative Text generation <ref type="bibr" target="#b37">[38]</ref>.</p><p>Confidence <ref type="bibr" target="#b37">[38]</ref>.</p><p>BLEU <ref type="bibr" target="#b37">[38]</ref>; ROUGE <ref type="bibr" target="#b37">[38]</ref>; F1 <ref type="bibr" target="#b37">[38]</ref>; Layers <ref type="bibr" target="#b37">[38]</ref>.</p><p>Performance Class TC <ref type="bibr" target="#b6">[7]</ref>;</p><p>Fake News Detection <ref type="bibr" target="#b138">[139]</ref>.</p><p>Gumbel <ref type="bibr" target="#b6">[7]</ref>; MCD <ref type="bibr" target="#b6">[7]</ref>; Ensemble <ref type="bibr" target="#b6">[7]</ref>; Gaussian <ref type="bibr" target="#b138">[139]</ref>.</p><p>F1 <ref type="bibr" target="#b138">[139]</ref>; Accuaracy <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b138">139]</ref>.</p><p>Performance Structure KGs <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b141">142]</ref>. Gumbel <ref type="bibr" target="#b140">[141]</ref>; Confidence <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b141">142]</ref>.</p><p>BS <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b141">142]</ref>; MAE <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b140">141]</ref>; Accuaracy <ref type="bibr" target="#b139">[140]</ref>; F1 <ref type="bibr" target="#b139">[140]</ref>.</p><p>Performance SeqLab NER <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b142">143]</ref>.</p><p>MCD <ref type="bibr" target="#b36">[37]</ref>; Entropy <ref type="bibr" target="#b142">[143]</ref>; Smoothing <ref type="bibr" target="#b67">[68]</ref>.</p><p>F1 <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b142">143]</ref>; ECE <ref type="bibr" target="#b67">[68]</ref>.</p><p>Performance Generative MT <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b143">144,</ref><ref type="bibr" target="#b144">145]</ref>;</p><p>QA <ref type="bibr" target="#b145">[146]</ref>.</p><p>Confidence <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b145">146]</ref>; KL <ref type="bibr" target="#b26">[27]</ref>; Norma Confidence <ref type="bibr" target="#b144">[145]</ref>; MCD <ref type="bibr" target="#b143">[144,</ref><ref type="bibr" target="#b144">145]</ref>.</p><p>BLEU <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b143">144,</ref><ref type="bibr" target="#b144">145]</ref>; Accuaracy <ref type="bibr" target="#b145">[146]</ref>; AUROC <ref type="bibr" target="#b145">[146]</ref>.</p><p>how to rank examples according to confidence metrics and balancing the trade-offs between risk and coverage are still considerations in selective prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NLP System Efficiency and Performance</head><p>The framework of an NLP system is usually multi-layer or multi-module. Uncertainty estimation can be exploited for each component to improve the computational efficiency or performance of the network. Although performances could be also improved by better dealing with data in Sec 4.1, here in this section, we review from the perspective to leverage uncertainty in the interior computation of NLP systems and models. As presented in Fig. <ref type="figure" target="#fig_5">6</ref>, uncertainty is introduced for deciding the necessity of calculation. And it promotes performance by providing more information or guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Uncertainty and Efficiency</head><p>PLMs have impressive performance but require substantial computational resources, which limits their use in realworld applications <ref type="bibr" target="#b134">[135,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b137">138]</ref>. In particular, autoregressive decoding using a full stack of Transformer layers for each output token is computationally expensive, and this approach is commonly used in NLP <ref type="bibr" target="#b37">[38]</ref>. This limitation makes it difficult where fast inference and low computational cost are essential. Additionally, over-parameterization of PLMs can lead to overthinking in decision-making, where the model may focus on complex or irrelevant features in later layers instead of relying on simpler features from earlier layers that generalize better <ref type="bibr" target="#b135">[136]</ref>. As demonstrated in Fig. <ref type="figure" target="#fig_6">7</ref>, the calibration-based uncertainty estimation approach is more commonly leveraged to improve efficiency. Its calculation is convenient and sufficient. Accurate confidence information allows the model to improve efficiency in a more optimal way.</p><p>An uncertainty-based early exit in NLP is a technique to improve the efficiency of neural models. As shown in Fig. <ref type="figure">8</ref>, early exit allows the model to exit the sequence being processed early based on uncertainty about the predicted output. This strategy works by introducing a threshold value that determines when the model should stop computation in advance via the uncertainty of the predicted output. The threshold value can be set through the desired trade-off between accuracy and efficiency. If the uncertainty score of the prediction is below the threshold, the model con-tinues processing the sequence. Yet if the uncertainty score exceeds the threshold, the model exits early and returns the prediction. This approach can significantly reduce the computation required to process the sequence and improve the overall efficiency of the model.</p><p>To name a few, Schuster et al. <ref type="bibr" target="#b37">[38]</ref> propose confidence adaptive language modeling to improve the efficiency of autoregressive decoding in PLMs. Confidence-based uncertainty estimation allows PLMs to generate new tokens with intermediate network layers rather than the full model. Thereby it reduces the amount of computation. The approach builds on distribution-free uncertainty quantification and provides a principled approach to improving model efficiency while maintaining predictive quality. Liu et al. <ref type="bibr" target="#b35">[36]</ref> design a pre-trained model called FastBERT that applies self-distillation and knowledge distillation to achieve faster and more accurate results in Chinese and English sentence classification tasks. To further improve efficiency, FastBERT utilizes normalized entropy for uncertainty estimation, removing cases with low uncertainty from the batch and sending cases with high uncertainty to the next layer for further reasoning. Xin et al. <ref type="bibr" target="#b134">[135]</ref> propose BERxiT, which combines BERT and exit for regression tasks in NLP. Furthermore, Table <ref type="table" target="#tab_2">2</ref> shows the application is mainly general performance metrics and efficiency improvement comparison metrics. By using uncertainty-based early exit, NLP models can achieve faster processing times and reduce computational costs without sacrificing accuracy.</p><p>Overall, uncertainty estimation has the potential to enhance the performance and efficiency of NLP systems. However, one challenge is how to incorporate uncertainty estimates into the decision-making process of NLP systems without introducing bias or overconfidence. Additionally, it is essential to trade off the computational cost of uncertainty estimation with its potential benefits in terms of performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Uncertainty and Performance</head><p>Uncertainty information can also be used to guide the model and improve its performance. For instance, Xiao et al. <ref type="bibr" target="#b2">[3]</ref> show that applying uncertainty estimation to NLP tasks such as sentiment analysis, NER, and language modeling models can learn accurate mappings. Additionally, uncertainty can provide richer embedding information. By incorporating uncertainty into the model, knowledge graph embeddings can improve the accuracy and robustness of models. Uncertain-aware embeddings can incorporate probabilistic uncertainty into the embedding process, which allows for more precise representations <ref type="bibr" target="#b139">[140,</ref><ref type="bibr" target="#b140">141,</ref><ref type="bibr" target="#b141">142,</ref><ref type="bibr" target="#b146">147]</ref>. It can be seen from Table <ref type="table" target="#tab_2">2</ref> that, in addition to performance indicators, uncertainty estimation is mainly exploited for uncertainty calibration indicators.</p><p>Uncertainty can also be incorporated into the model's training and inference stages to improve model performance. Gui et al. <ref type="bibr" target="#b36">[37]</ref> propose to improve NER task performers with MCD, which predicts error-prone draft labels in the first stage. A two-stream self-attention model in the second stage aims to refine these draft predictions. This approach helps to prevent error propagation and improves the accuracy of NLP models. In addition, Zhou et al. <ref type="bibr" target="#b144">[145]</ref> assume that language features alone cannot QA <ref type="bibr">[149, 150] [26, 151]</ref>.</p><p>Confidence <ref type="bibr" target="#b148">[149]</ref>, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b147">148,</ref><ref type="bibr" target="#b149">150]</ref>;</p><p>TS <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b148">149]</ref>;</p><p>GPs <ref type="bibr" target="#b92">[93]</ref>;</p><p>MCD <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b147">148]</ref>;</p><p>Ensemble <ref type="bibr" target="#b147">[148]</ref>;</p><p>Heteroscedastic <ref type="bibr" target="#b17">[18]</ref>.</p><p>Accuracy <ref type="bibr">[26] [149, 150]</ref>; MSE <ref type="bibr" target="#b149">[150]</ref>; NLPD <ref type="bibr" target="#b92">[93]</ref>; NLL <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b147">148]</ref> [18]; MAE <ref type="bibr" target="#b92">[93]</ref>; ECE <ref type="bibr">[148, 149] [18, 26]</ref>; AUROC <ref type="bibr" target="#b25">[26]</ref>; BS <ref type="bibr" target="#b25">[26]</ref>; Sharpness <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b147">148]</ref>; Pearson <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b147">148]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Quality</head><p>Assessment <ref type="bibr" target="#b151">[152]</ref>.</p><p>GPs <ref type="bibr" target="#b151">[152]</ref>.</p><p>NLPD <ref type="bibr" target="#b151">[152]</ref>; RMSE <ref type="bibr" target="#b151">[152]</ref>; Pearson <ref type="bibr" target="#b151">[152]</ref>.</p><p>completely solve the difficulties encountered by the MT model. They propose an adaptive curriculum learning strategy to evaluate the uncertain information of the model. This strategy enhances the performance of neural machine translation, taking into account the complexity and rarity of information in translation pairs. In MT tasks, a sourcelanguage sentence may have multiple semantically similar expressions. And a source-language sentence may also have multiple valid counterparts in another language. Thus such tasks are many-to-many problems, which inherently contain uncertainty. Wei et al. <ref type="bibr" target="#b26">[27]</ref> leverage confidence and KL distributions to capture the relationship between multiple semantically equivalent source sentences and use general semantic information to enhance hidden representations for better translation results. Accurate uncertainty refers to the process of adjusting a model's uncertainty estimate to better reflect its true performance. This helps ensure the model makes reliable predictions and improves its accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reliability and Trustworthy Assessment</head><p>In this section, we review the applications of uncertainty in the output stage of NLP system. Here the main insight is that can we fully trust the output of a neural network? The answer is obviously no. The main reason is that its output relies on probability and thereby mistakes are inevitable. The question can be converted to how much can we trust the output? In other words, it means reliability or trustworthy extents. Since uncertainty is reflected as a numerical value, which naturally helps us to assess the quality of the model or output. By quantifying and analyzing uncertainty, we can gain valuable insights into the strengths and weaknesses of our NLP models, and better understand the risks and limitations of the decisions we make based on them. For instance, if an NLP model has high uncertainty in its predictions for certain data points or scenarios, we expect to investigate further or use additional information to improve the performance of the model. Uncertainty can be used to evaluate models' performance. Confidence scores can be compared to the actual correctness of decisions made by the model. This approach enables us to determine how reliable and trustworthy the model is in terms of making accurate decisions. In particular, some metrics given in Sec 3.3 can be exploited to evaluate the confidence calibration and uncertainty classification ability of the model, and then reflect the advantages and disadvantages of the model. Furthermore, assessing the reliability of generated content is crucial. Especially in the exponential search space of the generative model, the use of uncertainty for model and output analysis is valuable. Table <ref type="table" target="#tab_3">3</ref> shows that in recent years, research on uncertainty estimation in MT and QA has gradually increased, and various evaluation indicators have increased to explain the model or output results. Typically, multiple candidate translations can be generated in MT, making it difficult to determine the best translation. Estimating the uncertainty of each candidate translation can help researchers choose the translation with the lowest uncertainty, leading to better translation quality <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b147">148]</ref>. Recently, as PLMs have become more powerful and widely used, it has become increasingly important to be able to explain how they arrive at answers and provide some degree of uncertainty or confidence in predictions. There are various techniques for evaluating the performance and credibility of PLMs, including self-evaluation techniques, calibration scores, and other methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b148">149,</ref><ref type="bibr" target="#b149">150,</ref><ref type="bibr" target="#b150">151]</ref>. These techniques can help to ensure that NLP systems are trustworthy and reliable, and can be used with confidence by users in a range of different applications.</p><p>Last but not least, an intuitive question is how to express uncertainty to improve model evaluation and obtain valuable information? This is a natural challenge because language expression itself is inherently uncertain. Quantifying the uncertainty expressed by the model, making effective evaluations, and reducing the interaction bias caused by expression are all important considerations. We provide a more detailed discussion of this challenge in Sec 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CHALLENGES AND FUTURE DIRECTIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Extremely High-Dimensional Language-space</head><p>The field of NLP is challenged by the extremely highdimensional language space, as highlighted by the need to estimate predictive entropy, which requires taking an expectation in output space. This output space has a dimensionality of O(|T | N ), which presents significant computational challenges. Additionally, the lack of a normalized probability density function over sentences necessitates approximating the expectation using Monte Carlo integration, which involves averaging the likelihoods of a finite set of sampled sentences. However, Monte Carlo integration becomes difficult for entropy as it is often dominated by lowprobability sentences that have large and negative logs. MT models, for example, can contain hundreds of millions of parameters, which exponentially increases the search space. Additionally, with only a single reference for each source sentence, it becomes difficult to measure the fitness of MT models to data distributions. As a result, researchers face significant scientific challenges in adapting tools from both the machine learning and statistics fields to effectively tackle this problem.</p><p>In addition, the complexity of uncertainty in NLP systems is also constantly increasing due to the evolving mod-els and the demand for human-AI interaction. Although the current uncertainty estimation techniques are rich, further exploration is still needed to address issues such as time cost and inference accuracy, and how to consider the impact of natural language features on uncertainty estimation. Some uncertainty estimation techniques can also be limited in their scalability. Firstly, TS is a convenient calibration technique, but the performance of TS is very sensitive to the choice of the scaling factor. Choosing the optimal scaling factor may require extensive hyperparameter tuning, which can be time-consuming and computationally expensive. Secondly, ensembling multiple models or MCD can be effective for uncertainty estimation but can also be resource-intensive and difficult to scale to large models. Finally, distributionbased uncertainty estimation can provide valuable insights into the uncertainty of DNNs, but it requires distributional assumptions, such as Gaussian or a mixture of Gaussian distributions, which may not apply in all situations and limit its applicability. In the era of large PLMs such as ChatGPT and GPT-4, researchers are not accessible to the model. Even if a PLM is open-sourced, retraining is often necessary, which can be computationally expensive and time-consuming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Variable Length Generation</head><p>The challenge of variable text length in NLP cannot be ignored. One major obstacle is the significant variation in the length of sentences <ref type="bibr" target="#b54">[55]</ref>. As Malinin et al. <ref type="bibr" target="#b7">[8]</ref> point out, variable-length text generation is particularly challenging in NLG, where longer sequences tend to have lower joint likelihoods due to the conditional independence of token probabilities. In other words, as the length of a sequence increases, its joint likelihood decreases exponentially, leading to a linear increase in its negative log probability. This means that longer sentences contribute more to entropy, which exacerbates the difficulty of dealing with variable-length texts. Text length normalization is commonly adopted to combat variable text length, which assumes that the uncertainty in a sentence is independent of its length <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b152">153,</ref><ref type="bibr" target="#b153">154]</ref>. Specifically, for estimating the probability e = e 1:l of generating the output sequence, length normalization divides the score s(e) by the length l of the survival text to get s ′ (e) = s(e)/l. For example, Kuhn et al. <ref type="bibr" target="#b3">[4]</ref> apply length normalization of the log probability when estimating the semantic entropy of a QA system. However, this technique may not be useful in all cases, especially when dealing with long sentences, as it fails to capture the increased complexity and difficulty of longer text sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Expression of Uncertainty in Language Model</head><p>In real life, information is rarely black and white, which is why expressing uncertainty is necessary to support the decision-making process. While highly certain expressions are commonly leveraged, uncertain expressions tell us about the confidence, source, and limitations of the information. In the current research, we find two main forms of uncertainty expression: probabilistic expression and natural expression, as demonstrated in Fig. <ref type="figure" target="#fig_8">9</ref>. The former one generally has relevant values available for analysis, but the community has found different results on the calibration of neural models.</p><p>Different Templates for Uncertainty Representation Answer Prefix: 1. I'm 90% sure it's ... 2. I vaguely remember it's ... Answer Suffix: 1. ...But I would need to double-check. 2. ...With 100% confidence. Answer Self-evaluation: 1. ...With what confidence could you answer this question? and output an answer like 0%, 10%, 20%, ..., 100%. For example, Desai and Durrett <ref type="bibr" target="#b113">[114]</ref> show that pre-trained Transformers are relatively well calibrated, while Wang et al. <ref type="bibr" target="#b154">[155]</ref> find severe miscalibration in MT. Jiang et al. <ref type="bibr" target="#b148">[149]</ref> study calibration in generative QA and observe only a weak correlation between the log-likelihood model assigned to their answers and the correctness of the answers. In general, PLMs output a possibility for a given tag sequence, but do not output the entire meaning <ref type="bibr" target="#b3">[4]</ref>.</p><p>On the one hand, whether this uncertainty expression is suitable for generative NLP systems remains questionable. We may need stronger correlation indicators to focus on the uncertainty expression of probability. On the other hand, naturalistic representations of uncertainty cover a wide range of discursive behaviors, such as signaling hesitancy, attributing information, or acknowledging limitations, and such representations are intuitive to humans. As the examples displayed in Fig. <ref type="figure" target="#fig_8">9</ref>, QA prompts are able to express uncertainty. Through different prefixes, or confidence levels of self-assessment outputs, we can obtain numerical uncertainty representations to further analyze the calibration of the model. <ref type="bibr">Kadavath et al. [26]</ref> expect to observe large benefits of few-shot evaluation with natural language methods, but instead. Yet no major gains are observed in early QA experiments. Zhou et al. <ref type="bibr" target="#b150">[151]</ref> find that non-deterministic expressions can affect language production, and that changes in these expressions can have a substantial impact on overall accuracy, especially when using high-certainty expressions, including in accuracy and calibration. The authors further hypothesize that this may be due to the usage of hyperbolic or exaggerated language in the training set, where numbers are used non-literally. Then, the model somehow recognizes idiomatic, non-literal usage of these extreme values, resulting in lower performance of the task when hints are introduced.</p><p>In summary, effective understanding and expression of uncertainty are crucial for NLP applications to ensure better decision-making. While there is a wealth of literature on methods for estimating uncertainty, there is little understanding of how linguistic uncertainty interacts with natural language. This lack of attention has resulted in a poor understanding of how models interact with natural language, leading to challenges in formulating and evaluating uncertainty estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">NLP and Social Security</head><p>It is imperative to analyze uncertainty information in NLP systems when integrating social and ethical demands. Therefore, in the following discussion, we will examine future directions from these three perspectives. As an important part of general AI, NLP systems need to account for moral uncertainty, that is, try to explain what decisions should be made, given various details of the circumstances of their decisions, including the choices they face and their theoretical rationale. Newberry et al. <ref type="bibr" target="#b155">[156]</ref> discuss the idea of using an automated "moral parliament" as a way to mitigate the risk of value erosion in AI systems. By incorporating a variety of values from different stakeholders, the AI could be directed by a simulated group representing different values. This approach would better reflect the moral uncertainty of humans and avoid committing AI to one value system. Perhaps the AI system will become a superior version of the current chatbot, surpassing humans in multiple fields <ref type="bibr" target="#b156">[157]</ref>. As AI systems become more powerful and have a greater impact on society, there is an increasing need to ensure that they are designed with appropriate ethical and moral considerations, despite uncertainty surrounding implementation details across multiple paradigms. This includes developing AI systems that can incorporate ethical uncertainty and allow for multi-stage scrutiny to ensure they are safe and beneficial for society.</p><p>Overall, in addition to the limitations and challenges of existing uncertainty estimation methods, there is a need for more effective methods that can provide interpretable and reliable uncertainty estimates. This is an active area of research, and developing better uncertainty estimation methods can help improve the reliability, robustness, and safety of NLP systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Considering the semantic ambiguity of natural language, uncertainty is an important attribute of text information. For the first time, this survey aims to provide a comprehensive review of uncertainty estimation in the NLP field, including its sources, quantification approaches, and applications. We first depict the backgrounds of uncertainty types and summarize the uncertainty sources of an NLP system by its multiple stages. Then, uncertainty estimation methods are systematically reviewed, with pointing out the technical advantages and application difficulties respectively. Meanwhile, evaluation metrics for uncertainty estimation are also discussed. We further investigate relevant literature on uncertainty-aware applications in the NLP field, dividing it into three directions and providing detailed analyses. Finally, we highlight four challenges in the combination between uncertainty estimation and the development of PLMs. Promising research areas are also explored, including the discussion on the scalability of uncertainty technology, the expression of uncertainty, and AI security.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of NLP systems applied in the medical domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Rationale visualization of three types of uncertainty modeling. For a given input sample x, each method provides a prediction y whose uncertainty is quantified as u. (a) Calibration-based uncertainty representation, (b) Sampling-based uncertainty estimation method, (c) Distribution-based uncertainty estimation method. Ξ denotes a specific distribution. The mean E and variance V ar are only used to keep the visualization simple, there are other quantifications in practice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An overview of the taxonomy of uncertainty estimation techniques. The inner circle represents uncertainty modeling methods, the outer circle represents actual uncertainty methods, and some methods are represented by abbreviations.</figDesc><graphic coords="6,355.09,45.48,165.83,166.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of uncertainty representation method based on calibration confidence. a) Calibration curve, the closer to the perfect curve, the better the confidence calibration. b) Reliability diagram combined with ECE calibration indicators.</figDesc><graphic coords="7,421.13,43.70,129.15,95.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>, a distribution-based model is also a deterministic model, but requires a specific distribution for uncertainty modeling. In general, distribution-based models need to pass some prior/posterior assumptions to allow the model to learn distribution information during training. The following are two commonly used examples of distributionbased uncertainty modeling. Dirichlet-based Uncertainty Models (DBU) utilize logits and construct a Dirichlet distribution. Unlike standard (Softmax) neural networks, this model predicts the parameters of the Dirichlet distribution instead of point estimates. The density of the Dirichlet distribution is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.<ref type="bibr" target="#b5">6</ref>. An overview of the application of uncertainty estimation to NLP systems. The red box indicates how the uncertainty is applied, and the blue box is the action taken using the uncertainty.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. An overview of the applied classification of uncertainty. The inner circle represents the application, the middle circle represents the uncertainty modeling method used, and the outer circle refers to the specific uncertainty estimation method.</figDesc><graphic coords="11,90.98,45.59,165.94,166.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Layer/Module 1 LayerFig. 8 .</head><label>18</label><figDesc>Fig. 8. Illustration of the early exit method based on uncertainty. Common layers/modules such as Transformer, C i are usually a classifier, determining whether the early exit is determined by its confidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Illustration of a problem template for uncertainty estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Application of Uncertainty-Based Data Correlation in NLP, and some task/method names are represented as abbreviations.</figDesc><table><row><cell>Application</cell><cell>Paradigm</cell><cell>Tasks Types</cell><cell>UE Methods</cell><cell>Metrics</cell></row><row><cell>Active Learning</cell><cell>Class.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Uncertainty-based applications related to efficiency and performance improvement in NLP systems.</figDesc><table><row><cell>Application</cell><cell>Paradigm</cell><cell>Tasks Types</cell><cell>UE Methods</cell><cell>Metrics</cell></row><row><cell></cell><cell></cell><cell>GLUE</cell><cell></cell><cell></cell></row><row><cell>Efficacy</cell><cell>Class</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Uncertainty-based applications related to NLP system evaluation.</figDesc><table><row><cell>Tasks Types</cell><cell>UE Methods</cell><cell>Metrics</cell></row><row><cell>MT [18, 93]</cell><cell></cell><cell></cell></row><row><cell>[148];</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Paradigm shift in natural language processing</title>
		<author>
			<persName><forename type="first">T.-X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-P</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="183" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Bayesian active learning for natural language processing: Results of a large-scale empirical study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2904" to="2909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantifying uncertainties in natural language processing tasks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7322" to="7329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is your classifier actually biased? measuring fairness under uncertainty with bernstein bounds</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ethayarajh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2914" to="2919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Uncertainty estimation with calibrated confidence scores</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kivimäki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer uncertainty estimation with hierarchical stochastic attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluating robustness of predictive uncertainty estimation: Are dirichlet-based models reliable</title>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Kopetzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Ünnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5707" to="5718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding neural abstractive summarization models via uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6275" to="6281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gawlikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R N</forename><surname>Tassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kruspe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roscher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03342</idno>
		<title level="m">A survey of uncertainty in deep neural networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Waegeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="457" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disentangling uncertainty in machine translation evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Glushkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8622" to="8641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ebbe-text: Explaining neural networks by exploring text classification decision boundaries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Delaforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Azé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bringay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mollevi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sallaberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Servajean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An ontological analysis of uncertainty in soft data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FUSION</title>
		<imprint>
			<biblScope unit="page" from="1566" to="1573" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Language (technology) is power: A critical survey of &quot;bias&quot; in NLP</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5454" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The translator&apos;s invisibility</title>
		<author>
			<persName><forename type="first">L</forename><surname>Venuti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="179" to="212" />
		</imprint>
	</monogr>
	<note>Criticism</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analyzing uncertainty in neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3956" to="3965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A noisy-channel model of human sentence comprehension under uncertain input</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural relation extraction via inner-sentence noise reduction and transfer learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2195" to="2204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Language models (mostly) know what they know</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Schiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tran-Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05221</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Uncertaintyaware semantic augmentation for neural machine translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2724" to="2735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SUN: Exploring intrinsic uncertainties in text-to-SQL parsers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5298" to="5308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Types of out-of-distribution texts and how to detect them</title>
		<author>
			<persName><forename type="first">U</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">701</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selective question answering under domain shift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5684" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">News aggregation with diverse viewpoint identification using neural embeddings and semantic understanding models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carlebach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheruvu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Ilharco</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jaume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Argument Mining</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The art of abstention: Selective prediction and error regularization for natural language processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1040" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep evidential regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schwarting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soleimany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">937</biblScope>
			<biblScope unit="page" from="14" to="927" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast-BERT: a self-distilling BERT with adaptive inference time</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6035" to="6044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Uncertainty-aware label refinement for sequence labeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2316" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Confident adaptive language modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="17" to="456" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5232" to="5270" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Representation problems in linguistic annotations: Ambiguity, variation, uncertainty, error and bias</title>
		<author>
			<persName><forename type="first">C</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Assady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Linguistic Annotation Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="60" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Overview and state-of-the-art of uncertainty visualization</title>
		<author>
			<persName><forename type="first">G.-P</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Hege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rheingans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Visualization: Uncertainty, Multifield, Biomedical, and Scalable Visualization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Uncertainty in visual text analysis in the context of the digital humanities</title>
		<author>
			<persName><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data statements for natural language processing: Toward mitigating system bias and enabling better science</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="page" from="587" to="604" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06305</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Bond: Bert-assisted open-domain named entity recognition with distant supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multilinguals at SemEval-2022 task 11: Complex NER in semantically ambiguous settings for low resource languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Unnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1469" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bidirectional lstm-crf for named entity recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Panchendrarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Pacific Asia conference on language, information and computation</title>
		<meeting>the 32nd Pacific Asia conference on language, information and computation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A survey on deep learning for named entity recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="page" from="50" to="70" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Neural models for sequence chunking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning to search for recognizing named entities in Twitter</title>
		<author>
			<persName><forename type="first">I</forename><surname>Partalas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Derbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kalitvianski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>WNUT</publisher>
			<biblScope unit="page" from="171" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Calibrated structured prediction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evaluating evaluation methods for generation in the presence of variation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CICLing</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="341" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Correcting length bias in neural machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Uncertainty estimation in autoregressive structured prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Uncertainty estimation and reduction of pre-trained models for text regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Verspoor</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2022.tacl-1.39" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="680" to="696" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The FLORES evaluation datasets for low-resource machine translation: Nepali-English and Sinhala-English</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D19-1632" />
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11">Nov. 2019</date>
			<biblScope unit="page" from="6098" to="6111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Measuring the semantic similarity of texts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W05-1203" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</title>
		<meeting>the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Accurate uncertainties for deep learning using calibrated regression</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2796" to="2804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Structured and efficient variational deep learning with matrix gaussian posteriors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1708" to="1716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning to select data for transfer learning with Bayesian optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017</date>
			<biblScope unit="page" from="372" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Calibrating deep neural networks using focal loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kulharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dokania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="15" to="288" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Boundary smoothing for named entity recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7096" to="7108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Uncertainty quantification with pre-trained language models: A large-scale empirical analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Findings</title>
		<imprint>
			<date type="published" when="2022-12">Dec. 2022</date>
			<biblScope unit="page" from="7273" to="7284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Algorithmic learning in a random world</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shafer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bayesian training of backpropagation networks by the hybrid monte carlo method</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Camp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">On variational bounds of mutual information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5171" to="5180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning uncertainty for unknown domains with zero-target-assumption</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1112.5745</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Multi-task dense retrieval via model uncertainty fusion for open-domain question answering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-Findings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="274" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Answer uncertainty and unanswerability in multiple-choice machine reading comprehension</title>
		<author>
			<persName><forename type="first">V</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-Findings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1020" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Efficient, uncertainty-based moderation of neural networks text classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maalej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-Findings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1536" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Deep ensemble bayesian active learning: Addressing the mode collapse issue in monte carlo dropout via ensembles</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fulop</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03897</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Can you trust your model&apos;s uncertainty? evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Nado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02757</idno>
		<title level="m">Deep ensembles: A loss landscape perspective</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Evidential deep learning to quantify classification uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sensoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Posterior network: Uncertainty estimation without ood samples via densitybased pseudo-counts</title>
		<author>
			<persName><forename type="first">B</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Ünnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1356" to="1367" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">E-ner: Evidential deep learning for trustworthy named entity recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.17854</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6623</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Contrastive out-of-distribution detection for pretrained transformers</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1100" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Uncertainty estimation of transformer predictions for misclassification detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vazhentsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kuzmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shelmanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsvigun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tsymbalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fedyanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Panov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burtsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Avetisian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8237" to="8252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<title level="m">Gaussian processes for machine learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">An investigation on the effectiveness of features for translation quality estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<editor>MTSummit</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Joint emotion analysis via multitask gaussian processes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1798" to="1803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Exploring prediction uncertainty in machine translation quality estimation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGNLL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="208" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">The brier score does not evaluate the clinical utility of diagnostic tests or prediction models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Assel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Sjoberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Vickers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Diagnostic and prognostic research</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Evaluating predictive uncertainty challenge</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quinonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">On compositional uncertainty quantification for seq2seq graph parsing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">On the foundations of noise-free selective classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wiener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="page" from="1605" to="1641" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">The art of abstention: Selective prediction and error regularization for natural language processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1040" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Active learning with sampling by uncertainty and density for word sense disambiguation and text classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Cold-start active learning through self-supervised language modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7935" to="7948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Active Learning for BERT: An Empirical Study</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ein-Dor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halfon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shnarch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dankin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Danilevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="7949" to="7962" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">AcTune: Uncertainty-based active self-training for active fine-tuning of pretrained language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1422" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Active learning by acquiring contrastive examples</title>
		<author>
			<persName><forename type="first">K</forename><surname>Margatina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vernikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aletras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Active sentence learning by adversarial uncertainty sampling in discrete space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="4908" to="4917" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Uncertainty-aware self-training for few-shot text classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Awadallah</surname></persName>
		</author>
		<editor>NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">212</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Uncertainty-aware cross-lingual transfer with pseudo partial labels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-Findings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1987" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Active learning for sequence tagging with deep pre-trained models and Bayesian uncertainty estimates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shelmanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Puzyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kupriyanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Larionov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Khromov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kozlova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Artemova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1698" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A little annotation does a lot of good: A study in bootstrapping lowresource named entity recognizers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5164" to="5174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Ltp: a new active learning strategy for crf-based named entity recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Neural Processing Letters</publisher>
			<biblScope unit="page" from="2433" to="2454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">You need only uncertain answers: Data efficient multilingual question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duolikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TWorkshop on Uncertainty and Ro-Bustness in Deep Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Should we trust this summary? Bayesian abstractive summarization to the rescue</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gidiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsoumakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-Findings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4119" to="4131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Pretrained transformers improve out-of-distribution robustness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2744" to="2751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Enhancing the generalization for intent classification and out-of-domain detection in slu</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2443" to="2453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Calibration of pre-trained transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Mitigating uncertainty in document classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3126" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Towards more accurate uncertainty estimation in text classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alhamadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8362" to="8372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Uncertainty-aware reliable text classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="628" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Wat zei je? detecting outof-distribution translations with variational transformers</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08344</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Uncertainty-aware balancing for multilingual and multidomain neural machine translation training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7291" to="7305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Will this question be answered? question filtering via answer model distillation for efficient question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7329" to="7346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Investigating selective prediction approaches across several tasks in IID, OOD, and adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-Findings</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1995" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Towards improving selective prediction ability of nlp systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RepL4NLP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="221" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<title level="m" type="main">Reply-aided detection of misinformation via bayesian deep learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lipani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="2333" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Estimating predictive uncertainty for rumour verification models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6964" to="6981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eskenazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<title level="m">none of the above&quot;: Measure uncertainty in dialog response retrieval</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2013" to="2020" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Deterministic neural networks with inductive biases capture epistemic and aleatoric uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11582</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Neural sentence embedding using only in-domain sentences for out-of-domain sentence detection in dialog systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="page" from="26" to="32" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Out-of-domain detection based on generative adversarial network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="714" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Out-of-domain detection for low-resource text classification tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3566" to="3572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Training confidencecalibrated classifiers for detecting out-of-distribution samples</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Romebert: Robust training of multi-exit bert</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09755</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">BERxiT: Early exiting for BERT with better fine-tuning and extension to regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="91" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Bert loses patience: Fast and robust inference with early exit</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="330" to="348" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">The right tool for the job: Matching model and instance complexities</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6640" to="6651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Consistent accelerated inference via confident adaptive transformers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4962" to="4979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Uncertainty-aware propagation structure reconstruction for fake news detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2759" to="2768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Embedding uncertain knowledge graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3363" to="3370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Probabilistic box embeddings for uncertain knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boratko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mc-Callum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="882" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Uncertain ontology-aware knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">K</forename><surname>Boutouhami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIST</title>
		<imprint>
			<biblScope unit="page" from="129" to="136" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Rethinking negative sampling for handling missing entity annotations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7188" to="7197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Improving back-translation with uncertainty-based confidence estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="791" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Uncertainty-aware curriculum learning for neural machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6934" to="6944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Knowing more about questions can help: Improving calibration in question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1958" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Gtranse: generalizing translation-based model on uncertain knowledge graph embedding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kertkeidkachorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JSAI</title>
		<imprint>
			<biblScope unit="page" from="170" to="178" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Uncertaintyaware machine translation evaluation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Glushkova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-Findings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3920" to="3938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">How can we know when language models know? on the calibration of language models for question answering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="page" from="962" to="977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main">Teaching models to express their uncertainty in words</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<monogr>
		<title level="m" type="main">Navigating the grey area: Expressions of overconfidence and uncertainty in language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13439</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Modelling uncertainty in collaborative document quality assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Noisy User-generated Text</title>
		<meeting>the 5th Workshop on Noisy User-generated Text</meeting>
		<imprint>
			<publisher>W-NUT</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="191" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for WMT&apos;15</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">On the inference calibration of neural machine translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3070" to="3079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">The parliamentary approach to moral uncertainty</title>
		<author>
			<persName><forename type="first">T</forename><surname>Newberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ord</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Natural selection favors ais over humans</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
