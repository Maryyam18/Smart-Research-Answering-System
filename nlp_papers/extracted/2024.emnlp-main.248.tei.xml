<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models</title>
				<funder ref="#_Aj2WMaU">
					<orgName type="full">LG AI</orgName>
				</funder>
				<funder ref="#_CjntJmA">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
							<email>seungone@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juyoung</forename><surname>Suk</surname></persName>
							<email>juyoung@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bill</forename><forename type="middle">Yuchen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jamin</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Moontae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
							<email>minjoon@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaist</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lg</forename><forename type="middle">Ai</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carnegie</forename><surname>Mellon University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Allen Institute for AI</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Chicago 6</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1CDFED8741FB58B88CF4EB4439D1CD1E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-24T16:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of opensource LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they often do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2. Prometheus 2 is more powerful than its predecessor, and closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, PROMETHEUS 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available. 1 * equal contribution. Work was done while Seungone was an intern at LG AI Research and a MS student at KAIST.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Evaluating the quality of outputs produced by language models (LMs) is progressively becoming difficult, as the outputs cover an extremely diverse distribution of text and complex tasks. To address this issue, language model-based evaluation has emerged as a scalable and cheap paradigm for assessing LM-generated text <ref type="bibr" target="#b27">(Li et al., 2024;</ref><ref type="bibr"></ref> Figure <ref type="figure">1</ref>: Weak evaluators (e.g., Llama-2-Chat-70B, Prometheus, and GPT-3.5-Turbo) achieve low scoring correlation with strong evaluators (e.g., Humans, GPT-4, and Claude-3-Opus). On the other hand, scores provided by strong evaluators highly correlate with each other. <ref type="bibr" target="#b8">Gao et al., 2024)</ref>. In this paradigm, LMs are either prompted to output a scalar indicator of quality (denoted as direct assessment) <ref type="bibr" target="#b3">(Zheng et al., 2023;</ref><ref type="bibr">Liu et al., 2023b;</ref><ref type="bibr">Ye et al., 2023;</ref><ref type="bibr" target="#b20">Kim et al., 2023)</ref> or to determine which of two outputs are preferred (denoted as pairwise ranking) <ref type="bibr">(Wang et al., 2023b;</ref><ref type="bibr">Li et al., 2023b;</ref><ref type="bibr" target="#b22">Lambert et al., 2024)</ref>. Prior works employing proprietary LMs as evaluators have demonstrated not only high correlations with human evaluations but also increased speed and cost-effectiveness <ref type="bibr" target="#b3">(Zheng et al., 2023;</ref><ref type="bibr">Liu et al., 2023b;</ref><ref type="bibr" target="#b6">Dubois et al., 2023;</ref><ref type="bibr">Ye et al., 2023)</ref>.</p><p>However, relying on proprietary LMs for evaluation poses significant challenges. The lack of transparency about their training data compromises both fairness and reproducibility, making it problematic to use them in evaluation pipelines. Additionally, concerns regarding controllability and affordability also persist <ref type="bibr" target="#b20">(Kim et al., 2023)</ref>. To address these issues, recent works have focused on developing evaluator LMs that are open-access, transparent, and controllable <ref type="bibr" target="#b20">(Kim et al., 2023;</ref><ref type="bibr">Wang et al., 2023a,b;</ref><ref type="bibr">Li et al., 2023a;</ref><ref type="bibr" target="#b43">Zhu et al., 2023;</ref><ref type="bibr">Jiang et al., 2023b,c;</ref><ref type="bibr" target="#b23">Lee et al., 2024</ref>). Yet, these models often yield scoring decisions that do not correlate well enough with human judgments or those made by proprietary LMs, failing to effectively simulate them. Moreover, open evaluator LMs are not flexible since they are typically trained only to perform either direct assessment or pairwise ranking and assess based on general public preferences like helpfulness and harmlessness, limiting their ability to handle diverse real-life scenarios.</p><p>To close the gap with proprietary LMs, we investigate unifying the two model-based evaluation paradigms -direct assessment and pairwise ranking -to train a robust unified evaluator LM. We propose a recipe based on merging the weights of two evaluator LMs trained separately on direct assessment and pairwise ranking formats. Our key empirical observation is that weight merging can yield an evaluator LM that not only works in both formats, but also outperforms evaluator LMs that are jointly trained or only trained on a single format.</p><p>To demonstrate our approach, we develop the PREFERENCE COLLECTION, a new fine-grained pairwise ranking feedback dataset that builds on the FEEDBACK COLLECTION <ref type="bibr" target="#b20">(Kim et al., 2023)</ref>, which is a direct assessment feedback dataset. We choose Mistral-7B <ref type="bibr">(Jiang et al., 2023a)</ref> and Mixtral-8x7B <ref type="bibr" target="#b17">(Jiang et al., 2024)</ref> as our base models, and merge the weights of evaluator LMs separately trained on the FEEDBACK COLLECTION and the PREFERENCE COLLECTION to obtain our resulting models, PROMETHEUS 2 (7B &amp; 8x7B).</p><p>On four direct assessment benchmarks (Vicuna Bench, MT Bench, FLASK, Feedback Bench), the PROMETHEUS 2 models demonstrate the highest correlation with both human evaluators and proprietary LM-based judges compared to existing open evaluator LMs, with the Pearson correlation surpassing other baselines by 0.2 units across all datasets. Similarly, on four pairwise ranking benchmarks (HHH Alignment, MT Bench Human Judgment, Auto-J Eval, Preference Bench), the PROMETHEUS 2 models show the highest agreement with human evaluators among all the open evaluator LMs we tested, reducing the performance gap with GPT-4 in half.</p><p>Our contributions are summarized as follows:</p><p>• We introduce PROMETHEUS 2 (7B &amp; 8x7B), state-of-the-art open evaluator LMs that score high correlations with both human evaluators and proprietary LM-based judges on both direct assessment and pairwise ranking.</p><p>• We introduce a pairwise ranking feedback dataset called the PREFERENCE COLLEC-TION, which includes 1K custom evaluation criteria beyond helpfulness and harmlessness.</p><p>• We show that merging the weights of evaluator LMs trained on direct assessment and pairwise ranking feedback datasets results in a unified evaluator LM that excels in both schemes.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Language Model-based Evaluation</head><p>To assess the generation capabilities of LMs, prior works such as the GEM benchmark <ref type="bibr" target="#b9">(Gehrmann et al., 2021</ref><ref type="bibr" target="#b10">(Gehrmann et al., , 2022) )</ref> employed ROUGE <ref type="bibr" target="#b28">(Lin, 2004)</ref>, BLEU <ref type="bibr" target="#b32">(Papineni et al., 2002)</ref>, and BERTScore <ref type="bibr" target="#b41">(Zhang et al., 2019)</ref> as their metrics, which measure the lexical or semantic similarity between a reference answer and a response. However, these conventional metrics are prone to false negatives because they are not expressive enough to recognize responses that are of good quality but differ from the reference answer <ref type="bibr">(Schluter, 2017;</ref><ref type="bibr" target="#b7">Freitag et al., 2020;</ref><ref type="bibr" target="#b12">Hanna and Bojar, 2021)</ref>.</p><p>Recently, employing language models as a judge has gained attention as a promising paradigm to mimic the depth and granularity that human evaluation offers <ref type="bibr" target="#b3">(Zheng et al., 2023;</ref><ref type="bibr">Liu et al., 2023b;</ref><ref type="bibr">Li et al., 2023b;</ref><ref type="bibr" target="#b2">Chan et al., 2023;</ref><ref type="bibr">Ye et al., 2023)</ref>. To reduce the over-reliance on proprietary LMs, follow-up works suggest training language models specialized in evaluations <ref type="bibr" target="#b4">(Cui et al., 2023;</ref><ref type="bibr" target="#b20">Kim et al., 2023;</ref><ref type="bibr">Jiang et al., 2023b,c;</ref><ref type="bibr">Li et al., 2023a;</ref><ref type="bibr" target="#b23">Lee et al., 2024)</ref>. Yet, open evaluator LMs do not possess the flexibility to function in different evaluation schemes and show weak evaluation performance compared to proprietary LMs. We aim to bridge this gap by introducing PROMETHEUS 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Weight Merging</head><p>Prior works have demonstrated that weight merging can enhance performance across various domains, including language modeling <ref type="bibr" target="#b25">(Li et al., 2022;</ref><ref type="bibr" target="#b31">Matena and Raffel, 2022;</ref><ref type="bibr" target="#b13">Ilharco et al., 2022;</ref><ref type="bibr" target="#b5">Don-Yehiya et al., 2022;</ref><ref type="bibr" target="#b11">Gururangan et al., 2023;</ref><ref type="bibr" target="#b39">Yadav et al., 2024;</ref><ref type="bibr">Sukhbaatar et al., 2024)</ref>, instruction-tuning <ref type="bibr">(Jang et al., 2023b;</ref><ref type="bibr" target="#b40">Yu et al., 2023)</ref>, and aligning to user preferences <ref type="bibr">(Jang et al., 2023a;</ref><ref type="bibr" target="#b33">Rame et al., 2024;</ref><ref type="bibr" target="#b36">Wang et al., 2024)</ref>. In our work, we specifically focus on enhancing the evaluation capabilities of open evaluator LMs. By merging models trained on different assessment formats-specifically, direct assessment and pairwise ranking-we aim to obtain an evaluator LM that not only functions in both formats but also shows as good evaluation performances as proprietary LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We propose a new recipe for training a unified evaluator LM based on merging the weights of models trained for direct assessment and pairwise ranking. We begin with background on direct assessment and pairwise ranking for evaluator LMs (Section 3.1, 3.2), followed by the construction process of our training data (Section 3.3). Finally, we present our methods to train state-of-the-art evaluator LMs, Prometheus 2 models (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Direct Assessment</head><p>Direct assessment is mapping an instruction i and response r into a scalar value score s, such as f direct : (i, r) → s where s ∈ R. For the scoring range, we use an integer between 1 and 5.</p><p>Prior works have identified several recipes to align the scores provided by evaluator LMs (s LM ) and the scores assigned by humans (s human ). For instance, <ref type="bibr">Liu et al. (2023a)</ref> and <ref type="bibr" target="#b3">Zheng et al. (2023)</ref> have shown that it is crucial to add a reference answer a as input to the evaluator LM to maximize the correlation between s LM and s human . Also, <ref type="bibr" target="#b3">Zheng et al. (2023)</ref> and <ref type="bibr">Ye et al. (2023)</ref> showed that prompting the language model to write verbal feedback v r before s also improves the correlation between s LM and s human . Lastly, <ref type="bibr">Ye et al. (2023)</ref> and <ref type="bibr" target="#b20">Kim et al. (2023)</ref> showed that by explicitly integrating evaluation criteria e, users can define the standards for model assessment, ensuring evaluations are flexible to specific needs rather than generic qualities. Specifically, e is represented as a score rubric including a description for the criterion itself and a set of descriptions for each score between the scoring range. This is expressed as:</p><formula xml:id="formula_0">f direct : (i, r, a, e) → (v r , s)</formula><p>where s ∈ {1, 2, 3, 4, 5}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pairwise Ranking</head><p>Pairwise ranking is mapping an instruction i and two pair of responses (r m , r n ) into either i or j, such as f pair : (i, r m , r n ) → s where s ∈ {m, n}. Similar to direct assessment, prior works have identified that integrating a reference answer a and verbal feedback v rm,rn into the evaluation pipeline is crucial <ref type="bibr" target="#b3">(Zheng et al., 2023;</ref><ref type="bibr">Li et al., 2023b,a)</ref>. In addition, to support granular assessment under</p><p>Data PREFERENCE FEEDBACK COLLECTION COLLECTION Evaluation Scheme Pairwise Ranking Direct Assessment # Evaluation Criteria 1,000 1,000 # Instructions 20,000 20,000 # Reference Answer 20,000 20,000 # Instances 200,000 100,000 #Verbal Feedback 200,000 100,000 Table 1: Statistics of our training datasets, the FEED-BACK COLLECTION and the PREFERENCE COLLEC-TION. Note that the 1K evaluation criteria, 20K instructions, and 20K reference answers are shared among the two datasets. Both datasets have an equal number of scoring decisions ("A" or "B"; 100K each &amp; 1-5; 20K each) to prevent unintended biases after training.</p><p>custom criterion, we add the evaluation criteria e as input to the evaluator LM <ref type="bibr">(Ye et al., 2023;</ref><ref type="bibr" target="#b20">Kim et al., 2023)</ref>. To the best of our knowledge, we are the first to study such fine-grained evaluation in pairwise ranking settings. This is expressed as:</p><formula xml:id="formula_2">f pair : (i, r m , r n , a, e) → (v rm,rn , s) where s ∈ {m, n}<label>(2)</label></formula><p>In pairwise ranking, the evaluation criterion e does not include a set of descriptions for each score; instead, only the description of the evaluation criterion itself. Also, it is noteworthy that the verbal feedback v rm,rn compares the commonalities and differences between r m and r n concerning e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Preference Collection</head><p>Popular pairwise ranking datasets such as HH-RLHF <ref type="bibr" target="#b1">(Bai et al., 2022)</ref> or Ultra Feedback <ref type="bibr" target="#b4">(Cui et al., 2023)</ref> do not include an evaluation criterion e and a verbal feedback v rm,rn . To train an evaluator LM that could assess based on such criteria, we construct the PREFERENCE COLLECTION, including 1K evaluation criteria. We apply two modifications to the FEEDBACK COLLECTION. First, since the FEEDBACK COLLECTION includes five responses for each instruction, each corresponding to a scoring decision between 1 and 5, we pair two out of the five responses, resulting in a total of ten combinations per instruction. Using the existing scoring decisions for each response, we determine which response is better and assign a new scoring decision for that pair (i.e., "Response A is better" or "Response B is better"). Second, to generate new verbal feedback v rm,rn for each pair of responses, we prompt GPT-4-1106 to identify the commonalities and differences between the two responses. The statistics of the resulting dataset are listed in Table <ref type="table">1</ref> along with the FEEDBACK COLLECTION. We explain about our quality verification process of the PREFERENCE COLLECTION in Appendix A. Also, we include the prompts we use for the augmentation process in Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Methods &amp; Baselines</head><p>Prompting Prompting involves querying an LM to make judgments in a specified evaluation format without training. We employ Llama-2-Chat-7,13,70B <ref type="bibr">(Touvron et al., 2023)</ref>; Mistral-7B-Instruct-v0.2 <ref type="bibr">(Jiang et al., 2023a)</ref>; and Mixtral-8x7B-Instruct-v0.1 <ref type="bibr" target="#b17">(Jiang et al., 2024)</ref> as our baselines. It's worth noting that models not explicitly trained on feedback data often fail to generate responses in the required format, making it extremely difficult to parse scoring decisions. Although it is impractical for regular use, we make a fair comparison by infinitely looping until scores can be parsed. Also, we include proprietary LMs such as GPT-3.5-Turbo-0613; GPT-4-1106; and Claude-3-Opus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Format Training Single-Format training</head><p>involves training a base model θ on either on a direct assessment feedback dataset D d or a pairwise ranking feedback dataset D p . For singleformat trained evaluator LMs, we test Prometheus-7,13B <ref type="bibr" target="#b20">(Kim et al., 2023</ref>) (direct assessment); UltraRM-13B <ref type="bibr" target="#b4">(Cui et al., 2023)</ref> (pairwise ranking); and PairRM-0.4B <ref type="bibr">(Jiang et al., 2023c</ref>) (pair-wise ranking). In addition, we also report the performances of single-format training Mistral-7B-Instruct-v0.2 and Mixtral-8x7B-Instruct-v0.1 on either direct assessment or pairwise ranking.</p><p>Joint Training Joint training involves training a base model θ on both a direct assessment feedback dataset D d and a pairwise ranking feedback dataset D p . This enables the resulting evaluator LM to function across both evaluation formats. For jointly trained evaluator LMs, we test Auto-J <ref type="bibr">(Li et al., 2023a)</ref>. In addition, we report the performances of jointly training Mistral-7B and Mixtral-8x7B on both direct assessment and pairwise ranking.</p><p>Weight Merging Weight Merging involves training two models, θ d and θ p , separately on a direct assessment feedback dataset D d and a pairwise ranking feedback dataset D p . Then, the final evaluator LM θ f inal is obtained by merging θ d and θ p . For example, linear merging is as follows:</p><formula xml:id="formula_3">θ f inal = α × θ d + (1 -α) × θ p (3)</formula><p>In addition to linear merging, we test 5 additional variants, namely Task Arithmetic merging <ref type="bibr" target="#b13">(Ilharco et al., 2022)</ref>, TIES merging <ref type="bibr" target="#b39">(Yadav et al., 2024)</ref>, DARE-TIES and DARE-Linear merging <ref type="bibr" target="#b40">(Yu et al., 2023)</ref>, and SLERP merging <ref type="bibr" target="#b11">(Goddard et al., 2024)</ref>. We include an explanation of these merging methods and ablation experiment results of the performance differences in Appendix G. Among them,</p><p>Evaluation Method Benchmark Metrics Judgment Source Reference Answer # Score Rubrics # Instructions # Judgments Direct Assessment Vicuna Bench Correlation Proprietary LMs Y 80 80 320 MT Bench Correlation Proprietary LMs Y 80 80 320 FLASK Correlation Proprietary LMs &amp; Humans Y 12 200 2,000 Feedback Bench Correlation Proprietary LMs Y 200 200 1,000 Pairwise Ranking HHH Align. Accuracy Humans N 4 221 221 MT Bench Human Judg. Accuracy Humans N 1 80 3,360 Auto-J Eval Accuracy Humans N 1 58 1,392 Preference Bench Accuracy Proprietary LMs Y 200 200 2,000 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>The statistics of all the benchmarks are in Table <ref type="table" target="#tab_2">2</ref>. The four direct assessment benchmarks are:</p><p>• Vicuna Bench (Chiang et al., 2023): A singleturn chat benchmark that includes 80 test prompts, 80 hand-crafted score rubrics from Kim et al. (2023), and 320 responses obtained by WizardLM-13B, Vicuna-13B, Llama-2-Chat-13B, GPT-3.5-Turbo-0613. • MT Bench (Zheng et al., 2023): A multiturn chat benchmark that consists of 80 test prompts, 80 hand-crafted score rubrics from Kim et al. (2023), and 320 responses obtained by WizardLM-13B, Vicuna-13B, Llama-2-Chat-13B, GPT-3.5-Turbo-0613. • FLASK (Ye et al., 2023): A fine-grained evaluation benchmark comprised of 200 test prompts, 12 score rubrics, and 2000 responses acquired from Alpaca-7B, Vicuna-13B, Bard, GPT-3.5-Turbo-0613. In addition to scores from proprietary LMs, this benchmark also includes scores marked by human evaluators. • Feedback Bench (Kim et al., 2023): The test set of the FEEDBACK COLLECTION with 1K score rubrics, 200 instructions, and 1K responses that do not overlap with the train data. The four pairwise ranking benchmarks are: • HHH Alignment (Askell et al., 2021): A benchmark consisting of 221 prompts; 4 score rubrics (helpfulness, harmlessness, honesty, and other) and 221 response pairs (graded as 'win' or 'lose') judged by human evaluators. • MT Bench Human Judgment (Zheng et al., 2023): A benchmark that shares the same 80 prompts as MT-Bench. In addition, it provides 3,360 response pairs (graded as 'win', 'tie', or 'lose') judged by human evaluators. • Auto-J Eval (Li et al., 2023a): A benchmark consisted of 58 prompts and 1,392 response pairs (graded as 'win', 'tie', or 'lose') judged by human evaluators. This benchmark is used as the in-domain test set of Auto-J. • Preference Bench: Our in-domain test set for the PROMETHEUS models. Similar to how the PREFERENCE COLLECTION was made with the FEEDBACK COLLECTION, we adjust the FEEDBACK BENCH and pair two out of the five responses, resulting in a test set with 200 prompts, 2,000 response pairs (graded as 'win' or 'lose'), and 200 evaluation criteria.</p><p>In direct assessment, we conduct referencebased evaluations by appending the reference answer as the input. We use Pearson, Spearman, and Kendall-Tau as performance metrics to measure scoring correlations against reference evaluators. Moreover, we include the results of the referencefree direct assessment evaluation in Appendix F.</p><p>In pairwise ranking, we conduct reference-free evaluations. Based on judgments assigned by humans, we use accuracy as our metric to measure agreement between evaluator LMs and humans.</p><p>Also, the MT Bench Human Judgment and Auto-J test set includes a 'tie' option assessed by human evaluators. We evaluate in two ways: by excluding all 'tie' options for pairwise ranking (denoted as 'w/o tie'), or by using direct assessment where responses scored as 'ties' are grouped, and pairwise rankings are applied to the remaining responses with differing scores (denoted as 'w/ tie'). Table 4: Pairwise Ranking Results Accuracy on human preference datasets. The best comparable accuracies are bolded and second best underlined except proprietary LMs. Note that HHH Alignment is an in-domain test set for PairRM, Auto-J Eval is an in-domain test set for Auto-J, and the Preference Bench is an in-domain test set for Prometheus-2 models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we compare the evaluation capabilities of PROMETHEUS-2 models with other baselines using a direct assessment format (Section 5.1) and a pairwise ranking format (Section 5.2). Additionally, we measure the consistency of the scores from evaluator LMs in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Direct Assessment Results</head><p>The direct assessment results are shown in Table <ref type="table" target="#tab_4">3</ref>. The scoring decisions of PROMETHEUS 2 models (7B &amp; 8x7B), GPT-4-1106, Claude-3-Opus, and human evaluators all strongly correlate with each other, yielding Pearson correlations higher than 0.5 regardless of the reference evaluator and benchmark. On the other hand, base LMs, single-format trained LMs, and jointly trained LMs show lower correlations, mostly falling below 0.5.</p><p>Notably, PROMETHEUS 2 models outperform Prometheus and Auto-J by at least 0.2 units across benchmarks in their correlation with proprietary LMs. Moreover, on the FLASK benchmark, while the correlation between humans and GPT-4 is 0.679, the highest correlation previously achieved by Prometheus-13B with humans was 0.449. PROMETHEUS-2-8X7B achieves a correlation of 0.555 with humans, halving the gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pairwise Ranking Results</head><p>The pairwise ranking results are shown in Table <ref type="table">4</ref>. We exclude the results of Pair RM and Ultra RM on 'w/ Tie' settings since they could not process it.</p><p>On all of the 4 benchmarks, the PROMETHEUS 2 models achieve the highest scores, showing that they could effectively simulate human judgments. Notably, while HHH Alignment is an in-domain test set for Pair RM, and Auto-J Eval is for Auto-J, PROMETHEUS-2-8X7B achieves higher scores. This shows that training a large LM (i.e., Mixtral-8x7B) with feedback data could be an effective strategy to obtain a robust evaluator LM that could generalize beyond its training data. Moreover, the PROMETHEUS 2 models at least halve the performance gap with proprietary LMs compared to existing evaluator LMs on out-of-domain test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analyses of Weight Merging</head><p>To understand the effectiveness of our proposed weight merging method in the context of evaluations, we address the following research questions:</p><p>• RQ1: Is weight merging more effective compared to joint training? (Section 6.1)</p><p>• RQ2: Is the effectiveness of weight merging due to model ensembling? (Section 6.2)</p><p>• RQ3: To what extent does learning with direct assessment help pairwise ranking performance, and vice versa? (Section 6.3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Weight Merging vs Joint Training</head><p>Table <ref type="table" target="#tab_7">5</ref> compares the performance of evaluator LMs trained via weight merging and joint training. Alongside this, we also add and compare the results of prompting and single-format training. Surprisingly, evaluator LMs trained via joint training often show lower performance compared to those trained only in single-format, which indicates negative task transfer. Specifically, evaluator LMs trained only on direct assessment formats obtain higher correlations compared to their jointly trained counterparts across different model scales. Similarly, evaluator LMs trained solely on pairwise ranking formats achieve higher average accuracy compared to those trained on multiple tasks, particularly when using Mixtral-8x7B as the base model.</p><p>On the other hand, evaluator LMs trained via weight merging show superior performance not only compared to jointly trained evaluator LMs but also single-format trained evaluator LMs, indicating positive task transfer. Also, while both benefit each other, merging the pairwise ranking evaluator LM weights improves direct assessment performance more significantly than the reverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Is the Effectiveness of Weight Merging due to Model Ensembling?</head><p>While we empirically find that weight merging is effective, the underlying reason remains unclear. A natural assumption is that this effectiveness results from the ensembling effect of combining multiple models. To test this hypothesis, we conduct an ablation experiment where we train multiple evaluator LMs on different random seeds and merge them. Specifically, we merge two evaluator LMs trained on direct assessment formats (denoted as 'Direct Assessment &amp; Direct Assessment') and two evaluator LMs trained on pairwise ranking formats (denoted as 'Pairwise Ranking &amp; Pairwise Ranking').</p><p>We use Mistral-7B-Instruct as our base model.</p><p>The results are presented in Table <ref type="table" target="#tab_8">6</ref>. Across multiple benchmarks, merging evaluator LMs trained on the same evaluation format does not enhance evaluation performance. Specifically, merging two evaluator LMs trained on the same evaluation format-whether direct assessment or pairwise ranking-negatively impacts performance on average for both direct assessment and pairwise ranking benchmarks. In contrast, merging two evaluator LMs, each trained on direct assessment and pairwise ranking formats, results in superior performance compared to the other settings. This indicates that the beneficial task transfer in weight merging arises from integrating different evaluation formats, not ensembling multiple models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Quantifying Positive Transfer across Evaluation Formats</head><p>To explore how training on direct assessment feedback data influences pairwise ranking accuracy and vice versa, we experiment by adjusting the α value during linear merging. We evaluate the average performance using all eight benchmarks in our experiments. To illustrate the average performance (colored in black), we adjust the scale by multiplying the Pearson correlations from direct assessment, which originally range from 0 to 1, by 100 before averaging them with the pairwise ranking accuracy.</p><p>The results are shown in Figure <ref type="figure" target="#fig_1">3</ref>. For direct assessment benchmarks, evaluator LMs obtain the optimal performance when α is set to 0.5. This indirectly indicates that both pairwise ranking and direct assessment feedback data contribute equally. On the other hand, for pairwise ranking benchmarks, the performance is optimal when α is set to 0.3. This also implies that while both benefit each other, training on pairwise ranking improves direct assessment performance more than the reverse.</p><p>Training Method DIRECT ASSESSMENT BENCHMARKS PAIRWISE RANKING BENCHMARKS Vicuna Ben. MT Ben. FLASK Average HHH Align. MT Ben. H.J. Auto-J Eval Average Mistral-Instruct-7B PROMPTING 0.486 0.284 0.480 0.417 67.42 63.82 60.94 64.06 DIRECT ASSESSMENT ONLY 0.537 0.561 0.519 0.539 73.33 56.76 64.38 64.82 PAIRWISE RANKING ONLY ----78.73 67.06 72.03 72.61 JOINT TRAINING 0.548 0.450 0.457 0.485 80.09 65.49 73.60 73.06 WEIGHT MERGING 0.666 0.548 0.659 0.624 74.66 70.78 75.07 73.50 Mixtral-Instruct-8x7B PROMPTING 0.566 0.551 0.507 0.541 77.38 71.42 73.55 74.56 DIRECT ASSESSMENT ONLY 0.625 0.664 0.587 0.625 74.21 53.14 65.85 64.40 PAIRWISE RANKING ONLY ----84.16 66.27 75.66 75.36 JOINT TRAINING 0.628 0.560 0.596 0.595 82.35 68.73 74.78 75.29 WEIGHT MERGING 0.685 0.665 0.659 0.670 85.52 71.96 79.98 79.15  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduce PROMETHEUS 2, an open-source LM specialized in evaluating other responses. Unlike existing open evaluator LMs that cannot effectively process both direct assessment and pairwise rank-ing-the two most prevalent evaluation schemesthe PROMETHEUS 2 models demonstrate superior performance on both schemes, significantly narrowing the gap with proprietary LM-based evaluations.</p><p>To train the PROMETHEUS 2 models, we develop the PREFERENCE COLLECTION, the first pairwise ranking dataset that includes over 1,000 instancewise evaluation criteria beyond basic qualities such as helpfulness and harmlessness. Notably, we find that merging evaluator LMs trained on either direct assessment or pairwise ranking formats can lead to a unified evaluator LM with strong performance. We hope that our work encourages more research on using open-source LMs as evaluators.</p><p>the Institute of Information &amp; Communications Technology Planning &amp; Evaluation(IITP) grant funded by the Korea government(MSIT) (RS-2024-00397966, Development of a Cybersecurity Specialized RAG-based sLLM Model for Suppressing Gen-AI Malfunctions and Construction of a Publicly Demonstration Platform, 50%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Evaluation is fundamentally a very multi-faceted task. In this paper, we used an indirect method to assess the evaluation capability of evaluator LMs by measuring if they perform evaluations similar to human evaluators or proprietary LMs, such as GPT-4-1106 and Claude-3-Opus. However, this may not necessarily be the best approach. Future work could explore meta-evaluation pipelines that reevaluate the results of evaluator LMs or methodologies that allow humans to efficiently review evaluation results. Also note that it is crucial to use modelbased evaluations in conjunction with human evaluation instead of solely relying on it. Additionally, the degree to which evaluator LMs can generalize was based on an analysis by <ref type="bibr" target="#b20">Kim et al. (2023)</ref>, which checked for overlap between the data used to train the evaluator LMs and the data used to evaluate them. This study extended the evaluation to eight different datasets with human judgments to check the generalization capability of evaluation under various circumstances. However, this may not be sufficient. One of the major challenges in evaluating evaluator LMs is obtaining the "evaluation results" (e.g., human judgment). Automating evaluations with LMs could greatly benefit many areas of NLP research, hence the role of future work in creating feedback benchmarks that include human judgment or data for training evaluator LMs is crucial.</p><p>One downside of the PROMETHEUS 2 is that it operates only on a 1-5 point Likert scale for absolute evaluation or a comparative evaluation style of 'A is better &amp; B is better'. Depending on the use case, people may need a 1-10 point absolute evaluation, a ranking method for five responses at once, or a checklist-based evaluation not covered in the paper. While proprietary LMs can flexibly conduct evaluations in any format if a well-described prompt is devised, open-source LMs cannot produce good evaluation results without training, and conversely, if trained in one or two formats, they lose the flexibility to conduct different evaluations. Future work could examine whether evaluator LMs trained in each format, as done in this paper, can handle evaluations for added formats well when weight merging is employed.</p><p>Lastly, the paper presents an evaluation model that can handle both absolute and comparative evaluation formats well through weight merging based on empirical experiments. However, fundamentally explaining why weight merging works well remains a challenging task. To address this, Section 6 indirectly analyzes the effectiveness of weight merging by comparing it with joint training, demonstrating that the improvement in evaluation performance is not due to model ensembling, and showing that the impact of comparative evaluation on absolute evaluation is greater than the reverse. Our best current interpretation is that "absolute and comparative evaluations are not completely different tasks, so weight merging could handle both without degeneration, and conversely, because they are not too similar, weight merging performed better than joint training." Future work could theoretically analyze this or further explore whether weight merging can effectively work in fields other than LLM evaluation.</p><p>Table 10: Hyperparameters used to train PROMETHEUS 2 8x7B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Quality Verification of the PREFERENCE COLLECTION</head><p>To ensure the quality of the PREFERENCE COL-LECTION, particularly the generated verbal feedback v rm,rn , we employ five annotators with backgrounds in natural language processing. The annotation study was designed and administered in accordance with [Affiliation X]'s ethical guidelines. Crowd workers were informed of the potential risks of participation and researcher contact information before hand in the study consent form. The hourly wage and expected study time were informed in the Prolific platform. We compensated workers 9 GBP per hour. 3 were from USA and 2 were from Asian demographics.</p><p>We randomly sample 200 instances with different instructions and conduct a three-part verification process. First, we assess the coherence of v rm,rn with the scoring decision (i.e., 'A is better' or 'B is better'). Second, we evaluate the suitability of v rm,rn against the evaluation criteria e. Lastly, to determine the criticality of the feedback, we compare the newly generated v rm,rn with a concatenation of v rm and v rn . This aims to determine if v rm,rn effectively leverages the mutual information between r m and r n . Annotators then vote on whether v rm,rn or the concatenation of r m and r n is more critical. The results are shown in Table <ref type="table">7</ref>. Note that the Preference Collection only includes English instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training and Inference Details</head><p>The configurations we used for prompting and training evaluator LMs are shown in Table <ref type="table">8</ref>, 9, 10. For Auto-J, PairRM and UltraRM, we utilize their prompt template, inference hyperparameter mentioned in the model cards or github repositories in order to ensure the configuration is optimal for a fair performance comparison. For proprietary LMs, PROMETHEUS 1, and PROMETHEUS 2 models, we use the same prompt template and evaluation configurations.</p><p>For both training and inference, we utilized eight 40GB NVIDIA A100 GPUs. Training required approximately 800 GPU hours, using the implementation from the Alignment Handbook repository<ref type="foot" target="#foot_0">foot_0</ref> . For inference, we used the vllm framework<ref type="foot" target="#foot_1">foot_1</ref> .</p><p>The results from Direct Assessment are averaged after three multiple runs, and pairwise grading is conducted in a single run. Instead of using error bars, we report the consistency in assessment formats, Krippendorff's alpha for consistency in direct assessment, and transitivity statistics for consistency in pairwise ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Direct Assessment Results: Extended</head><p>Table 11 and 12 (on the next page) shows the extended results Table 3. Even when changing the metrics to either Kendall-Tau and Spearman, the overall trends are maintained. PROMETHEUS 2 shows superior evaluation performances among the open evaluator LMs, achieving high correlations with humans and proprietary LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D License</head><p>Our models are released under the Apache 2.0 license. The Preference Collection dataset is subject to OpenAI's Terms of Use for generated data. The model could be used for commercial purposes while the dataset is intended for research purposes. We used perspective API to ensure that the training data or evaluation datasets do not include PIIincluded instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Reference-free Evaluation in Direct Assessment Formats</head><p>In this section, we assess the impact of excluding a reference answer in evaluations conducted using direct assessment formats. The results are presented in Table <ref type="table" target="#tab_8">16</ref> (on the previous page). For this experiment, we employ FLASK (Ye et al., 2023) which includes human judgments and additionally the BiGGen Bench <ref type="bibr" target="#b21">(Kim et al., 2024)</ref>. The BiGGen Bench is a generation benchmark which includes a evaluation criteria tailored to each instance and provides 2840 human judgments (excluding the multilingual tasks) in direct assessment formats. Across both benchmarks and different evaluator LM variants, the correlation with humans diminishes when the reference answer is discarded. Even for GPT-4-1106, there is a significant performance degradation (0.045, 0.063). This suggests that including a reference answer is crucial for conducting effective evaluations with LMs. Interestingly, PROMETHEUS-2-7B achieves better performance in a reference-free setting (0.403, 0.425) than Mistral-7B-Instruct-v0.2 (0.310, 0.374). Similar trends are observed for PROMETHEUS-2-8X7B (0.424, 0.411) and <ref type="bibr">0.386)</ref>. This implies that one effect of training an evaluator LM with a reference answer included is to induce the ability to ground judgments to the given reference answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Merging Method Ablation</head><p>In this section, in addition to linear merging, we also test different merging techniques including:</p><p>• Slerp merging <ref type="bibr" target="#b11">(Goddard et al., 2024)</ref> operates by interpolating two weights θ d and θ p while preserving the geometric properties of the spherical space in which θ d and θ p reside. Specifically, this is conducted by normalizing θ d and θ p into unit length and then merging the two weights based on the coefficient α such as:</p><formula xml:id="formula_4">θ f inal = α × θ d ||θ d || + (1 -α) × θ p ||θ p ||<label>(4)</label></formula><p>• Task Arithmetic merging <ref type="bibr" target="#b13">(Ilharco et al., 2022)</ref> which can be expressed as follows:</p><formula xml:id="formula_5">θ f inal = θ init + α × (θ d -θ init )+ (1 -α) × (θ p -θ init )<label>(5)</label></formula><p>where θ init is the weight of the base model. However, we empirically find that the resulting evaluator LM θ f inal often does not generate valid scoring decisions (e.g., generating an integer during pairwise ranking).</p><p>• TIES merging <ref type="bibr" target="#b39">(Yadav et al., 2024)</ref>, while similar to Task Arithmetic merging, adds (1) a Trim operation to remove redundant weights in the task vector θ dθ init and θ pθ init and (2) Elect and Disjoint operations to resolve disagreement (i.e., opposite directed weights) between θ dθ init and θ pθ init . • DARE merging (Yu et al., 2023), while also similar to Task Arithmetic and TIES merging, performs a Random Drop and Re-scale operations in the task vector θ dθ init and θ pθ init to remove redundant weights. We find that DARE merging work best when we choose Mixtral-8x7B as our base model. DARE-linear merging is what was originally proposed by Yu et al. (2023). In DARE-TIES merging, the Elect operation from Yadav et al. ( <ref type="formula">2024</ref>) is additionally added after the Re-scale operation.</p><p>We conduct our experiments based on the implementation from MergeKit <ref type="bibr" target="#b11">(Goddard et al., 2024)</ref>. <ref type="foot" target="#foot_2">4</ref>In Table <ref type="table">17</ref> (on the previous page), we measure the performance of evaluator LMs employing different merging methods. In direct assessment benchmarks, DARE-Linear achieves the best performance, followed by DARE-TIES and Linear merging. In pairwise ranking benchmarks, Task Arithmetics achieves the best performance, with only a minimal difference compared to other methods. On average, DARE-Linear performs best. Based on these results, we have trained Prometheus-2-7B with DARE-Linear merging. We also opted to train Prometheus-2-8x7B using DARE-Linear merging. Although the optimal merging method might differ, we have not conducted additional experiments due to computational limitations. Future work could explore whether these findings hold true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H PREFERENCE COLLECTION Augmentation Prompt</head><p>Prompt for Generating Verbal Feedback in Pairwise Ranking ###Task Description: An instruction (might include an Input inside it), two responses to evaluate (denoted as Response A and Response B), a reference answer, and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback explaining why {sub_str}, focusing strictly on the aspects highlighted in the evaluation criteria. 2. While writing the feedback, make comparisons between Response A, Response B, and Reference Answer. Instead of examining Response A and Response B separately, go straight to the point and mention about the commonalities and differences between them. 3. While writing the feedback, do not start by mentioning {sub_str} in the first sentence. Instead, try to write a reasoning process that delves into the commonalities and differences of the two responses and mention {sub_str} at the last part of your justification. 4. Within the feedback, do not explicitly mention about the reference answer. For instance, do not use phrases like "Compared to the reference answer". Assume that you inherently know the reference answer which could be used to determine details that are not present in both responses under assessment. 5. Please do not generate any other opening, closing, and explanations. Just write the feedback. 6. Within the feedback, generate a string phrase "[END]" after you are finished. ###Instruction: {instruction} ###Response A: {response_A} ###Response B: {response_B} ###Reference Answer: {reference_answer} ###Score Rubric: {criteria} ###Feedback: I Direct Assessment Prompt Direct Assessment System Prompt You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance. Direct Assessment Prompt Template ###Task Description: An instruction (might include an Input inside it), a response to evaluate, and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: "Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: {orig_instruction} ###Response to evaluate: {orig_response} ###Score Rubrics: {score_rubric} ###Feedback: J Pairwise Ranking Prompt Pairwise Ranking System Prompt You are a fair judge assistant assigned to deliver insightful feedback that compares individual performances, highlighting how each stands relative to others within the same cohort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of direct assessment and pairwise ranking. Both responses could be considered decent under the umbrella of 'helpfulness'. However, the scoring decision might change based on a specific evaluation criterion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: When merging models, the influence of relative evaluation on absolute evaluation is greater than the influence of absolute evaluation on relative evaluation. Performance of Direct Assessment (colored in green) and Pairwise Ranking (colored in blue) when altering the α value to merge evaluator LMs trained on different formats.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of our evaluation benchmarks to assess the evaluation capabilities of evaluator LMs.</figDesc><table><row><cell>DARE-Linear showed the best performance, and</cell></row><row><cell>hence we used it to train the PROMETHEUS 2 (7B</cell></row><row><cell>&amp; 8x7B) models. Details on the hyper-parameters</cell></row><row><cell>for training and inference along with the prompt</cell></row><row><cell>templates are all listed in Appendix B, I, J.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Direct Assessment ResultsPearson correlations between reference evaluators (listed on top) and evaluator LMs.The best comparable statistics are bolded and second best underlined except proprietary LMs. Spearman and Kendall-Tau correlations are reported in Appendix C. Note that the Feedback Bench is an in-domain test set of the PROMETHEUS models.</figDesc><table><row><cell>Evaluator LM</cell><cell cols="3">VICUNA BENCH</cell><cell></cell><cell cols="2">MT BENCH</cell><cell></cell><cell>FLASK</cell><cell></cell><cell></cell><cell>Feedback Bench</cell></row><row><cell></cell><cell cols="10">GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus GPT-4-1106 Claude-3-Opus Humans</cell><cell>GPT-4-0613</cell></row><row><cell>LLAMA2-CHAT 7B</cell><cell>0.205</cell><cell></cell><cell>0.243</cell><cell></cell><cell>0.036</cell><cell>0.055</cell><cell>0.317</cell><cell>0.256</cell><cell>0.299</cell><cell></cell><cell>0.523</cell></row><row><cell>LLAMA2-CHAT 13B</cell><cell>0.185</cell><cell></cell><cell>0.141</cell><cell></cell><cell>-0.042</cell><cell>-0.002</cell><cell>0.239</cell><cell>0.247</cell><cell>0.263</cell><cell></cell><cell>0.545</cell></row><row><cell>LLAMA2-CHAT 70B</cell><cell>0.350</cell><cell></cell><cell>0.463</cell><cell></cell><cell>0.178</cell><cell>0.228</cell><cell>0.388</cell><cell>0.402</cell><cell>0.317</cell><cell></cell><cell>0.592</cell></row><row><cell>MISTRAL-INSTRUCT-7B</cell><cell>0.486</cell><cell></cell><cell>0.561</cell><cell></cell><cell>0.284</cell><cell>0.396</cell><cell>0.448</cell><cell>0.437</cell><cell>0.377</cell><cell></cell><cell>0.586</cell></row><row><cell>MIXTRAL-INSTRUCT-8X7B</cell><cell>0.566</cell><cell></cell><cell>0.579</cell><cell></cell><cell>0.551</cell><cell>0.539</cell><cell>0.483</cell><cell>0.495</cell><cell>0.420</cell><cell></cell><cell>0.673</cell></row><row><cell>PROMETHEUS-7B</cell><cell>0.484</cell><cell></cell><cell>0.528</cell><cell></cell><cell>0.378</cell><cell>0.382</cell><cell>0.352</cell><cell>0.331</cell><cell>0.348</cell><cell></cell><cell>0.847</cell></row><row><cell>PROMETHEUS-13B</cell><cell>0.492</cell><cell></cell><cell>0.534</cell><cell></cell><cell>0.404</cell><cell>0.477</cell><cell>0.462</cell><cell>0.470</cell><cell>0.449</cell><cell></cell><cell>0.860</cell></row><row><cell>AUTO-J (13B)</cell><cell>0.351</cell><cell></cell><cell>0.262</cell><cell></cell><cell>0.432</cell><cell>0.375</cell><cell>0.430</cell><cell>0.370</cell><cell>0.473</cell><cell></cell><cell>0.637</cell></row><row><cell>PROMETHEUS-2-7B</cell><cell>0.666</cell><cell></cell><cell>0.654</cell><cell></cell><cell>0.548</cell><cell>0.517</cell><cell>0.617</cell><cell>0.561</cell><cell>0.545</cell><cell></cell><cell>0.882</cell></row><row><cell>PROMETHEUS-2-8X7B</cell><cell>0.685</cell><cell></cell><cell>0.635</cell><cell></cell><cell>0.665</cell><cell>0.614</cell><cell>0.659</cell><cell>0.626</cell><cell>0.555</cell><cell></cell><cell>0.898</cell></row><row><cell>GPT-3.5-TURBO-0613</cell><cell>0.335</cell><cell></cell><cell>0.349</cell><cell></cell><cell>0.183</cell><cell>0.194</cell><cell>0.437</cell><cell>0.396</cell><cell>0.450</cell><cell></cell><cell>0.594</cell></row><row><cell>GPT-4-1106</cell><cell>/</cell><cell></cell><cell>0.694</cell><cell></cell><cell>/</cell><cell>0.717</cell><cell>/</cell><cell>0.736</cell><cell>0.679</cell><cell></cell><cell>0.753</cell></row><row><cell>CLAUDE-3-OPUS</cell><cell>0.694</cell><cell></cell><cell>/</cell><cell></cell><cell>0.717</cell><cell>/</cell><cell>0.736</cell><cell>/</cell><cell>0.573</cell><cell></cell><cell>0.788</cell></row><row><cell>Evaluator LM</cell><cell></cell><cell cols="3">HHH ALIGNMENT</cell><cell></cell><cell cols="2">MT BENCH HUMAN JUDG.</cell><cell cols="2">AUTO-J EVAL</cell><cell></cell><cell>Preference Bench</cell></row><row><cell></cell><cell>Help.</cell><cell>Harm.</cell><cell>Hon.</cell><cell>Other</cell><cell>Total Avg.</cell><cell>w/ TIE</cell><cell>w/o TIE</cell><cell>w/ TIE</cell><cell>w/o TIE</cell><cell cols="2">Instance-wise Criteria</cell></row><row><cell>LLAMA2-CHAT 7B</cell><cell>55.93</cell><cell>62.07</cell><cell>49.18</cell><cell>62.79</cell><cell>57.01</cell><cell>46.68</cell><cell>50.39</cell><cell>45.76</cell><cell>45.73</cell><cell></cell><cell>58.60</cell></row><row><cell>LLAMA2-CHAT 13B</cell><cell>71.19</cell><cell>77.59</cell><cell>60.66</cell><cell>62.79</cell><cell>68.33</cell><cell>51.22</cell><cell>49.61</cell><cell>47.84</cell><cell>43.28</cell><cell></cell><cell>63.00</cell></row><row><cell>LLAMA2-CHAT 70B</cell><cell>62.71</cell><cell>81.03</cell><cell>65.57</cell><cell>65.12</cell><cell>68.78</cell><cell>55.14</cell><cell>60.88</cell><cell>53.38</cell><cell>50.64</cell><cell></cell><cell>64.70</cell></row><row><cell>MISTRAL-INSTRUCT-7B</cell><cell>59.32</cell><cell>68.97</cell><cell>63.93</cell><cell>81.40</cell><cell>67.42</cell><cell>53.81</cell><cell>63.82</cell><cell>53.88</cell><cell>60.94</cell><cell></cell><cell>79.40</cell></row><row><cell>MIXTRAL-INSTRUCT-8X7B</cell><cell>83.05</cell><cell>87.93</cell><cell>67.21</cell><cell>69.77</cell><cell>77.38</cell><cell>51.85</cell><cell>71.42</cell><cell>53.81</cell><cell>73.50</cell><cell></cell><cell>84.00</cell></row><row><cell>PAIR RM (0.4B)</cell><cell>84.75</cell><cell>84.48</cell><cell>80.33</cell><cell>90.70</cell><cell>84.62</cell><cell>-</cell><cell>59.00</cell><cell>-</cell><cell>59.05</cell><cell></cell><cell>81.80</cell></row><row><cell>ULTRA RM (13B)</cell><cell>86.44</cell><cell>79.31</cell><cell>81.97</cell><cell>88.37</cell><cell>83.71</cell><cell>-</cell><cell>56.00</cell><cell>-</cell><cell>59.85</cell><cell></cell><cell>86.97</cell></row><row><cell>AUTO-J (13B)</cell><cell>77.97</cell><cell>79.31</cell><cell>70.49</cell><cell>74.42</cell><cell>75.57</cell><cell>42.56</cell><cell>69.12</cell><cell>43.46</cell><cell>76.64</cell><cell></cell><cell>81.35</cell></row><row><cell>PROMETHEUS-2-7B</cell><cell>72.78</cell><cell>79.31</cell><cell>77.05</cell><cell>76.74</cell><cell>74.66</cell><cell>50.45</cell><cell>70.78</cell><cell>54.96</cell><cell>75.07</cell><cell></cell><cell>93.25</cell></row><row><cell>PROMETHEUS-2-8X7B</cell><cell>84.75</cell><cell>96.55</cell><cell>81.97</cell><cell>76.74</cell><cell>85.52</cell><cell>55.07</cell><cell>71.96</cell><cell>58.41</cell><cell>79.98</cell><cell></cell><cell>90.65</cell></row><row><cell>GPT-3.5-TURBO-0613</cell><cell>77.97</cell><cell>81.03</cell><cell>77.05</cell><cell>67.44</cell><cell>76.47</cell><cell>54.65</cell><cell>69.41</cell><cell>45.98</cell><cell>72.13</cell><cell></cell><cell>75.05</cell></row><row><cell>GPT-4-1106-PREVIEW</cell><cell>89.83</cell><cell>96.55</cell><cell>91.80</cell><cell>83.72</cell><cell>90.95</cell><cell>60.38</cell><cell>79.90</cell><cell>52.80</cell><cell>83.12</cell><cell></cell><cell>85.50</cell></row><row><cell>CLAUDE-3-OPUS</cell><cell>91.53</cell><cell>100.00</cell><cell>91.80</cell><cell>95.35</cell><cell>94.57</cell><cell>55.35</cell><cell>77.65</cell><cell>60.70</cell><cell>82.92</cell><cell></cell><cell>89.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Single-Format Training vs Joint Training vs Weight Merging Pearson correlations between evaluator LMs trained with different methods and GPT-4-1106. Evaluator LMs trained with weight merging outperform single-format-trained and jointly-trained evaluator LMs across multiple benchmarks.</figDesc><table><row><cell>Training Data Evaluation Format</cell><cell cols="4">DIRECT ASSESSMENT BENCHMARKS</cell><cell cols="3">PAIRWISE RANKING BENCHMARKS</cell><cell></cell></row><row><cell></cell><cell>Vicuna Ben.</cell><cell>MT Ben.</cell><cell>FLASK</cell><cell>Average</cell><cell>HHH Align.</cell><cell>MT Ben. H.J.</cell><cell>Auto-J Eval</cell><cell>Average</cell></row><row><cell>NO TRAINING (PROMPTING)</cell><cell>0.486</cell><cell>0.284</cell><cell>0.480</cell><cell>0.417</cell><cell>67.42</cell><cell>63.82</cell><cell>60.94</cell><cell>64.06</cell></row><row><cell>DIRECT ASSESSMENT ONLY</cell><cell>0.537</cell><cell>0.561</cell><cell>0.519</cell><cell>0.539</cell><cell>73.33</cell><cell>56.76</cell><cell>64.38</cell><cell>64.82</cell></row><row><cell>PAIRWISE RANKING ONLY</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.73</cell><cell>67.06</cell><cell>72.03</cell><cell>72.61</cell></row><row><cell>DIRECT ASSESSMENT &amp; DIRECT ASSESSMENT</cell><cell>0.552</cell><cell>0.493</cell><cell>0.505</cell><cell>0.517</cell><cell>73.30</cell><cell>55.00</cell><cell>63.69</cell><cell>64.13</cell></row><row><cell>PAIRWISE RANKING &amp; PAIRWISE RANKING</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.70</cell><cell>65.20</cell><cell>72.72</cell><cell>72.21</cell></row><row><cell>DIRECT ASSESSMENT &amp; PAIRWISE RANKING</cell><cell>0.666</cell><cell>0.548</cell><cell>0.659</cell><cell>0.624</cell><cell>74.66</cell><cell>70.78</cell><cell>75.07</cell><cell>73.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Unifying Formats vs Ensembling Pearson correlations with GPT-4-1106 (Vicuna Bench, MT Bench, FLASK) and agreement with human evaluators (HHH Alignment, MT Bench Human Judgment, Auto-J Eval). Merging models trained with the same evaluation formats (ensembling) underperforms merging models trained with different formats (unifying formats).</figDesc><table><row><cell>Direct Assessment Pearson Correlation</cell><cell></cell><cell>Pairwise Ranking Agreement Accuracy</cell></row><row><cell cols="3">(Direct Assessment : Pairwise Ranking) Merging Ratio</cell></row><row><cell>Direct Assessment Correlation</cell><cell>Pairwise Ranking Accuracy</cell><cell>Average Performance</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/huggingface/alignment-handbook</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/vllm-project/vllm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/arcee-ai/mergekit</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the <rs type="institution">KAIST AI LKLab members</rs> for helpful discussions. This work was partly supported by <rs type="funder">LG AI</rs> <rs type="grantName">Research grant</rs> (<rs type="projectName">Self-improving logical reasoning capabilities of LLMs</rs>, <rs type="grantNumber">2024</rs>, <rs type="grantNumber">50%</rs>) and</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Aj2WMaU">
					<idno type="grant-number">2024</idno>
					<orgName type="grant-name">Research grant</orgName>
					<orgName type="project" subtype="full">Self-improving logical reasoning capabilities of LLMs</orgName>
				</org>
				<org type="funding" xml:id="_CjntJmA">
					<idno type="grant-number">50%</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Consistency of Evaluator LMs</head><p>In addition to obtaining high correlation and accuracy, achieving high consistency is another important aspect for evaluator LMs. We first test if evaluator LMs could give consistent scoring decisions in direct assessment formats. We inferencing multiple times with non-deterministic decoding (e.g., using temperature 1.0). Following the experimental design from Ye et al. ( <ref type="formula">2023</ref>), we choose to inference 3 times and report the Krippendorff's alpha value.</p><p>As shown in Table <ref type="table">14</ref>, the results indicate that training on feedback data only slightly improves consistency. On the other hand, we find that the LMs with a large number of parameters achieve high consistency. This indicates the importance of selecting a large LM as the base model when training an evaluator LM. Notably, PROMETHEUS-2-8X7B obtains the highest correlation among open evaluator LMs. Moreover, to evaluate consistency in pairwise ranking settings (Table <ref type="table">15</ref>), we measure transitivity (i.e., a higher score for response B over A, and for C over B, results in a higher score for C over A). As shown in Table <ref type="table">15</ref>, the PROMETHEUS 2 models achieve performances on par with GPT-4, showing that they could provide robust judgments in pairwise ranking schemes.</p><p>Lastly, we conduct an experiment to test if evaluator LMs could achieve consistent scores across different evaluation formats. To do this, we use pairwise ranking benchmarks and measure the performance differences when prompted with direct assessment formats and pairwise ranking formats. Specifically, following <ref type="bibr" target="#b20">Kim et al. (2023)</ref>, to process pairwise ranking datasets in a direct assessment scheme, we evaluate each response separately and compare the scoring decisions. We mark it as correct if the evaluator LM provides a higher score for the human-chosen response over the rejected one. As shown in Table <ref type="table">13</ref> (on the previous page), the results show that PROMETHEUS 2 models show lower performance differences across evaluation formats, indicating their robustness.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<editor>Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam Mc-Candlish, Chris Olah</editor>
		<imprint/>
	</monogr>
	<note>and Jared Kaplan. 2021. A general language assistant as a laboratory for alignment</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<title level="m">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.07201</idno>
		<title level="m">Chateval: Towards better llm-based evaluators through multi-agent debate</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vicuna: An opensource chatbot impressing gpt-4</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>with 90%* chatgpt quality</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guotong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01377</idno>
		<title level="m">Ultrafeedback: Boosting language models with high-quality feedback</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Elad</forename><surname>Shachar Don-Yehiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Venezian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leshem</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><surname>Choshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.01378</idno>
		<title level="m">Cold fusion: Collaborative descent for distributed multitask finetuning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Alpacafarm: A simulation framework for methods that learn from human feedback</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori B</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14387</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06063</idno>
		<title level="m">Bleu might be guilty but references are not innocent</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Mingqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.01383</idno>
		<title level="m">Llm-based nlg evaluation: Current status and challenges</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karmanya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aremu</forename><surname>Anuoluwapo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miruna</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Clinciu</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
		<author>
			<persName><surname>Kaustubh D Dhole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01672</idno>
		<title level="m">The gem benchmark: Natural language generation, its evaluation and metrics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abinaya</forename><surname>Mahendiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Papangelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11249</idno>
	</analytic>
	<monogr>
		<title level="m">Multilingual nlg benchmarking in a single line of code</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Scaling expert language models with unsupervised domain discovery</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Goddard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamane</forename><surname>Siriwardhana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malikeh</forename><surname>Ehghaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Benedict</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mcquade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Solawetz ; Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.13257</idno>
		<idno>arXiv:2303.14177</idno>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Arcee&apos;s mergekit: A toolkit for merging large language models</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fine-grained analysis of bertscore</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Machine Translation</title>
		<meeting>the Sixth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04089</idno>
		<title level="m">Editing models with task arithmetic</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.11564</idno>
		<title level="m">Personalized soups: Personalized large language model alignment via post-hoc parameter merging</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Kyungjae Lee, and Minjoon Seo. 2023b. Exploring the benefits of training expert language models over instruction tuning</title>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonghyeon</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03202</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<imprint/>
	</monogr>
	<note>et al. 2023a. Mistral 7b. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blanche</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Savary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><forename type="middle">Bou</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><surname>Bressand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.04088</idno>
		<title level="m">Mixtral of experts</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">2023b. Tigerscore: Towards building explainable metric for all text generation tasks</title>
		<author>
			<persName><forename type="first">Dongfu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yishan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00752</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Dongfu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02561</idno>
		<title level="m">Llm-blender: Ensembling large language models with pairwise ranking and generative fusion</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Prometheus: Inducing fine-grained evaluation capability in language models</title>
		<author>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwaran</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongjin</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08491</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models</title>
		<author>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyoung</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">Yong</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaeeun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongkeun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guijin</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheikh</forename><surname>Shafayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.05761</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Pyatkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyathi</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Zick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.13787</idno>
		<title level="m">Rewardbench: Evaluating reward models for language modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Seongyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geewook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.06591</idno>
		<title level="m">Prometheusvision: Vision-language model as a judge for fine-grained evaluation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Run-Ze</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05470</idno>
		<title level="m">Generative judge for evaluating alignment</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Branch-train-merge: Embarrassingly parallel training of expert language models</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.03306</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">2023b. Alpacaeval: An automatic evaluator of instructionfollowing models</title>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/alpaca_eval" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Chen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.07103</idno>
		<title level="m">Leveraging large language models for nlg evaluation: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Ruochen Xu, and Chenguang Zhu. 2023a. G-eval: Nlg evaluation using gpt-4 with better human alignment</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16634</idno>
		<title level="m">Ruochen Xu, and Chenguang Zhu. 2023b. Gpteval: Nlg evaluation using gpt-4 with better human alignment</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Merging models with fisher-weighted averaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="17703" to="17716" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing Systems, 36. Natalie Schluter. 2017. The limits of automatic summarisation according to rouge</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Rame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Couairon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Gaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Shukor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Wen-tau Yih, Jason Weston, et al. 2024. Branch-train-mix: Mixing expert llms into a mixture-of-experts llm</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Golovneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.07816</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Blecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian Canton</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Esiobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jude</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Fuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saghar</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Inan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Kloumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Korenev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenya</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushkar</forename><surname>Mishra</surname></persName>
		</author>
		<title level="m">Igor Molybog</title>
		<editor>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jeremy</forename><surname>Reizenstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rashi</forename><surname>Rungta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalyan</forename><surname>Saladi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alan</forename><surname>Schelten</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruan</forename><surname>Silva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ranjan</forename><surname>Subramanian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ellen</forename><surname>Xiaoqing</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Binh</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ross</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adina</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jian</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Puxin</forename><surname>Xiang Kuan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iliyan</forename><surname>Yan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuchen</forename><surname>Zarov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Angela</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Melanie</forename><surname>Fan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sharan</forename><surname>Kambadur</surname></persName>
		</editor>
		<editor>
			<persName><surname>Narang</surname></persName>
		</editor>
		<imprint>
			<publisher>Aurelien Rodriguez</publisher>
		</imprint>
	</monogr>
	<note>Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards</title>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.18571</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Sui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.08935</idno>
		<title level="m">Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Xing Xie, et al. 2023b. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization</title>
		<author>
			<persName><forename type="first">Yidong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengran</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoya</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05087</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ties-merging: Resolving interference when merging models. Ad-vances in Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leshem</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.10928</idno>
	</analytic>
	<monogr>
		<title level="m">Flask: Fine-grained language model evaluation based on alignment skill sets</title>
		<editor>
			<persName><forename type="first">Seonghyeon</forename><surname>Ye</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Doyoung</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hyeonbin</forename><surname>Hwang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Seungone</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yongrae</forename><surname>Jo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Juho</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2024. 2023</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.03099</idno>
		<title level="m">Language models are super mario: Absorbing abilities from homologous models as a free lunch</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09675</idno>
		<title level="m">Bertscore: Evaluating text generation with bert</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05685</idno>
		<title level="m">Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Lianghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.17631</idno>
		<title level="m">Judgelm: Fine-tuned large language models are scalable judges</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
