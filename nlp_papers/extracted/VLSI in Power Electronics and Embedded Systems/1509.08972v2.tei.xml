<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2016-08-24">24 Aug 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">VLSI Implementation of Deep Neural Network Using Integral Stochastic Computing</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-08-24">24 Aug 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">DADF4A8D0091A1A0B0A3A5A49A9518B3</idno>
					<idno type="arXiv">arXiv:1509.08972v2[cs.NE]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-28T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep neural network</term>
					<term>machine learning</term>
					<term>hardware implementation</term>
					<term>integral stochastic computation</term>
					<term>pattern recognition</term>
					<term>Very Large Scale Integration (VLSI)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The hardware implementation of deep neural networks (DNNs) has recently received tremendous attention: many applications in fact require high-speed operations that suit a hardware implementation. However, numerous elements and complex interconnections are usually required, leading to a large area occupation and copious power consumption. Stochastic computing has shown promising results for low-power area-efficient hardware implementations, even though existing stochastic algorithms require long streams that cause long latencies. In this paper, we propose an integer form of stochastic computation and introduce some elementary circuits. We then propose an efficient implementation of a DNN based on integral stochastic computing. The proposed architecture has been implemented on a Virtex7 FPGA, resulting in 45% and 62% average reductions in area and latency compared to the best reported architecture in literature. We also synthesize the circuits in a 65 nm CMOS technology and we show that the proposed integral stochastic architecture results in up to 21% reduction in energy consumption compared to the binary radix implementation at the same misclassification rate. Due to fault-tolerant nature of stochastic architectures, we also consider a quasi-synchronous implementation which yields 33% reduction in energy consumption w.r.t. the binary radix implementation without any compromise on performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recently, the implementation of biologically-inspired artificial neural networks such as the Restricted Boltzmann Machine (RBM) has aroused great interest due to their high performance in approximating complicated functions. A variety of applications can benefit from them, in particular machine learning algorithms. They can be split in two phases, which are referred to as learning and inference phases <ref type="bibr" target="#b1">[2]</ref>. The learning engine finds a proper configuration to map learning input data into their desired outputs, while the inference engine uses the extracted configuration to compute outputs for new data.</p><p>Deep neural networks, especially Deep Belief Networks (DBN), have shown state-of-the-art results on various computer vision and recognition tasks <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b7">[8]</ref>. DBN can be formed by stacking RBMs on top of each other to construct a deep network, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>  <ref type="bibr" target="#b3">[4]</ref>. RBMs used in DBN are pretrained using Gradient-based Contrastive Divergence (GCD) algorithms, followed by gradient descent and backpropagation algorithms for classification and fine-tuning the results <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>A preliminary version of this paper was published in <ref type="bibr" target="#b0">[1]</ref>. In the past few years, general purpose processors have been mainly used for software realization of both training and inference engines of DBN. However, large power consumption and high resource utilization have pushed researchers to explore ASIC and FPGA implementations of neural networks. Rapid expansion of devices and sensors connected to the internet of things (IoT) allows to perform the training procedure once on cloud servers equipped with Graphics Processing Unit (GPU), and extract weights for inference engine usage through the IoT platforms. The inference engine can then be implemented using ASIC or FPGA platforms.</p><p>DBNs are constructed of multiple layers of RBMs and a classification layer at the end. The main computation kernel consists of hundreds of vector-matrix multiplications followed by non-linear functions in each layer. Since multiplications are costly to implement in hardware, existing parallel or semiparallel VLSI implementations of such a network suffer from high silicon area and power consumption <ref type="bibr" target="#b8">[9]</ref>. The nonlinearity function is also implemented using Look-Up Tables (LUTs), requiring large memories. Moreover, hardware implementation of this network results in large silicon area: this is caused by the connections between layers, that lead to severe routing congestion. Therefore, an efficient VLSI implementation of DBN is still an open problem.</p><p>Recently, Stochastic Computing (SC) has shown promising results for ultra low-cost and fault-tolerant hardware implementation of various systems <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Using SC, many computational units have simple implementation. For instance, using unipolar SC, the multiplication and addition are implemented using an AND gate and a multiplexer, respectively <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, the multiplexer-based adder introduces a scaling factor that can cause a precision loss <ref type="bibr" target="#b21">[22]</ref>, resulting in the failure of SC for deep neural networks, which require many additions. An OR gate can provide a good approximation to addition if its input values are small <ref type="bibr" target="#b20">[21]</ref>. However, using OR gates to perform addition in DBNs results in a huge misclassification error compared to its fixed-point hardware implementation. Therefore, an efficient stochastic implementation that maintains the performance of DBN is still missing.</p><p>In this paper, an integral stochastic computation is introduced to solve the precision loss issue of conventional scaled-adder, while also reducing the latency compared to conventional binary stochastic computation. It is also worth mentioning that the proposed technique results in lower latency compared to conventional binary stochastic computation. A novel Finite State Machine (FSM)-based tanh function is then proposed as the nonlinearity function used in DBN. Finally, an efficient stochastic implementation of DBN based on the aforementioned techniques with an acceptable misclassification error is proposed, resulting in 45% smaller area on average compared to the state-of-the-art stochastic architecture.</p><p>A nanoscale memory-resistor (memristor) device is a nonvolatile digital memory, which consumes substantially less energy compared to CMOS and can be scaled to sizes below 10 nm <ref type="bibr" target="#b22">[23]</ref>. A challenging problem with memristor devices is the presence of significant random variations. A promising approach for dealing with the non-determinism of memristors is to design SC systems that are fault-tolerant <ref type="bibr" target="#b22">[23]</ref>. In this paper, we show that the proposed architectures can tolerate a fault rate of up to 16% when timing violations are allowed to occur, making them suitable for memristor devices.</p><p>The manuscript can be divided in two major parts: the proposed algorithms and their hardware implementation results. In the first part, we analyze elementary computational units. Also, some simulation results and examples are provided to shed light on the proposed algorithm in comparison with the existing methods. In the second part, design aspects of a deep neural network based on the proposed method are studied and some implementation results under different conditions are provided. The rest of this paper is organized as follows. Section II provides a review of SC and its computational elements. Section III introduces the proposed integral stochastic computation and operations in this domain. Section IV describes the integral stochastic implementation of DBN. Implementation results of the proposed architecture is provided in Section V. In this section, the performance of the stochastic implementation is studied when the circuit is affected by timing violations. Note that accepting occasional timing violations allows to reduce the supply voltage, which can improve the energy efficiency of the system. In Section VI, we conclude the paper and discuss future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AND</head><p>A: 1,0,1,0,0,0,0,0 (2/8) B: 1,0,0,1,0,1,0,1 (4/8) Y: 1,0,0,0,0,0,0,0 (1/8) (a)</p><p>A: 1,0,1,0,0,0,0,0 (-6/8) B: 1,0,0,1,0,1,0,1 (0) Y: 1,1,0,0,1,0,1,0 (0) XNOR </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. STOCHASTIC COMPUTING AND ITS COMPUTATIONAL ELEMENTS</head><p>In stochastic computation, numbers are represented as sequences of random bits. The information content of the sequence does not depend on the particular value of each bit, but rather on their statistics. Let us denote by X ∈ {0, 1} a bit in the random sequence. To represent a real number x ∈ [0, 1], we simply generate the sequence such that:</p><formula xml:id="formula_0">E[X] = x,<label>(1)</label></formula><p>where E[X] denotes the expected value of the random variable X. This is known as the unipolar format. The bipolar format is another commonly used format where x ∈ [-1, 1] is represented by setting:</p><formula xml:id="formula_1">E[X] = (x + 1)/2. (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Note that any real number can be represented in one of these two formats by scaling it down to fit within the appropriate interval. In this paper, we use upper case letters to represent elements of a stochastic stream, while lower case letters represent the real value associated with that stream. It is also worth mentioning that a stochastic stream of a real value x is usually generated by a linear feedback shift register (LFSR) and a comparator. This unit is hereafter referred to as binary to stochastic convertor (B2S) <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multiplication In SC</head><p>Multiplication of two stochastic streams is performed using AND and XNOR gates in unipolar and bipolar encoding formats, respectively, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>(a) and 2(b). In unipolar format, the multiplication of two input stochastic streams of A and B is computed as:</p><formula xml:id="formula_3">Y = AND (A, B) = A • B,<label>(3)</label></formula><p>where "• • • " denotes bit-wise AND and if the input sequences are independent, we have:</p><formula xml:id="formula_4">y = E[Y ] = a × b.<label>(4)</label></formula><p>Multiplications in bipolar format can be performed as:</p><formula xml:id="formula_5">Y = XNOR (A, B) = OR (A • B, (1 -A) • (1 -B)) ,<label>(5)</label></formula><formula xml:id="formula_6">E[Y ] = E[A • B] + E[(1 -A) • (1 -B)].<label>(6)</label></formula><p>A: 1,0,1,0,</p><p>B: 1,0,0,0,0,0,1,0 (2/8) Y: 1,0,1,0,1,0,1,0 (4/8) S: 1,0,0,1,0,1,0,1 (4/8) 0 1 (a)</p><p>A: 1,0,1,0,0,0,0,0 (2/8) B: 0,1,0,0,0,1,0,1 (3/8) Y: 1,1,1,0,0,1,0,1 (5/8) OR If the input streams are independent,</p><formula xml:id="formula_8">E[Y ] = E[A] × E[B] + E[1 -A] × E[1 -B].<label>(7)</label></formula><p>By simplifying the above equation, we have:</p><formula xml:id="formula_9">y = 2E[Y ] -1 = (2E[A] -1) × (2E[B] -1) .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Addition In SC</head><p>Additions in SC are usually performed by using either scaled adders or OR gates <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. The scaled adder uses a multiplexer (MUX) to perform addition. The output of a MUX Y is given by</p><formula xml:id="formula_10">Y = A • S + B • (1 -S) .<label>(9)</label></formula><p>As a result, the expected value of Y would be (E[A]+E[B])/2 when the select signal S is a stochastic stream with probability of 0.5, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>(a). This 2-input scaled adder ensures that its output is in the legitimate range of each encoding format by scaling it down by factor of 2. Therefore, L-input addition can be performed by using a tree of multiple 2-input MUXs. In general, the result of an L-input scaled adder is scaled down L times, which can decrease the precision of the stream. To achieve the desired accuracy, longer bit-streams must be used, resulting in larger latency.</p><p>OR gates can also be used as approximate adders as shown in Fig. <ref type="figure" target="#fig_2">3(b</ref>). The output Y of an OR gate with inputs A, B can be expressed as</p><formula xml:id="formula_11">Y = A + B -A • B. (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>OR gates function as adders only if E[AB] is close to 0. Therefore, the inputs should first be scaled down to ensure that the aforementioned conditions are met. This type of adder still requires long bit-streams to overcome a precision loss incurred by the scaling factor.</p><p>To overcome this precision loss, which could potentially lead to inaccurate results, the Accumulative Parallel Counter (APC) is proposed in <ref type="bibr" target="#b21">[22]</ref>. The APC takes N parallel bits as inputs and adds them to a counter in each clock cycle of the system. Therefore, this adder results in lower latency due to its small variance of the sum. It is also worth mentioning that this adder converts the stochastic stream to binary form <ref type="bibr" target="#b21">[22]</ref>. Therefore, this adder is restricted to cases where additions are performed to obtain the final result, or requiring an intermediate result in binary format. Stochastic Stream X 1 : 1,0,1,0,1,1,1,1 (0.75)</p><formula xml:id="formula_13">S0 S1 Sn/2-1 Sn/2 Sn-2 Sn-1 X X' Y = 0 Y = 1 (a) S0 S1 Sn-G-1 Sn-G Sn-2 Sn-1 X X' Y = 1 Y = 0 (b)</formula><formula xml:id="formula_14">Stochastic Stream X 2 : 1,1,1,0,1,0,1,1 (0.75) (a) X 1 X 2</formula><p>Integer stochastic stream S: 2,1,2,0, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. FSM-Based Functions In SC</head><p>Hyperbolic tangent and exponentiation functions are computations required by many applications. These functions are implemented in the stochastic domain by using a FSM <ref type="bibr" target="#b24">[25]</ref>. Fig. <ref type="figure" target="#fig_3">4</ref>(a) and 4(b) show the state transition diagram of the FSM implementing tanh and exponentiation functions. The FSM is constructed such that</p><formula xml:id="formula_16">tanh nx 2 ≈ E[Stanh (n, X)],<label>(11)</label></formula><formula xml:id="formula_17">exp (-2Gx) ≈ E[Sexp (n, G, X)] : x &gt; 0. (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>where n denotes the number of states in the FSM, G the linear gain of the exponentiation function and Y the stochastic output sequence. Let us define as Stanh and Sexp the approximated functions of tanh and exp in stochastic domain. It is worth mentioning that both input and output of the Stanh function are in bipolar format, while the input and output of the Sexp function are in bipolar and unipolar formats respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED INTEGRAL STOCHASTIC COMPUTING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generation of Integer Stochastic Stream</head><p>An integer stochastic stream is a sequence of integer numbers which are represented by either 2's complement or sign and magnitude. The average value of this stream is a real number s ∈ [0, m] for unipolar format and s ∈ [-m, m] for bipolar format, where m ∈ {1, 2, . . . }. In other words, the real value s is the summation of two or more binary stochastic stream probabilities. For instance, 1.5 can be expressed as 0.75 + 0.75. Each of these probabilities can be represented by a conventional binary stochastic stream as shown in Fig. <ref type="figure" target="#fig_4">5(a)</ref>. Therefore, the integer stochastic representation of 1.5 can be readily achieved as a summation of generated binary stochastic streams as illustrated in Fig. <ref type="figure" target="#fig_4">5(b)</ref>. In general, the integer stochastic stream S representing the real value s is a sequence with elements S i , i = {1, 2, . . . , N }:</p><formula xml:id="formula_19">X: 1,1,1,0,1,1,1,1 (0.875) Integer Stochastic Computational Element Stochastic Stream of 0.5625: 1,0,1,0,1,0,1,1 , 0,1,0,0,1,0,1,1 S: 1,1,1,0,2,0,2,2 (9/16) (a) Stochastic Stream X of 0.875: 1,1,1,0,1,1,1,1,1,1,1,1,1,0,1,1 Stochastic Stream Y of 0.5625: 1,0,1,0,1,0,1,1,0,1,0,0,1,0,1,1 Stochastic Computational Element X(1:8): 1,1,1,0,1,1,1,1 S(1:8): 1,0,1,0,1,0,1,1 Stochastic Computational Element X(9:16): 1,1,1,1,1,0,1,1 S(9:16): 0,1,0,0,1,0,1,1 Y(1:8) Y(9:16) (b)</formula><formula xml:id="formula_20">S i = m j=1 X j i ,<label>(13)</label></formula><p>where X j i denotes each element of a binary stochastic sequence representing a real value x j . The expected value of the integer stochastic stream is then given by</p><formula xml:id="formula_21">s = E[S i ] = m j=1 x j .<label>(14)</label></formula><p>We can also generate integer stochastic streams in the bipolar format. In that case, the elements S i of the stream are given by:</p><formula xml:id="formula_22">S i = 2 × m j=1 X j i -m,<label>(15)</label></formula><p>and the value represented by the stream is</p><formula xml:id="formula_23">s = E[S i ] = 2 × m j=1 E[X j i ] -m = 2 × m j=1 x j -m. (<label>16</label></formula><formula xml:id="formula_24">)</formula><p>Any real number can be approximated by using an integer stochastic stream without prior scaling, as opposed to a conventional stochastic stream which is restricted only to the [-1, 1] interval. In integral SC, computation on two streams with different effective length is also possible while conventional SC fails to provide this property. For instance, representation of 0.875 and 0.5625 require effective bit-stream lengths of 8 and 16, respectively, using conventional SC. Therefore, effective bit-stream lengths of 16 is used to generate the conventional stochastic bit-stream of these two numbers for operations. However, the second number which requires higher effective length, i.e., 0.5625 in this example, can be generated by using the proposed integral SC with m = 2 as shown in Fig. <ref type="figure" target="#fig_5">6</ref>(a). In this case, the bit-stream length of 8 is used for both numbers and operations can be performed by using lower lengths w.r.t. conventional SC. This technique potentially reduces the latency brought by stochastic computations, making integral SC suitable for throughput-intensive applications. It is worth mentioning that the integral SC is different from the conventional parallelized SC <ref type="bibr" target="#b25">[26]</ref>. For the sake of clarity, the aforementioned example is illustrated in Fig. <ref type="figure" target="#fig_5">6</ref>(b) by using the conventional parallelized SC by factor of two. This is due to the fact that if several copies of a binary SC system are instantiated, the inputs still need to have the same effective length.</p><p>In summary, a real number s ∈ [0, m] is first divided into the summation of multiple numbers which are in [0, 1] interval. Then, the integer stochastic stream of this number is generated by using column-wise addition (see equations ( <ref type="formula" target="#formula_20">13</ref>)-( <ref type="formula" target="#formula_21">14</ref>)). The bipolar format of the integer stochastic stream is generated in a similar way. Note that the binary to integer stochastic convertor is hereafter referred to as B2IS and it is composed of m B2S convertors followed by and adder as shown in Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implicit Scaling of Integer Stochastic Stream</head><p>The integer stochastic representation of a real number s ∈ [0, 1] can also be generated by using an implicit scaling factor. In this method, the expected value of the individual binary streams is chosen as x j = s, and the value s represented by the integer stream is given by</p><formula xml:id="formula_25">s = E[S i ] m . (<label>17</label></formula><formula xml:id="formula_26">)</formula><p>This method avoids the need to divide s by m to obtain x j , and can be easily taken into account in subsequent computations.</p><p>For instance, a real number 9/16 can be represented using an integer stream length of 8 with m = 2. We can set x j = 9/16 (with an implicit scaling factor of 1/2) and generate two binary sequences of length 8. These sequences are then added together to form the integer sequence S. We obtain </p><formula xml:id="formula_27">Data: Stochastic stream X i ∈ {0, 1} where i ∈ {1, 2, ..., N } Result: Y i Counter ← Initial value; for i ← 1 : N do Counter ← Counter + 2X i -1; if Counter &gt; n-1 then Counter ← n-1; end if Counter &lt; 0 then Counter ← 0; end if Counter &gt; of f set then Y i ← 1; else Y i ← 0; end end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multiplication In Integral SC</head><p>The main advantage of SC compared to its binary radix format is the low complexity implementation of mathematical operations. It is shown that multiplication can be implemented by using AND or XNOR gates depending on the coding format. However, integer stochastic multipliers make use of binary radix multipliers (see Fig. <ref type="figure" target="#fig_6">7(a)</ref>). The multiplication of two real numbers s 1 ∈ [0, m] and s 2 ∈ [0, m ] with integer stochastic streams S 1 and S 2 in unipolar format is performed as follows:</p><formula xml:id="formula_28">y = s 1 × s 2 = E[S 1 i × S 2 i ] = E[S 1 i ] × E[S 2 i ],<label>(18)</label></formula><p>if S 1 i and S 2 i are independent. The above equation holds true for integer stochastic multiplication in bipolar format as well. The implementation cost of this multiplier strongly depends on m and m . Considering one of these two values to be equal to "1", the multiplication can be implemented using bit-wise AND gate or a MUX as depicted in Fig. <ref type="figure" target="#fig_6">7(b</ref>). The range of y is [0, m × m ] in the unipolar case, and [-m × m , m × m ] in the bipolar case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Addition In Integral SC</head><p>Conventional SC suffers from precision loss incurred by using scaled adder, making SC inappropriate for applications which require many additions. On the other hand, integral SC uses binary radix adders to perform additions in this domain, preserving all information. Using <ref type="bibr" target="#b13">(14)</ref>, addition in unipolar format is performed as follows: </p><formula xml:id="formula_29">y = s 1 + s 2 = E[s 1 + s 2 ] = E[S 1 i ] + E[S 2 i ],<label>(19)</label></formula><formula xml:id="formula_30">Y i Counter ← Initial value; for i ← 1 : N do Counter ← Counter + S i ; if Counter &gt; n × m-1 then Counter ← n × m-1; end if Counter &lt; 0 then Counter ← 0; end if Counter &gt; of f set then Y i ← 1; else Y i ← 0; end end Algorithm 2:</formula><p>Pseudo code of the proposed algorithm for integer stochastic FSM-based functions unipolar and bipolar formats respectively. This adder provides some advantages similar to APC. First of all, due to the fact that it retains all information provided as inputs, it reduces the variance of the sum. Secondly, it potentially reduces the bit-stream length required for computations compared to conventional SC <ref type="bibr" target="#b21">[22]</ref>. Moreover, the output of this adder is still an integer stochastic stream, which can be used by subsequent stochastic computational units, as opposed to APC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. FSM-Based Functions In Integral SC</head><p>The inputs of stochastic FSM-based tanh and exponentiation functions are restricted to real values in the [-1, 1] interval. Therefore, a desired tanh or exponentiation function can be achieved by scaling down the inputs and adjusting the term n in ( <ref type="formula" target="#formula_16">11</ref>) and <ref type="bibr" target="#b11">(12)</ref>, which potentially increases bit-stream length and results in long latency. The transition between each state of FSM is performed according to the input value in bipolar format, which is either 1 or 0. This state transition can be formulated as shown in Algorithm 1 in conventional SC. According to the Algorithm 1, the input value in bipolar format is first converted to either 1 or -1 as an input of either 1 or 0, respectively. Then, the counter of FSM is added with the new encoded values which are similar to the values in an integral stochastic stream with m = 1. Therefore, the values of the conventional stochastic stream can be viewed as hard values of an integral stochastic stream. The FSM-based functions in integral SC can be achieved by extending the conventional FSM-based functions to support soft values in integral SC, which is explained below.</p><p>The integer stochastic tanh and exponentiation functions are proposed by generalizing Alg. 1. In integral SC, each element of a stochastic stream is represented using 2's complement or sign-magnitude representations in {-m, . . . , m} for bipolar format. A state counter is increased or decreased according to the integer input value S i ∈ {-m, . . . m} where i ∈ 0 0.5 1 1.5 2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 s Output Exp(-s) NSexp(512,1,S), m=2 NSexp(1024,2,S), m=4 NSexp(2048,4,S), m=8 (a) 0 0.5 1 1.5 2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 s Output Exp(-2s) NSexp(1024,2,S), m = 2 NSexp(2048,4,S), m = 4 NSexp(4096,8,S), m = 8 Sexp(512,1,X) (b) Fig. 9. (a) Integer stochastic implementation of exp(-s) and (b) Integer stochastic implementation of exp(-2s) {1, 2, ..., N }. Therefore, the state counter is incremented or decremented by up to m in each clock cycle, as opposed to conventional FSM-based functions which are restricted to one-step transitions. The algorithm for integer FSM-based functions is proposed as shown in Algorithm 2.</p><p>The output of the proposed integer FSM-based functions in integral SC domain and its encoding format are similar to the conventional FSM-based functions. For instance, the output of the integer tanh function is in bipolar format while the output of integer exponentiation function is in unipolar format. Moreover, the integer FSM-based functions require m times more states compared to its conventional counterpart. Therefore, the approximate transfer function of integer tanh and exponentiation functions, which are referred to as NStanh and NSexp, respectively, are:</p><formula xml:id="formula_31">tanh ns 2 ≈ E[NStanh (m × n, S)],<label>(20) exp</label></formula><formula xml:id="formula_32">(-2Gs) ≈ E[NSexp (m × n, m × G, S)] : s &gt; 0. (<label>21</label></formula><formula xml:id="formula_33">)</formula><p>In order to show the validity of the proposed algorithm, Monte-Carlo simulation is used. Fig. <ref type="figure">8</ref> illustrates two examples of the proposed NStanh function compared to its corresponding Stanh and tanh functions for different values of m. Simulation results show that NStanh is more accurate than Stanh for m &gt; 1 and that the accuracy improves as the value of m increases. Moreover, NStanh is able to approximate tanh for input values outside of the [-1, 1] range with negligible performance loss, while Stanh does not work. The proposed NStanh function can also approximate tanh functions with fractional scaling factor, e.g. tanh (3/2x) ≈ NStanh (3 × m, S), as long as the value m is even, to make sure that the number of states is even. The aforementioned statements also hold true for NSexp, unlike with Sexp, as shown in Fig. <ref type="figure">9</ref>. The proposed FSM-based functions in integral</p><p>TABLE I HARDWARE COMPLEXITY OF THE PROPOSED FSM-BASED FUNCTIONS @ 400 MHZ IN A 65 NM CMOS TECHNOLOGY m (Stream Length) 1 (1024) 2 (512) 4 (256) 8 (128) Area (µm 2 ) Power (µW) Area (µm 2 ) Power (µW) Area (µm 2 ) Power (µW) Area (µm 2 ) Power (µW) tanh(s) 24 3.5 74 9.8 117 18.3 150 24.9 tanh(2s) 63 9.4 107 17.2 141 22.6 182 31.2 exp(-s) --424 62.1 474 72.5 480 80 exp(-2s) 424 57.1 440 65.1 491 74.5 532 93.1 SC also result in better approximation as the value of n increases, similar to conventional stochastic FSM-based functions. The hardware complexity of the proposed FSM-based functions in a 65 nm CMOS technology is also summarized in Table I. The implementation results show that the proposed FSM-based functions consume roughly 7 times more power at most while having 8 times less latency, which results in a lower energy consumption, compared to the conventional FSM-based functions (i.e., FSM-based functions with m = 1).</p><p>Note that the stream length of FSM-based functions denotes the latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. INTEGER STOCHASTIC IMPLEMENTATION OF DBN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Review on the DBN Algorithm</head><p>DBNs are the hierarchical graphical models obtained by stacking RBMs on top of each other and training them in a greedy unsupervised manner <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. DBNs take lowlevel inputs and construct higher-level abstractions through the composition of layers. Both the number of layers and the number of inputs in each layer can be adjusted. Increasing the number of layers and their size tends to improve the performance of the network.</p><p>In this paper, we exploit a DBN constructed using two layers of RBM, which are also called hidden layers, followed by a classification layer at the end for handwritten digit recognition.. As a benchmark, we use the Mixed National Institute of Standards and Technology (MNIST) data set <ref type="bibr" target="#b26">[27]</ref>. This data set provides thousands of 28×28 pixel images for both training and testing procedures. Each pixel is represented by an integer number between 0 to 255, requiring 8 bits for digital representation. As mentioned in Section I, the training procedure can be performed on remote servers in the cloud. Therefore, the extracted weights are stored in a memory for the hardware inference engine to classify the input images in real-time.</p><p>Fig. <ref type="figure" target="#fig_7">10</ref> shows the DBN used for handwritten digits classification in this paper. Inputs of DBN and outputs of a hidden layer are hereafter referred to as visible nodes and hidden nodes, respectively. Each hidden node is also called neuron. The hierarchical computations of each neuron are performed as follows: where M denotes the number of visible nodes, v j the value of visible nodes, W ij the extracted weights, b j the bias term, z j intermediate value, h j the output value of each hidden node and j an index to each hidden node. The nonlinearity function used in DBN , i.e., equation <ref type="bibr" target="#b22">(23)</ref>, is called a sigmoid function. The classification layer does not require a sigmoid function as it is only used for quantization. In other words, the maximum value of the output denotes the recognized label.</p><formula xml:id="formula_34">z j = M i=1 W ij v i + b j ,<label>(22)</label></formula><formula xml:id="formula_35">h j = 1 1 + exp(-z j ) = σ(z j ),<label>(23)</label></formula><formula xml:id="formula_36">+ + σ σ σ σ v 1 v 2 v M σ σ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Proposed Stochastic Architecture of a DBN</head><p>VLSI implementations of a DBN network in binary form are computationally expensive since they require many matrix multiplications. Moreover, there is no straightforward way to implement the sigmoid function in hardware. Therefore, this unit is normally implemented by LUTs, which requires additional memory in addition to the memory used for storing weights. Considering 10 bits for weights, 78400 10b×8bmultipliers are required to do the matrix multiplications of the first hidden layer for a parallel implementation of a network with configuration of 784-100-200-10, meaning 784 visible nodes, 100 first-layer hidden nodes, 200 second-layer hidden nodes and 10 output nodes. Note that the parallel implementation of such a networks results in huge silicon area in part due to its routing congestion caused by the layer interconnection.</p><p>Stochastic implementation of DBN is a promising approach to perform the mentioned complex arithmetic operations using simple and low-cost elements. In order to find the output value of the first hidden node, 784 multiplications are required, which can be easily performed by using AND gates in unipolar format. Then, addition of multipliers output should be performed by using a scaled adder or an OR gate. Using a scaled adder to sum 784 numbers requires an extremely long bit-stream due to the fact that the output result of this adder is scaled down by 784 times, a very small number to be represented by short stream length. In <ref type="bibr" target="#b27">[28]</ref>, an OR gate is used as an adder to perform this computation while the inputs first are scaled down to make the term "A • B" close to 0 in <ref type="bibr" target="#b9">(10)</ref>, which potentially increases the required stream length for computations. An APC is also proposed in <ref type="bibr" target="#b21">[22]</ref> to realize the matrix operations. Despite its good performance on additions, it is not a suitable approach for a stochastic DBN, since it converts the results to a binary form <ref type="bibr" target="#b21">[22]</ref>.</p><p>We have shown in Section III-A that the integer stochastic stream can be generated by adding conventional stochastic streams. Considering that the multiplications of the first layer of a DBN are performed in conventional stochastic domain, the nature of the algorithm is to add the multiplication results together. Exploiting a binary tree adder, the addition result remains in integer-stochastic form without any precision loss. The sigmoid function can also be implemented in the integer stochastic domain.</p><p>It is well-known that the sigmoid function can be computed using the tanh function as follows:</p><formula xml:id="formula_37">σ(x) = 1 + tanh x 2 2 . (<label>24</label></formula><formula xml:id="formula_38">)</formula><p>The tanh function can also be implemented by NStanh function (see <ref type="bibr" target="#b19">(20)</ref>) in integer stochastic domain. The output of NStanh is in bipolar format in conventional stochastic domain. Therefore, considering its output in unipolar format according to ( <ref type="formula" target="#formula_37">24</ref>) and ( <ref type="formula" target="#formula_1">2</ref>), the output of NStanh is equivalent to the sigmoid function in stochastic domain. Fig. <ref type="figure" target="#fig_11">11</ref> shows the proposed integer stochastic architecture of a single neuron. The input signal stream is generated by using conventional stochastic domain: however, the weights TABLE II THE MISCLASSIFICATION ERROR OF THE PROPOSED ARCHITECTURES FOR DIFFERENT NETWORK SIZES AND STREAM LENGTHS Misclassification Error (%) [29] Proposed Code Type Floating Point Integeral SC m -1 2 4 Stream Length -1024 512 256 784-100-200-10 2.29 2.40 2.46 2.33 784-300-600-10 1.82 2.01 1.89 1.90 are represented by 2's complement format in integer stochastic domain with range of m, which requires log 2 (m) + 1 bits for representation. The multiplications are performed bit-wise by AND gates since pixels and weights are represented by binary stochastic streams and integral stochastic streams, respectively.</p><p>A tree adder and an NStanh unit are used to perform the additions and nonlinearity function, respectively. The output of the integer stochastic sigmoid function is represented by a single wire in unipolar format. Therefore, the input and output formats are the same. Integer stochastic architecture of DBN is formed by stacking the proposed single neuron architecture. The input images require a minimum bit-stream length of 256, but since the weights lie in the [-4, 4] interval they require a minimum bit-stream length of 1024 in conventional stochastic domain. Therefore, the latency of the proposed integer-stochastic implementation of the DBN is equal to 1024 for m = 1.</p><p>The input range of the NStanh function, i.e. the value of m in Fig. <ref type="figure" target="#fig_11">11</ref>, is selected through simulation. The histogram of the adder outputs identifies this range by taking a window which covers 95% of data. For instance, Fig. <ref type="figure" target="#fig_12">12</ref> shows the histogram of integer values as inputs of NStanh function at the first layer of a 784-100-200-10 DBN. This diagram is generated based on the non-correlated stochastic inputs and the selected range for this network is 6, i.e., the value of m in Fig. <ref type="figure" target="#fig_11">11</ref>. This range strongly depends on the correlations among the stochastic inputs. The range would be a bigger number as the correlation increases. For instance, summation of two correlated stochastic streams, {1, 1, 0, 0, 1, 0} and {1, 1, 0, 1, 0, 0}, representing real value of 0.5 results in integral stochastic stream of {2, 2, 0, 1, 1, 0} and input range</p><p>TABLE III IMPLEMENTATION RESULTS OF THE PROPOSED ARCHITECTURE ON FPGA VIRTEX-7 Network Size Stream Length Misclassification Error Area (# of LUTs) Latency (µs) Throughput (Mbps) 784-100-200-10 256 2.33% 1,013,002 1.705 3822 Proposed 784-100-200-10 512 2.46% 682,352 3.412 1874 784-100-200-10 1024 2.40% 437,461 6.503 974 784-100-200-10 1024 5.72% 144,450 8.561 NA [28] 784-300-600-10 1024 2.92% 603,750 9.797 NA 784-500-1000-10 1024 2.32% 1,292,310 10.77 NA of 2 while summation of two uncorrelated stochastic streams, {0, 0, 1, 0, 1, 1} and {1, 1, 0, 1, 0, 0}, representing real value of 0.5 results in integral stochastic stream of {1, 1, 1, 1, 1, 1} and input range of 1. Correlation among the inputs is introduced when the same LFSR units are shared among several inputs, in order to reduce hardware area. In this paper, the set of LFSR units that are used for one neuron are shared for all the other neurons. More precisely, 785 11-bit LFSRs with different seeds are used in total to generated all inputs and weights of the proposed DBN architectures and guarantee non-correlated stochastic streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. IMPLEMENTATION AND SIMULATION RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Misclassification Error Rate Comparison</head><p>The misclassification error rate of DBNs plays a crucial role in the performance of the system. In this part, the misclassification errors of the proposed integer stochastic architectures of DBNs with different configurations are summarized in Table <ref type="table">II</ref>. Simulation results have been obtained by using MATLAB on 10000 MNIST handwritten test digits <ref type="bibr" target="#b26">[27]</ref> for both floating point code and the proposed architecture using LFSRs as the stream generators. The method proposed in <ref type="bibr" target="#b28">[29]</ref> is used as our training core to extract the network weights. In fixed-point format, a precision of 10 bits is used to represent the weights. A stochastic stream of equivalent precision requires a length of 1024. The length of the stream can be reduced by increasing m. For example, using m = 2 the length can be reduced to 512, and using m = 4 it can be reduced to 256. Because the input pixels only require 8 bits of precision, they can be represented using a binary (m = 1) stochastic stream of length 256. Therefore, by using m = 1 for the pixels and m = 4 for the weights, it is possible to reduce the stream length to 256 while still using AND gates to implement multiplications. The simulation results show the negligible performance loss of the proposed integer stochastic DBN for different sizes compared to their floating point versions. The reported misclassification errors for the proposed integral stochastic architecture were obtained using LFSR units as random number generators in MATLAB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FPGA Implementation</head><p>As mentioned previously, a fully-or semi-parallel VLSI implementation of DBN in binary form requires a lot of hardware resources. Therefore, many works target FPGAs <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b34">[35]</ref>, but none manage to fit a fully-parallel deep neural</p><p>TABLE IV ASIC IMPLEMENTATION RESULTS FOR A 784-100-200-10 NETWORK @ 400 MHZ AND 1V IN A 65 NM CMOS TECHNOLOGY Implementation Type Integral SC Binary Radix Stream Length 256 512 1024 -Misclassification error [%] 2.33 2.46 2.40 2.3 Energy [µJ] 2.96 3.3 3.35 0.380 Gate Count [M Gates (NAND2)] 4.2 2.2 1.1 23.6 Latency [ns] 650 1290 2570 30</p><p>network architecture in a single FPGA board. Recently, a fully pipelined FPGA architecture of a factored RBM (fRBM) was proposed in <ref type="bibr" target="#b8">[9]</ref>, which could implement a single layer neural network consisting of 4096 nodes using virtualization technique, i.e., time multiplex sharing technique, on a Virtex-6 FPGA board. However, the largest fRBM neural network achievable without virtualization is on the order of 256 nodes. In <ref type="bibr" target="#b27">[28]</ref>, a stochastic implementation of DBN on a FPGA board is presented for different network sizes, however, this architecture cannot achieve the same misclassification error rate as a software implementation. Table <ref type="table">III</ref> shows both the hardware implementation and performance results of the proposed integer stochastic architecture of DBN for different network sizes on a Virtex7 xc7v2000t Xilinx FPGA. The implementation results show that the misclassification error of the proposed architectures for network size of 784-100-200-10 is the same as for the largest network presented in <ref type="bibr" target="#b27">[28]</ref>, i.e., the network size of 784-500-1000-10, while the area of the proposed designs are reduced by 66%, 47% and 21% for m = 1, m = 2 and m = 4. Moreover, the latency of the proposed architectures are also reduced by 40%, 63% and 84% for m = 1, m = 2 and m = 4. Therefore, as the value of m increases, the latency of the integer stochastic hardware is reduced and becomes suitable for throughput-intensive applications. Note that the reported areas in Table <ref type="table">III</ref> include the costs of B2S and B2IS units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. ASIC Implementation</head><p>Table <ref type="table">IV</ref> shows the ASIC implementation results for a fixedpoint implementation of the network size of 784-100-200-10. Despite the improvements that the proposed architectures provide over previously proposed stochastic implementations, the stochastic implementations still uses more energy than the fixed-point implementation in 65 nm CMOS, even if the</p><p>TABLE V ASIC IMPLEMENTATION RESULTS FOR A 784-300-600-10 NETWORK BASED ON INTEGRAL SC @ 400 MHZ AND 1V IN A 65 NM CMOS TECHNOLOGY Implementation Type Integral SC Binary Radix Network Configuration 784-300-600-10 784-100-200-10 Value of m 1 2 4 -Stream Length 64 128 256 512 32 64 128 256 16 32 64 128 -Misclassification error [%] 2.49 2.24 2.22 2.07 2.42 2.30 2.24 1.96 2.27 2.22 2.07 1.95 2.3 Energy [µJ] 0.740 1.436 2.802 5.640 0.505 1.029 1.997 3.933 0.299 0.640 1.28 2.53 0.380 Gate Count [M Gates (NAND2)] 5.4 5.6 5.6 5.6 9.2 9.7 10.2 10.2 15.6 16.9 17.8 18.9 23.6 Latency [ns] 170 330 650 1290 90 170 330 650 50 90 170 330 30</p><p>TABLE VI DEVIATIONS OF LAYER-1 AND LAYER-2 NEURONS FOR A 784-300-600-10 NETWORK Deviation (%) Layer-1 Neuron Layer-2 Neuron 0.7V 17.90 15.41 0.75V 8.57 3.95 0.8V 0.011 ≈ 0</p><p>power consumption and area of a stochastic neuron are smaller.</p><p>A similar result was also obtained in <ref type="bibr" target="#b35">[36]</ref> for stochastic implementations of image processing circuits.</p><p>In order to improve the energy consumption of the proposed stochastic architectures, we select a bigger network size with better misclassification rate and reduce the stream length to achieve roughly the same misclassification error rate as the binary radix implementation in Table <ref type="table">IV</ref>. The implementation results of a 784-300-600-10 neural network based on integral SC for different stream lengths and values of m are summarized in Table <ref type="table">V</ref>. The implementation results show that the integral stochastic architecture for value of m = 4 and stream length of 16 at misclassification error rate of 2.3% consumes 21% less energy as well as 34% less area compared to the binary radix implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quasi-Synchronous Implementations</head><p>In order to further reduce the energy consumption of the system, we also consider a quasi-synchronous implementation, in which the supply voltage of the circuit is reduced beyond the critical voltage by permitting some timing violations to occur. Timing violations introduce deviations in the computations, but because the stochastic architecture is fault-tolerant, we can obtain the same classification performance by slightly increasing the length of the streams. This yields further energy savings without any compromise on performance.</p><p>We characterize the effect of timing violations on the algorithm by studying small test circuits that can be simulated quickly, using the same approach as in <ref type="bibr" target="#b36">[37]</ref>. In the proposed architecture, the same processing circuit can be replicated several times to form each layer, depending on the required degree of parallelism. Therefore, we characterize the effect of timing violations on these small processing circuits: each neuron processor (one for each layer) is synthesized in a 65 nm CMOS technology and deviations are measured at different voltages, from 0.7V to 1.0V in 0.05V increments,</p><p>TABLE VII ASIC IMPLEMENTATION RESULTS FOR A 784-300-600-10 NETWORK @ 400 MHZ IN A 65 NM CMOS TECHNOLOGY UNDER FAULTY CONDITIONS Implementation Type Integral SC Supply Voltage (Layer-1-layer-2-layer-3) 0.8-0.7-0.8 0.75-0.75-0.8 0.8-0.8-0.8 Stream Length 30 30 22 Misclassification error [%] 2.29 2.28 2.30 Energy [µJ] (improvement w.r.t. 1V) 0.283 (-5%) 0.286 (-4%) 0.256 (-14%) Gate Count [M Gates (NAND2)] 15.6 15.6 15.6 Latency [ns] 85 85 65</p><p>as shown in Table <ref type="table">VI</ref>. Note that no deviations are observed when the supply voltage is larger than 0.8V. The output of first and second layers is binary, while the output of classification layer has 6 bits. Binary to stochastic converter units are also considered for each neuron and the weights are hard coded for the implementations. The deviation error of the layer-3 neuron for 0.7V and 0.75V results in a huge misclassification error. It is not beneficial to allow large deviations to occur in that layer since there are only 10 neurons in the third layer, and therefore we do not expect the supply voltage of layer-3 processing circuits to have a big impact on the overall energy consumption. Therefore, the layer-3 neurons supplied with 0.8V are used. Note that no deviations are observed when the supply voltage is 0.8V in the layer-3 neurons.</p><p>The performance results for a 784-300-600-10 network and m = 4 at different supply voltages are provided in Table <ref type="table">VII</ref>. The misclassification performance obtained by the quasi-synchronous system is very similar to the performance of the reliable system, despite the fact that the deviation rate is up to 9% in layer-1 neurons and 16% in layer-2 neurons. This results in up to a 14% lower energy consumption without any compromise on performance. On the other hand, introducing bit-wise deviations at a rate of 1% in the fixed-point system results in a 87% misclassification rate. Note that the reported implementation results in this paper include costs of B2N and B2IS units.</p><p>Moreover, because a stochastic implementation is much more fault-tolerant than a fixed-point implementation, it can be preferable for future process technologies, and in particular for inherently unreliable ones such as nanoscale memristor devices. Note that memristor devices consume substantially less energy compared to CMOS and can be scaled to sizes below 10 nm <ref type="bibr" target="#b22">[23]</ref>. In <ref type="bibr" target="#b22">[23]</ref>, stochastic implementations were suggested as a promising approach for use in such devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Integral SC makes the hardware implementation of precision-intensive applications feasible in the stochastic domain, and allows computations to be performed with streams of different lengths, which can improve the latency of the system. An efficient stochastic implementation of a deep belief network is proposed using integral SC. The simulation and implementation results show that the proposed design reduces the area occupation by 66% and the latency by 84% with respect to the state of the art. We also showed that the proposed design consumes 21% less energy than its binary radix counterpart. Moreover, the proposed architectures can save up to 33% energy consumption w.r.t. the binary radix implementation by using quasi-synchronous implementation without any compromise on performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A N-layer DBN where W and N denote the weights of each layer and number of layers respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Stochastic multiplications using (a) AND gate in unipolar format and (b) XNOR gate in bipolar format</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Stochastic additions using (a) MUX and (b) OR gate</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. State transition diagram of the FSM implementing (a) tanh and (b) exponentiation functions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Stochastic representations of 0.75 and (b) Integer stochastic representation of 1.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Increasing the range value m of the integer stochastic stream reduces computations latency. (b) Parallelized stochastic computation by factor of two.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Integer stochastic multiplier with m = 2 (b) Multiplication of integer stochastic stream with binary stochastic bit-stream using AND gate or MUX</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 :</head><label>1</label><figDesc>Pseudo code of the conventional algorithm for FSM-based functions E[S i ] = 9/8, which corresponds to s = 9/16 because of the implicit scaling factor of 1/2 (see Fig.6(a)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc>since the expected value operator is linear. Equation 19 remains valid also in the bipolar case, while the range of y is [0, m + m ] and [-(m + m ), m + m ] for Data: Integer value S i ∈ {-m, ..., m} where i ∈ {1, 2, ..., N } Result:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. (a) Integer stochastic implementation of tanh(s) and (b) Integer stochastic implementation of tanh(2s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>2 Fig. 10 .</head><label>210</label><figDesc>Fig. 10. The high-level architecture of 2-layer DBN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The proposed integer stochastic neuron. The B2IS and B2S denote binary to integer stochastic and binary to stochastic converters, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Histogram of integer values as inputs of NStanh function at the first layer of a 784-100-200-10 DBN.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>The authors would like to thank <rs type="person">C. Condo</rs> for his helpful suggestions.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VLSI Implementation of Deep Neural Network Using Integral Stochastic Computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ardakani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Leduc-Primeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Onizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hanyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Symp. on Turbo Codes &amp; Iterative Information Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">4.6 A1.93TOPS/W scalable deep learning/inference processor with tetraparallel MIMD architecture for big-data applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Solid-State Circuits Conference (ISSCC)</title>
		<imprint>
			<date type="published" when="2015-02">Feb 2015</date>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012-01">Jan 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Fast Learning Algorithm for Deep Belief Nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Handbook of Brain Theory and Neural Networks, 2nd ed</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Arbib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Switchable Deep Network for Pedestrian Detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="page" from="899" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-stage Contextual Deep Learning for Pedestrian Detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12">Dec 2013</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Fully Pipelined FPGA Architecture of a Factored Restricted Boltzmann Machine Artificial Neural Network</title>
		<author>
			<persName><forename type="first">L.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Asaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Reconfigurable Technol. Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stochastic circuits for real-time imageprocessing applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">50th ACM/EDAC/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully Parallel Stochastic LDPC Decoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tehrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5692" to="5703" />
			<date type="published" when="2008-11">Nov 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A hardware implementation of a radial basis function neural network using stochastic logic</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design, Automation Test in Europe Conference Exhibition (DATE)</title>
		<imprint>
			<date type="published" when="2015-03">March 2015</date>
			<biblScope unit="page" from="880" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Architectures for Recursive Digital Filters Using Stochastic Computing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Parhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="3705" to="3718" />
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Successive cancellation decoding of polar codes using stochastic computing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Parhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Symp. on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="3040" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Architecture for Fault-Tolerant Computation with Stochastic Logic</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bazargan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="105" />
			<date type="published" when="2011-01">Jan 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using stochastic computing to implement digital image processing algorithms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 29th International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2011-10">Oct 2011</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computation on Stochastic Bit Streams Digital Image Processing Case Studies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bazargan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</title>
		<imprint>
			<date type="published" when="2014-03">March 2014</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="449" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic Circuits for Real-time Image-processing Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Design Automation Conference, ser. DAC &apos;13</title>
		<meeting>the 50th Annual Design Automation Conference, ser. DAC &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hardware implementation of stochastic-based Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Rosselló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Canals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2010 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2010-07">July 2010</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic arithmetic implementations of neural networks with in situ learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Neural Networks</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="711" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Advances in Information Systems Science</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gaines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Systems Science, ser</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Tou</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer US</publisher>
			<date type="published" when="1969">1969</date>
			<biblScope unit="page" from="37" to="172" />
		</imprint>
	</monogr>
	<note>Stochastic Computing Systems</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic Logic Realization of Matrix Operations</title>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Euromicro Conf. on Digital System Design (DSD)</title>
		<imprint>
			<date type="published" when="2014-08">Aug 2014</date>
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A native stochastic computing architecture enabled by memristors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Knag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014-03">March 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A stochastic reconfigurable architecture for fault-tolerant computation with sequential logic</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 30th International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2012-09">Sept 2012</date>
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic neural computation. I. Computational elements</title>
		<author>
			<persName><forename type="first">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Card</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Computers</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="891" to="905" />
			<date type="published" when="2001-09">Sep 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An ultra-fast parallel architecture using sequential circuits computing on random bits</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS2013)</title>
		<imprint>
			<date type="published" when="2013-05">May 2013</date>
			<biblScope unit="page" from="2215" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An FPGA implementation of a Restricted Boltzmann Machine classifier using stochastic bit streams</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 26th Int. Conf. on Application-specific Systems, Architectures and Processors (ASAP)</title>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="68" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A Novel Inference of a Restricted Boltzmann Machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd Int. Conf. on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2014-08">Aug 2014</date>
			<biblScope unit="page" from="1526" to="1531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GANGLION-a fast hardware implementation of a connectionist classifier</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Blanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Custom Integrated Circuits Conf</title>
		<meeting>of the IEEE Custom Integrated Circuits Conf</meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stochastic connection neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Int. Conf. on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1995-06">Jun 1995</date>
			<biblScope unit="page" from="35" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An exact hardware implementation of the Boltzmann machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Skubiszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Fourth IEEE Symposium on Parallel and Distributed Processing</title>
		<meeting>of the Fourth IEEE Symposium on Parallel and Distributed essing</meeting>
		<imprint>
			<date type="published" when="1992-12">Dec 1992</date>
			<biblScope unit="page" from="107" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A highly scalable Restricted Boltzmann Machine FPGA implementation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcafee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Field Programmable Logic and Applications</title>
		<imprint>
			<date type="published" when="2009-08">Aug 2009</date>
			<biblScope unit="page" from="367" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A multi-FPGA architecture for stochastic Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Field Programmable Logic and Applications</title>
		<imprint>
			<date type="published" when="2009-08">Aug 2009</date>
			<biblScope unit="page" from="168" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-Performance Reconfigurable Hardware Architecture for Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1780" to="1792" />
			<date type="published" when="2010-11">Nov 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Computation on stochastic bit streams digital image processing case studies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lilja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bazargan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Very Large Scale Integration (VLSI) Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="449" to="462" />
			<date type="published" when="2014-03">March 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling and Energy Optimization of LDPC Decoder Circuits with Timing Violations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Leduc-Primeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Kschischang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
		<idno>abs/1503.03880</idno>
		<ptr target="http://arxiv.org/abs/1503.03880" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
