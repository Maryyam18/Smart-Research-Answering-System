<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Energy Consumption of VLSI Decoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2014-12-12">12 Dec 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Christopher</forename><surname>Blake</surname></persName>
							<email>christopher.blake@mail.utoronto.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="laboratory">Workshop on Information Theory</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<postCode>2013 18-21 2013</postCode>
									<settlement>Canadian June Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frank</forename><forename type="middle">R</forename><surname>Kschischang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="laboratory">Workshop on Information Theory</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<postCode>2013 18-21 2013</postCode>
									<settlement>Canadian June Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Energy Consumption of VLSI Decoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-12-12">12 Dec 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">8BE0A2330AAE0D2E68208AF75126F512</idno>
					<idno type="arXiv">arXiv:1412.4130v1[cs.IT]</idno>
					<note type="submission">Submitted for publication on November 7th, 2013, revised November 28th, 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-11-28T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Thompson's model of VLSI computation relates the energy of a computation to the product of the circuit area and the number of clock cycles needed to carry out the computation. It is shown that for any family of circuits implemented according to this model, using any algorithm that performs decoding of a codeword passed through a binary erasure channel, as the block length approaches infinity either (a) the probability of block error is asymptotically lower bounded by 1 2 or (b) the energy of the computation scales at least as Ω n √ log n , and so the energy of successful decoding, per decoded bit, must scale at least as Ω √ log n . This implies that the average energy per decoded bit must approach infinity for any sequence of codes that approaches capacity. The analysis techniques used are then extended to the case of serial computation, showing that if a circuit is restricted to serial computation, then as block length approaches infinity, either the block error probability is lower bounded by 1 2 or the energy scales at least as fast as Ω (n log n). In a very general case that allows for the number of output pins to vary with block length, it is shown that the average energy per decoded bit must scale as Ω n (log n) 1 5</p><p>. A simple example is provided of a class of circuits performing low-density parity-check decoding whose energy complexity scales as O n 2 log log n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S INCE the work of Shannon [1], information theory has sought to determine how much information can be communicated over a noisy channel; modern coding theory has sought ways to achieve this capacity using error control codes. A standard channel model is the additive white Gaussian noise (AWGN) channel, for which the maximum rate of information that can be reliably communicated (known as the capacity) is known and depends on the transmission power. This model does not, however, consider the energy it takes to encode and decode; a full understanding of energy use in a communication system requires taking into account these encoding and decoding energies, along with the transmission energy. Currently there has been very little work done in seeking a fundamental understanding of the energy required to execute an error control coding algorithm.</p><p>Early work in relating the area of circuits and number of clock cycles in circuits that perform decoding algorithms was presented by El Gamal et al. in <ref type="bibr" target="#b1">[2]</ref>. More recent work in trying to find fundamental limits on the energy of decoding can be attributed to Grover et al. in [3]. In this work, the authors consider decoding schemes that are implemented using a VLSI model attributed to Thompson [4]  (which we will describe later), and they are able to show that for any code, using any decoding algorithm, as the required block error probability approaches 0, the sum of the transmission, encoding, and decoding energy, per bit, must approach infinity at a rate of Ω 3 1 log P blk e , where P blk e is the block error probability of the code. This result is useful to the extent that it suggests how to judge the energy complexity of low error probability codes; however, it does not suggest how the energy complexity of decoding scales as capacity is approached.</p><p>The result of this paper uses a similar approach to Grover et al., but we generalize the computation model to both parallel and serial computation, and show how the energy of low block error rate decoders</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. Diagram of a possible VLSI circuit. The circuit is laid out on a grid of squares and the squares of the grid that are filled in fully represent computational nodes and the thinner lines represent wires. Also present is a wire-crossing in one of the grid squares. The area of this circuit is proportional to the number of grid-squares that contain either a computational node, a wire, or a wire crossing. must scale with block length n. We believe that this approach can guide the development of codes and decoding circuits that are optimal from an energy standpoint.</p><p>In this paper, in Section II we will describe the VLSI model that will be used to derive our bounds on decoding energy. Our results apply to a decoder for a standard binary erasure channel, which will be formally defined in Section III. In Section IV we will describe some terminology and some key lemmas used in our paper. The main contribution of this paper will be given in Section V where we describe a scaling rule for codes with long block length that have asymptotic error probability less than 1  2 . The approach used in this section is extended in Section VI to find a scaling rule for serial computation. Then, in Section VII we extend the approaches of the previous sections to derive a non-trivial super-linear lower bound on circuit energy complexity for a series of decoders in which the number of output pins can vary with increasing block length. These results are applied to find a scaling rule for the energy of capacity approaching decoders as a function of fraction of capacity in Section VIII. We then give a simple example in Section IX showing how an LDPC decoder can be implemented with at most O (n 2 log log n) energy, providing an upper bound to complement our fundamental lower bound.</p><p>Notation: To aid our discussion of scaling laws, we use standard Big-Oh and Big-Omega asymptotic notation, which is well discussed in <ref type="bibr" target="#b4">[5]</ref>. We say that a function f (x) = O (g (x)) if and only if there is some M and some x 0 such that for all x ≥ x 0 , f (x) ≤ M g (x). Similarly, we say that f (x) = Ω (g (x)) if and only if g (x) = O (f (x)). Intuitively, this means that the function f (x) grows at least as fast (in an order sense) as g (x) and hence we use it with lower bounds. In the following, a sequence of values b 1 , b 2 , . . . , b k is denoted b k 1 . Random variables are denoted with upper case letters; values in their sample spaces are denoted with lower-case letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. VLSI MODEL A. Description of Model</head><p>The VLSI model that we will use is based on the work of Thompson <ref type="bibr" target="#b5">[6]</ref>, and was used by El Gamal in <ref type="bibr" target="#b1">[2]</ref> and Grover et al. in <ref type="bibr" target="#b2">[3]</ref>. The model consists of a basic set of axioms that describe what is meant by a VLSI circuit and a computation. The model then relates two parameters of the circuit and computation, namely the number of clock cycles used for the computation and the area of the circuit, to the energy used in the computation. Thompson used this model to compute fundamental bounds on the energy required to compute the discrete Fourier transform, as well as other standard computational problems, including sorting. The results in this paper apply to any circuit implemented in a way that is consistent with these axioms, listed as follows:</p><p>• A circuit consists of two types of components: wires and nodes. In such a circuit, wires carry binary messages and nodes can perform simple computations (e.g., AND, XOR), all laid out on a grid of squares. Wires carry information from one node to another node. Wires are assumed to be bi-directional (at least for the purpose of lower bounds). In each clock cycle, each node sends one bit to each of the nodes it is connected to over the wire. We in general allow a node to perform a random function on its inputs. In a deterministic function, the output of the function is determined fully by its inputs. By a random function we mean that the outputs of a particular node, conditioned on the inputs being some element from the set of possible inputs, is a distribution p Y |X (•|•) where Y is a random variable representing the possible outputs of the node and X a random variable representing the inputs of a node. In the particular case of a node with 4 input wires (and thus 4 output wires because of our bidirectional assumption on the wires) the random variables X and Y can take on values from {0, 1} 4 . • A VLSI circuit is a set of computational nodes connected to each other using finite-width bi-directional wires. At each clock cycle, nodes communicate with the other nodes to which they are connected.</p><p>Computation terminates at a pre-determined number, τ , of clock cycles. • Wires run along edges and cross at grid points. There is only one wire per edge. Each grid point contains a logic element, a wire crossing, a wire connection, or an input/output pin. Grid points can be empty. A computational node is a grid point that is either a logic element, a wire-connection, or an input/output pin. • The circuit is planar, and each node has at most 4 wires leading from it.</p><p>• Inputs of computation are stored in source nodes and outputs are stored in output nodes. The same node can be a source node and an output node. Each input can enter the circuit only at the corresponding source node.</p><p>• Each wire has width λ w . Any two wires are separated by at least the wire-width. Any grid points are separated by distance at least λ w . Each node is assumed to require at least λ 2 w wire area (length and width at least λ w ).</p><p>• Processing is done in "batches," in which a set of inputs is processed and outputs released before the next set of inputs arrive. • Energy consumed in a computation is proportional to A c τ where A c is the area occupied by the wires and nodes of the circuit and τ is the number of clock cycles required to execute the computation. Precisely, the energy is assumed to be 1 2 C circuit V 2 DD τ where C circuit = C unit-area A c , where C unit-area is the capacitance per unit wired area of the circuit, and V DD is the voltage used to drive the circuit. The quantity 1  2 C unit-area V 2 DD is denoted by ξ tech , which is the "energy parameter" of the circuit. Processing energy for computation, E proc , is thus given by E proc = ξ tech A c τ . Since energy of a computation in our model and the area time complexity are essentially the same, in this paper we use the terms "energy complexity" and "Area-Time" complexity interchangeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion of Model</head><p>The circuit model described above allows us to consider a circuit as a graph in which the computational nodes correspond to the nodes of a graph and the wires correspond to edges.</p><p>The last assumption of our model, which relates the area and number of clock cycles to energy consumed in a circuit, assumes that a VLSI circuit is fully charged and then discharged to ground during each clock cycle. Since the wires in a circuit are made of conducting material and are laid out essentially flat, the circuit will have a capacitance proportional to the area of the wires. Assuming that all the wires will need to be charged at each clock cycle, there must be 1 2 C circuit V 2 DD energy supplied to the circuit. For now, we do not consider what will happen if at each clock cycle the state of some of the wires does not change. In the literature (see <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>) this model is often used to understand power consumption in a digital circuit so we do not attempt to alter these assumptions for the purposes of this paper. Sometimes leakage current of the circuit is factored into such models; we also neglect this as we assume the frequency of computation is high enough so that the power used in computation dominates.</p><p>There has been some work to understand the tradeoff between computational complexity and code performance. One such example is <ref type="bibr" target="#b8">[9]</ref>, in which the complexity of a Gallager Decoding Algorithm B was optimized subject to some coding parameters. This however does not correspond to the energy of such algorithms.</p><p>In <ref type="bibr" target="#b5">[6]</ref> it was proven that the Area-Time complexity of any circuit implemented according to this VLSI model that computes a Discrete Fourier Transform must scale as Ω (n 1.5 log n). However, there exist algorithms that compute in O (n log n) operations (for example, see <ref type="bibr" target="#b9">[10]</ref>); Thompson's results thus imply that, for at least some algorithms, energy consumption is not merely proportional to the computational complexity of an algorithm.</p><p>In the field of coding theory, Grover et al. in <ref type="bibr" target="#b2">[3]</ref> provided an example of two algorithms with the same computational complexity but different computational energies. The authors looked at the girth of the Tanner graph of an LDPC code. The girth is defined as the minimum length cycle in the Tanner graph that represents the code. They showed, using a concrete example, that for (3, 4)-regular LDPC codes of girth 6 and 8 decoded using the Gallager-A decoding algorithm, the decoders for girth 8 codes can consume up to 36% more energy than those for girth 6 codes. The girth of a code does not necessarily make the decoding algorithm require more computations (i.e., it does not increase computational complexity), but, for this example, it does increase the energy complexity. This is because codes with greater girth require the interconnection between nodes to be more complex, even though the same number of computational nodes and clock cycles may be required. This drives up the area required to make these interconnections, and thus drives up the energy requirements. Also in the field of coding theory, the work of Thorpe <ref type="bibr" target="#b10">[11]</ref> has shown that a measure of wiring complexity of an LDPC decoder can be traded off with decoding performance.</p><p>Thus, current research suggests that in decoding algorithms there appears to be a fundamental tradeoff between code performance and interconnect complexity. This paper attempts to find an analytical characterization of this trade-off.</p><p>Our paper considers a generic model of computation, but of course it does not completely reflect all methods of implementing a circuit. We discuss some circuit design techniques that our model does not directly consider below.</p><p>1) Multiple Layer VLSI Circuits: Modern VLSI circuits differ from the Thompson model in that the number of VLSI layers is not one (or two if one counts a wire crossing as another layer). Modern VLSI circuits allow multiple layers. Fortunately, it is known that if L layers are allowed, then this can decrease the total area by at most a factor of L 2 (see, for example, <ref type="bibr" target="#b3">[4]</ref> or <ref type="bibr" target="#b11">[12]</ref>). For the purposes of our lower bounds, if the number of layers remains constant as n increases, we can modify our energy lower bound results by dividing the lower bounds by L 2 . If, however, the number of layers can grow with n our results may no longer hold. Note also that this only holds for the purpose of lower bound. It may not be possible to implement a circuit with an area that decreases by a factor of L 2 , and so the upper bounds of Section IX cannot be similarly modified.</p><p>2) Adiabatic Computing: The model used in this paper assumes that after every clock cycle the circuit is entirely discharged and the energy used to charge the circuit is lost. There exists extensive research into circuit designs in which this is not the case (for an overview of this type of computing, called adiabatic computing, see <ref type="bibr" target="#b13">[13]</ref>). Our results do not apply to such circuit designs.</p><p>3) Using Memory Elements in Circuit Computation: The Thompson model does not allow for the use of special memory nodes in computation that can hold information and compute the special function of loading and unloading from memory. Such a circuit can be created using the Thompson model, but it may be that a strategic use of a lower energy memory element can decrease the total energy of a computation. Intuitively, however, the use of a memory element to communicate information within a circuit should still be proportional to the distance that information is communicated. Grover in <ref type="bibr" target="#b14">[14]</ref> proposed a "bit-meters" model of energy computation and derives scaling rules similar to our results, suggesting that, at least in an order sense, the circuit model we use is general enough to understand the scaling of high block length codes even if lower energy memory is used. Understanding precisely what kind of gain the use of a memory element can provide in energy complexity is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CHANNEL MODEL</head><p>We will consider a noisy channel model that is similar to the model used by Grover et al. in <ref type="bibr" target="#b2">[3]</ref>. An information sequence of k independent fair coin flips b k 1 ∈ {0, 1} k is encoded into 2 nR binary-alphabet codewords x n 1 ∈ {0, 1} n ; and thus this code has rate R = k/n bits/channel use. The sequence x n 1 is passed through a binary erasure channel with erasure probability , resulting in a received vector y n 1 ∈ {0, 1, ?} n , where the ? symbol corresponds to the erasure event.</p><p>The decoder estimates the input sequence bk 1 ∈ {0, 1} k by computing a function of the received vector y n</p><p>1 . The outputs of the noisy channel thus become inputs into the decoder circuit. In our most general model of computation, it is required that these channel outputs are eventually input into an input node of the circuit. In a parallel implementation model used for Theorem 1, each of these n decoder input symbols are input, at the beginning of the computation, into the n input nodes of the decoder. In a more general computational model used in Theorems 2 and 3, it is assumed that each of these symbols are input into the input nodes of the decoder during some clock cycle in the computation. Note that we allow each of the n symbols to be inserted into the circuit at any input node location during any clock cycle, but we also require, according to our model, that each input is injected only once into the circuit. Thus, our model does not subsume circuit implementations that, at no cost, allow the same input symbol to be inserted into the circuit in multiple places. The probability of block error is defined as</p><formula xml:id="formula_0">P blk e = Pr bk 1 = b k 1 .</formula><p>The lower bounds used in our result are valid for a binary erasure channel, but also for any channel that can result from a degraded version of a binary erasure channel. Hence, if we let = 2p ch then our results apply to lower bounds on decoders for binary symmetric channels with crossover probability p ch .</p><p>IV. DEFINITIONS AND MAIN LEMMAS The main theorems in this paper rely on the evaluation of a particular limit, which we present as a lemma below. Lemma 1. For any constant c, 0 &lt; c &lt; 1, and any constant c &gt; 0:</p><formula xml:id="formula_1">lim n→∞ (1 -exp (-c log n)) c n log n = 0.<label>(1)</label></formula><p>Proof: This result follows simply from taking the logarithm of the expression in (1) and using L'Hôpital's rule to show that the logarithm approaches -∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Relation Between Energy and Bits Communicated</head><p>Grover et al. in <ref type="bibr" target="#b2">[3]</ref> used a nested bisection technique, involving subdividing circuits, to derive the two key lemmas used in this paper. A circuit created according to the Thompson model can be considered a graph in which the computational nodes correspond to graph vertices and the wires are graph edges.</p><p>To understand these lemmas we must first understand what is meant by (a) a minimum bisection of a circuit and (b) a nested bisection of a circuit. Informally, a bisection of a graph is a set of edges whose removal results in at least two graphs of essentially the same size that are unconnected to each other. A bisection can also be defined in terms of separating a particular subset of vertices. A formal definition is given below. Definition 1. Let G = (V, E) be a graph. Let S ⊆ V be a subset of the vertices, and E s ⊆ E be a subset of the edges. Then E s bisects S in G if deletion of the edges in E s from the graph cuts A minimal bisection is a bisection of a graph whose size is minimal over all bisections. The minimum bisection width is the size of a minimal bisection. For a general graph, finding a minimum bisection is a difficult problem (in fact, it is NP-Complete <ref type="bibr" target="#b15">[15]</ref>), but all that is required for the results we will use is that a minimum bisection exists for every graph. Fig. <ref type="figure" target="#fig_0">2</ref> shows minimal bisections of a few simple graphs.</p><formula xml:id="formula_2">V into disconnected sets V 1 and V 2 , with V 1 ∪ V 2 = V , and S into sets S 1 ⊆ V 1 and S 2 ⊆ V 2 such that ||S 1 | -|S 2 || ≤ 1. A bisection of V in G is called a bisection of G.</formula><p>Note that the definition of a minimum bisection also applies to subsets of the vertices of a graph. A circuit has both input nodes and output nodes. The input (resp. output) nodes of the graph corresponding to a circuit are those nodes of the graph that correspond to the input (resp. output) nodes of the circuit. For the purposes of the results in this paper, we will consider bisections of the graph that bisect the output nodes.</p><p>We will also be using some other terms throughout the paper which we define below.</p><p>If one performs a minimum bisection on the output nodes of the graph corresponding to the interconnection graph of a circuit, this results in 2 disconnected subcircuits. The output nodes of these two (now disconnected) subcircuits can thus each be minimally-bisected again, resulting in 4 subcircuits. Definition 2. This process, when repeated r times, is said to perform an r-stage nested minimum bisection. Note that this divides the graph into 2 r disconnected components, which we will refer to as subcircuits.</p><p>When a circuit is viewed as a graph, a subcircuit can be viewed as a subgraph induced by a nested minimum bisection. When viewed as a circuit according to the Thompson model, it is a collection of computational nodes joined by wires laid out on a grid pattern. An example of a mesh-like circuit with 16 nodes undergoing two stages of nested minimum bisections is shown in Fig. <ref type="figure">3</ref>.</p><p>After a circuit undergoes r-stages of nested minimum bisections, label each of the 2 r subcircuits with a unique integer i in which 1 ≤ i ≤ 2 r . Consider a particular subcircuit i. During the r-stages of nested minimum bisections, a number of edges f i are removed that are incident on the graph corresponding to subcircuit i (we can think of this as the "fan-out" of this subcircuit). Definition 3. During the course of the computation, the number of bits communicated to subcircuit i is b i = τ f i , where τ is the number of clock cycles, and we refer to b i as the bits communicated to subcircuit i during a computation.</p><p>The quantity b i is associated with a particular subcircuit induced by a particular r-stage nested minimum bisection. When discussed, this quantity's association with a particular r-stage nested minimum bisection is implicit. Definition 4. The quantity B r = 2 r i=1 b i denotes the number of bits communicated across all edges deleted in an r-stage nested minimum bisection. Note that this is a quantity associated with a particular r-stage nested minimum bisection.</p><p>The quantity B r will be important in the proofs of the theorems in the paper. Specifically, it can be shown that if a decoding circuit (which we will define below) has a large B r for a particular r-stage nested minimum bisection, then the energy expended during a computation by this circuit must be high. As well, it can be shown that if this quantity is low, then the probability that the circuit makes an error is high.</p><p>The above definitions are general and can apply to the computation of any function. However, for our bounds we will be finding lower bounds on the energy complexity of decoding circuits. Definition 5. An (n, k) parallel decoding circuit is a circuit that has n input nodes (accepting symbols in {0, 1, ?}) and k output nodes (producing symbols in {0, 1}). The n input nodes are to receive the outputs of a noisy channel (which for the purposes of lower bound we assume to be a binary erasure channel with erasure probability ). At the end of the computation the decoder is to output the estimate of the original codeword. Note that this circuit decodes a rate R = k/n code.</p><p>Note that in the Thompson model it is assumed that all inputs are binary. For the purposes of lower bound, we allow for the inputs into the computation to be either 0, 1 or ?, where ? is the erasure symbol. At every clock cycle, we allow input nodes to perform a function on their input symbol, as well as the bits input into the node at the clock cycle. These nodes may then output any function of these inputs along the wires leading from the node. This definition will be generalized to serial computation models in the discussions preceding Theorem 2. Note also that our model of a decoding circuit allows for an input to be an erasure symbol, which is a slight relaxation of the Thompson circuit model. However, in our theorems, the key point will be that, if in a particular subcircuit all the n i input nodes of that subcircuit are erased, then, conditioned on this event, the distribution of the possible original inputs to the channel of the k i bits that a subcircuit i is to estimate remains uniform. This is a result of the symmetric nature of a binary erasure channel, and this will allow us to directly apply Lemma 2 to form lower bounds on probability of error.</p><p>After a decoding circuit undergoes r-stages of nested minimum bisections, each subcircuit will have roughly an equal number of output nodes, but the number of input nodes may vary (the actual number of input nodes in each subcircuit in general will be a function of the particular graph structure of the circuit, and the particular r-level nested minimum bisection performed). Definition 6. We refer to this quantity as the number of input nodes in subcircuit i and denote it n i .</p><p>Note that this quantity is determined by the particular r-stage nested minimum bisection, but for notational convenience we will consider the relation of this quantity to the particular structure of the r-stage nested minimum bisection to be implicit. Definition 7. A particular ith subcircuit formed by an r-stage nested minimum bisection will have a certain number of output nodes, which we denote k i . This quantity is referred to as the number of output nodes in subcircuit i.</p><p>In a fully parallel computation model (which we will employ in Theorem 1), at the end of the computation, these output nodes are to hold a vector ki ∈ {0, 1} k i , where the vector to be estimated is a vector k i ∈ {0, 1} k i which is produced by a series of fair coin flips as described in Section III. Since at the end of the computation these output nodes are to hold an estimate of a vector of length k i it is said that in this case k i is the number of bits responsible for decoding by subcircuit i. The probability of error for a subcircuit is precisely the probability that, after the end of the computation, Ki = K i . Lemma 2. Suppose that X, Y , and X are random variables that form a Markov chain X → Y → X. Suppose furthermore that X takes on values from a finite alphabet X with a uniform distribution (i.e., P (X = x) = 1 |X | for any x ∈ X ) and Y takes on values from an alphabet Y. Suppose furthermore that X takes on values from a set X such that X ⊆ X . Then,</p><formula xml:id="formula_3">P X = X ≤ |Y| |X | .</formula><p>Remark 1. This general lemma is meant to make rigorous a fundamental notion that will be used in this paper. As applied to our decoding problem, the random variable X can be thought of as the input to a binary erasure channel, and Y can be any inputs into a subcircuit of a computation, and X can be thought of as a subcircuit's estimate of X. This lemma makes rigorous the notion that if a subcircuit has fewer bits input into it than it is responsible for decoding, then the decoder must guess at least 1 bit, and makes an error with probability at least 1 2 . This scenario is actually a special case of this lemma in which |Y| = 2 m and |X | = 2 k for integers k and m, where m &lt; k.</p><p>Proof: Clearly, by the law of total probability,</p><formula xml:id="formula_4">P X = X = x∈X y∈Y P X,Y, X (x, y, x) = x∈X y∈Y P X (x) P Y |X (y|x) P X|Y (x|y)</formula><p>where we simply expand the term in the summation according to the definition of a Markov chain. Using P X (x) = 1 |X | we get:</p><formula xml:id="formula_5">P X = X = 1 |X | x∈X y∈Y P Y |X (y|x) P X|Y (x|y)</formula><p>and using P Y |X (y|x) &lt; 1 because it is a probability, and changing the order of summation gives us:</p><formula xml:id="formula_6">P X = X ≤ 1 |X | y∈Y x∈X P X|Y (x|y) .</formula><p>Since x∈X P X|Y (x|y) ≤ 1 (as we are summing over a subset of values that X can take on), we get:</p><formula xml:id="formula_7">P X = X ≤ 1 |X | y∈Y 1 = |Y| |X | .</formula><p>In the proofs of the theorems in this paper, we will be dividing a circuit up into pieces and then we will let n grow larger. Technically, a circuit can only be divided into an integer fraction of pieces. However, in most cases this does not matter. To make this notion rigorous, we will need to use the following lemma: Lemma 3. Let h : R → R be a function such that |h (x) -x| ≤ a for sufficiently large x and some positive constant a. If there are functions f, g : R → R, and g is continuous for sufficiently large x, and if</p><formula xml:id="formula_8">lim x→∞ f (g (x)) = c for some constant c ∈ R, and if lim x→∞ g (x) = ∞ then lim x→∞ f (h (g (x))) = c. Proof: Suppose lim x→∞ f (g (x)) = c.</formula><p>To show that lim x→∞ f (h (g (x))) = c we need to construct, given some , a particular x 0 such that for all x &gt; x 0 , |f (h (g (x))) -c| &lt; . Since g grows unbounded, and is continuous for sufficiently large x, then there must be a particular value of x (call it x ) such that g (x) takes on all values greater than g (x ) for some x &gt; x . As well, for any &gt; 0 there exists some x such that for all x &gt; x , |f (g (x)) -c| &lt; . In particular this is true for some x &gt; x . Thus, choose x 0 to be the least number greater than x in which g (x 0 ) = g (x ) + a (this must exist because g takes on all values greater than g (x )). Thus, for x &gt; x 0 h (g (x)) only takes on values greater than g (x ) (because |h (x) -x| ≤ a). Since |f (g (x)) -c| &lt; for all x &gt; x , thus |f (h (g (x))) -c| &lt; for all x &gt; x 0 , since h (g (x)) can only take on values that g (x) takes on for x &gt; x . Corollary 1. This result applies when h (•) is the floor function, denoted • , since | x -x| ≤ 1.</p><p>We will need to make one observation that will be used in the three main theorems of this paper, which we present in the lemma below. Lemma 4. If &gt; 0 and n 1 , n 2 , . . . , n m are positive integers subject to the restriction that m i=1 n i ≤ n then: <ref type="figure">3</ref>. Example of a possible circuit undergoing two stages of nested minimum bisections. The dotted line down the middle is a first nested bisection, and the other two horizontal dotted lines are the bisections that divide the two subcircuits that resulted from the first stage of the nested bisections, resulting in four subcircuits. We are concerned with the number of bits communicated across r-stages of nested minimum bisections. In these two stages of nested minimum bisections, we see that 8 wires are cut. Because we assume wires are bidirectional, and thus two bits are communicated across these wires every clock cycle, in the case of this circuit we have Br = 8 × 2 × τ , where τ is the number of clock cycles. It will not be important how to actually do these nested bisections, rather it is important only to know that any circuit can undergo these nested bisections.</p><formula xml:id="formula_9">m i=1 (1 -n i ) ≤ 1 - n m m Fig.</formula><p>Proof: The proof follows from a simple convex optimization argument. Grover et al. in <ref type="bibr" target="#b2">[3]</ref> uses a nested bisection technique to prove a relation between energy consumed in a circuit computation and bits communicated across the r-stages of nested bisections which we present as a series of two lemmas, the second which we will use directly in our results. Lemma 5. For a circuit undergoing r-stages of nested bisections, in which the total number of bits communicated across all r-stages of nested bisections is B r , then</p><formula xml:id="formula_10">Aτ 2 ≥ √ 2 -1 2 16 B 2 r</formula><p>2 r+1 λ 2 where A is the area of the circuit and τ is the number of clock cycles during the computation.</p><p>Proof: See <ref type="bibr" target="#b2">[3]</ref> for a detailed proof. Here we provide a sketch. To accomplish this proof, r-stages of nested minimum bisections on a circuit are performed and then a principle due to Thompson <ref type="bibr" target="#b5">[6]</ref> is applied that states that the area of a circuit is at least proportional to the square of the minimum bisection width of the circuit. Also, the number of bits communicated to a subcircuit cannot exceed the number of wires entering that subcircuit multiplied by the number of clock cycles. The area of the circuit (related to the size of the minimum bisections performed) and the number of clock cycles (more clock cycles allow more bits communicated across cuts) are then related to the number of bits communicated across all the edges deleted during the r-stages of nested bisections. Lemma 6. If a circuit as described in Lemma 5 in addition has at least β nodes, then the Aτ complexity of such a computation is lower bounded by:</p><formula xml:id="formula_11">Aτ ≥ √ 2 -1 4 √ 2 β 2 r B r λ 2 w</formula><p>Proof: Following the same arguments of Grover et al. in <ref type="bibr" target="#b2">[3]</ref> (which we reproduce to get the more general result we will need), note that if there are at least β computational nodes, then A ≥ βλ 2 w which, when combined with Lemma 5 results in:</p><formula xml:id="formula_12">A 2 τ 2 ≥ √ 2 -1 2 16 B 2 r 2 r+1 βλ 4</formula><p>w which yields the statement of the Lemma upon taking the square root.</p><p>Remark 2. In terms of our energy notation, the result of Lemma 6 implies that for such a circuit with at least β computational nodes, the energy complexity is lower bounded by:</p><formula xml:id="formula_13">E proc ≥ ξ tech λ 2 w √ 2 -1 4 √ 2 β 2 r B r = K tech β 2 r B r</formula><p>where</p><formula xml:id="formula_14">K tech = ξ tech λ 2 w √ 2-1 4 √ 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bound on Block Error Probability</head><p>The key lemma that will be used in the first theorem of this paper is due to Grover et al. <ref type="bibr" target="#b2">[3]</ref>. We modify the lemma slightly. Lemma 7. For any code implemented using the VLSI model for an erasure channel with erasure probability , for any r &lt; log 2</p><formula xml:id="formula_15">k 2 , either P blk e ≥ 1 2 -1 - n i 2 r-1 2 r-1 or B r ≥ k 2 .</formula><p>The proof uses the same approach as Grover et al. in <ref type="bibr" target="#b2">[3]</ref> but we modify it slightly to ease the use of our lemma for our theorem and to conveniently deal with the possibility that a decoder can guess an output of a computation.</p><p>Let s i be the number of input bits erased in the ith subcircuit after r-stages of nested bisections. Furthermore, recall from Definition 3 that b i is the number of bits injected into the ith subcircuit during the computation. Also, recall from Definition 6 that n i is the number of input nodes located within the ith subcircuit. We use the principle that if</p><formula xml:id="formula_16">k 2 r &lt; n i -s i + b i</formula><p>for any subcircuit then the probability of block error is at least 1  2 . This is a very intuitive idea; if the number of bits that are not erased, plus the number of bits injected into a circuit is less than the number of bits the circuit is responsible for decoding, the circuit must at least guess 1 bit. This argument will be made formal in the proof that follows.</p><p>Proof: (of Lemma 7) Suppose that all the n i input bits injected into the ith subcircuit are the erasure symbol. Then, conditioned on this event, the distribution of the k i bits that this subcircuit is to estimate is uniform (owing to the symmetric nature of the binary erasure channel). Furthermore, if b i &lt; k 2 r then the number of bits injected into the subcircuit is less than the number of bits the subcircuit is responsible for decoding. Combining these two facts allows us to apply Lemma 2 directly to conclude that, in the event all the inputs bits of a subcircuit are erased, and the number of bits injected into the subcircuit is less than k 2 r , then the subcircuit makes an error with probability at least 1 2 . Denote the event that all inputs bits in subcircuit i are erased as W r i . The probability of this event is given by P (W r i ) = n i . Suppose that B r &lt; k/2 (where we recall from Definition 4 that B r is the total number of bits communicated across all edges cut in r-stages of nested minimum bisections). Let S = i : b i &lt; k 2 r be the set of indices i in which b i (the bits communicated to the ith subcircuit) is smaller than k 2 r . We first claim that |S| &gt; 2 r-1 . To prove this claim, let</p><formula xml:id="formula_17">S = i : b i ≥ k 2 r and note that k 2 &gt; B r = b i ≥ i∈ S k 2 r = S k 2 r</formula><p>, from which it follows that S &lt; 2 r-1 . Since |S| + S = 2 r , the claim follows.</p><p>Hence, in the case that B r ≤ k/2, because of the law of total probability:</p><formula xml:id="formula_18">P (correct) = P ∩ i∈S W r i P correct| ∩ i∈S W r i + P (∪ i∈S W r i ) P (correct| ∪ i∈S W r i ) ≤ i∈S (1 -n i ) + 1 2<label>(2)</label></formula><p>where the event ∩ i∈S W r i is the event that each of the subcircuits indexed in S, after r-stages of nested bisections, do not have all their n i input bits erased. We then note that, in this case, the probability of the circuit being decoded correctly is at most 1. For the second term, we note that conditioned on the event that at least one of the subcircuits indexed in S has all their input bits erased, since the circuit must at least guess 1 bit, the probability of the circuit decoding successfully is at most 1 2 , by Lemma 2. Since i∈S n i ≤ 2 r i=1 n i = n, subject to this restriction, Lemma 4 shows the expression in (2) is maximized when n i = n |S| for each subcircuit in S. Hence,</p><formula xml:id="formula_19">P (correct) ≤ 1 - n |S| |S| + 1 2 .</formula><p>Thus, either B r ≤ k 2 or |S| ≥ 2 r-1 which implies</p><formula xml:id="formula_20">P (correct) ≤ 1 - n 2 r-1 2 r-1</formula><p>+ 1 2 and so</p><formula xml:id="formula_21">P blk e = 1 -P (correct) ≥ 1 2 -1 - n 2 r-1 2 r-1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. KEY RESULT: A FUNDAMENTAL SCALING RULE FOR ENERGY OF LOW BLOCK ERROR</head><p>PROBABILITY DECODERS We define a coding scheme as a sequence of codes of fixed rate together with decoding circuits, in which the block length of each successive code increases. We define P blk,n e as the block error probability for the decoder of block length n in this scheme. An example of a coding scheme would be a series of regular LDPC codes together with LDPC decoding circuits in which the block length n doubles for each decoder in the sequence. Our results are general and would apply to any particular coding scheme using any circuit implementation and any decoding algorithm. The key result of this paper is given in the following theorem: Theorem 1. For every coding scheme in which lim n→∞ P blk,n e &lt; 1  2 , there exists some n 0 such that for all n &gt; n 0 , for any circuit implemented according to the VLSI model with parameters ξ tech and λ w ,</p><formula xml:id="formula_22">E dec &gt; K tech (log 2 n) log 2 1 Rn 2<label>(3)</label></formula><p>where E dec is the energy used in the decoding and</p><formula xml:id="formula_23">K tech = ξ tech λ 2 w ( √ 2-1) 4 √ 2</formula><p>is a constant that depends on circuit technology parameters that we defined before. Remark 3. The requirement that lim n→∞ P blk,n e &lt; 1 2 for our bound in (3) though reasonable, is not necessary for a good design. Typical block error probability requirements may be on the order of 10 -5 or 10 -6 , although if the block error probabilities are lower bounded by 1  2 for a series of decoding schemes, this is not necessarily a bad design. The individual bit error probabilities (the probability that a randomly selected output bit of the decoder is decoded correctly) may indeed be acceptably low. However, our results do not consider such schemes. It is also not necessary for a decoding scheme to have a block length that gets arbitrarily large. However, a capacity-approaching code must have block length that approaches infinity and our result can be used to understand how the energy complexity of such decoding algorithms approach infinity.</p><p>Proof: The theorem follows from an appropriate choice for r, the number of nested bisections we perform. We can choose any nonnegative integer r so that r &lt; log 2 k 2 . Note that k = nR is the number of bits the decoder is responsible for decoding. As k gets large, we can thus choose any r so that 1 ≤ 2 r ≤ k 2 . Thus, we choose an r so that, approximately, 2 r = 2 log( 1 )n K log n , for a value of K which we will choose later. In particular, we will choose r = log 2 2 log( 1 )n</p><formula xml:id="formula_24">K log n .</formula><p>This is valid so long as n is sufficiently large, for some 0 &lt; K &lt; 1. Note that log 1 &gt; 0 since 0 &lt; &lt; 1. Since k 2 = Rn 2 , this is a valid choice for r so long as</p><formula xml:id="formula_25">2 log 1 n K log n &lt; R<label>2</label></formula><p>which must occur as the left side of the inequality approaches 0 as n gets large. We can plug this value for r into Lemma 7, but we will simplify the expression by neglecting the floor function, as application of Lemma 2 will show that this does not alter the evaluation of the limit that we will compute, as we can see our choice for r grows unbounded with n. Thus, either</p><formula xml:id="formula_26">P blk,n e ≥ 1 2 -(1 -exp (-K log n)) 1 K log ( 1 ) n log n<label>(4)</label></formula><p>or, applying Lemma 6 by recognizing that there are at least β = n nodes,</p><formula xml:id="formula_27">E dec &gt; K tech (K log n) log 1 Rn 2 . (<label>5</label></formula><formula xml:id="formula_28">)</formula><p>By a direct application of Lemma 1, so long as K &lt; 1, the bound in (4) approaches 1 2 which we can see as follows:</p><formula xml:id="formula_29">lim n→∞ P blk,n e ≥ 1 2 -lim n→∞ (1 -exp (-K (log n))) Kn (log n) = 1 2 .</formula><p>This implies that, in the limit of large block sizes, the probability of block error must be lower bounded by 1  2 , unless B r ≥ k 2 . But then by <ref type="bibr" target="#b4">(5)</ref>, it must be that</p><formula xml:id="formula_30">E dec &gt; K tech (log n) log 1 Rn 2<label>(6)</label></formula><p>which is the result we are seeking to prove. The following corollary is immediate. Corollary 2. If a sequence of decoding schemes in which in the limit of large n P blk,n e &lt; 1 2 , the average decoding energy, per decoded bit (which we denote E dec,avg ) is bounded as:</p><formula xml:id="formula_31">E dec,avg &gt; K tech (log 2 n) log 2 1 .<label>(7)</label></formula><p>Proof: The proof follows simply by dividing (6) by nR, the number of bits such a code is responsible for decoding.</p><p>VI. SERIAL COMPUTATION Our result in Section V applies to decoders implemented entirely in parallel; however, this does not necessarily reflect the state of modern decoder implementations. Below we provide a modified version of the Thompson VLSI model that allows for the source of the computation to be input serially and the outputs to be computed serially.</p><p>In this modified model, we assume that the circuit computes a function of n inputs and k outputs. However, instead of having n input nodes and k output nodes, the circuit has p input nodes and j output nodes. The computation terminates after a set τ number of clock cycles, and during the τ clock cycles, the inputs to the computation may be input into the p input nodes (where p bits at most can be input during a single clock cycle), and the outputs of the computation must appear in the output nodes during specified clock cycles of the computation. Remark 4. The number of clock cycles τ must at least be enough to output all the bits. If there are j output nodes and k outputs to the function being computed, then there must be at least k j clock cycles. If all the inputs into the computation are being used, then there must also be at least n p clock cycles, though it is technically possible for some functions to have inputs that "don't matter" so this is not a strict bound for all functions.</p><p>Hence, a lower bound on the energy complexity for this computation is:</p><formula xml:id="formula_32">E proc ≥ ξ tech A c k j</formula><p>where A c is the area of the circuit. Theorem 2. Suppose there is a sequence codes together with decoding schemes with rate R and block length n approaching infinity. We label the block error probability of the length n decoder as P blk,n e . Also suppose that the number of output pins remains a constant j. Then either (a) lim n→∞ P blk,n e ≥ 1  2 or (b) there exists some n 0 such that for all n greater than n 0</p><formula xml:id="formula_33">E dec ≥ ξ tech λ 2 w R 2 n j log 1 (log n -j) = Ω (n log n) .</formula><p>To prove this theorem, instead of dividing the circuit into subcircuits, we will divide the computation conceptually in time, by dividing the computation into epochs. More precisely, consider dividing the computation outputs into chunks of size m (with the exception of possibly one chunk if m does not evenly divide k), meaning that there are k m such chunks. Hence, the outputs, which can be labeled (k 1 , k 2 , . . . , k k ) can be divided into groups, or a collection of subvectors K 1 , K 2 , . . . , K k m in which</p><formula xml:id="formula_34">K 1 = (k 1 , k 2 , . . . k m ), K 2 = (k m+1 , k m+2 , . . . k 2m ) and so on, until K k m = k m k m , k m k m +1 , . . . , k k . Definition 8.</formula><p>The set of clock cycles in the computation in which the bits in K i are output is considered to be the ith epoch.</p><p>In our analysis, we are interested in analyzing the decoding problem for chunks of the output as defined above for an m that we will choose later for the convenience of our theorem. We are also interested in another set of quantities: the input bits injected into the circuit between the time when the last of the bits in K i are output and the first of the bits in K i+1 bits are output. Label the collection of these bits as N 1 , N 2 , . . . , N k m . Label the size of each of these of these subvectors as n 1 , n 2 , . . . , n k m , so that the number of bits injected before all of the bits in K 1 are computed is n 1 , and the number of those injected after the first n 1 bits are injected and until the clock cycle when the last of the bits in K 2 are output is n 2 , and so on. Let s i be the number of erasures that are injected into the circuit during the ith epoch. Note that by Lemma 2 an error occurs when</p><formula xml:id="formula_35">m ≤ n i + Ā -s i where Ā = Ac λ 2 w</formula><p>is the maximum number of bits that can be stored in the circuit, remembering that according to the computation model the maximum number of bits that can be stored in a circuit must be proportional to the area of the circuit, as each wire in the circuit at any given time in the computation can hold only the value 1 or 0.</p><p>Proof: (of Theorem 2) Suppose we divide the circuit into chunks each of size Ā + j, j more than the normalized circuit area. Then, if all the bits n i are erased, the probability that at least one of the bits of K i is not decoded must at least be 1  2 , because there are simply not enough non-erased inputs for the circuit to infer the m bits it is responsible for decoding in that window of time. Note that we choose m = Ā + j so that an error event occurs with probability at least 1 2 when all the n i bits are erased, because it is technically possible that in a clock cycle that outputs the last of the bits of K i , j -1 bits of K i+1 are output. Then, the number of bits required to be computed for the next chunk of outputs is at least k i+1 -j + 1. Let the size of each K i (except possibly K k m ) be Ā + j. Similar to what we did for in Section IV-B, denote the event that all input bits in N i are erased as W i . Thus:</p><formula xml:id="formula_36">P (correct) = P ∩ k m i=1 Wi P correct| ∩ k m i=1 Wi + P ∪ k m i=1 W i P correct| ∪ k m i=1 W i ≤ k m i=1 (1 -n i ) + 1 2 .</formula><p>The first term is simplified by recognizing the independence of erasure events in the channel and the second term is simplified by the fact that, conditioned on the event that at least one subcircuit has input nodes being all erasure symbols, Lemma 2 applies and at least one subcircuit must make an error with probability at least 1 2 . Thus:</p><formula xml:id="formula_37">P blk,n e = 1 -P (correct) ≥ 1 - k m i=1 (1 -n i ) . (<label>8</label></formula><formula xml:id="formula_38">)</formula><p>It must be that k m</p><p>i=1 n i = n, and thus k m</p><p>i=1 n i ≤ n, where again n is the total number of inputs. We can apply Lemma 4 to show that the product term in ( <ref type="formula" target="#formula_37">8</ref>) is maximized when each n i is equal to</p><formula xml:id="formula_39">n i = n k m</formula><p>. Thus, we show that:</p><formula xml:id="formula_40">P blk e ≥ 1 -1 - n k m k m .</formula><p>For the sake of the convenience of calculation, we replace k m with k m , which will not alter the evaluation of the limit by Lemma 3, giving us:</p><formula xml:id="formula_41">P blk,n e ≥ 1 -1 - mn k k m (9)</formula><p>Since we have assumed m = Ā + j, suppose that Ā ≤ cR log 1 log n -j, and recognizing that k = Rn, and that m = Ā + j, substituting into (9) and simplifying gives us:</p><formula xml:id="formula_42">P blk e ≥ 1 2 -(1 -exp (-c log n)) log ( 1 ) Rn log n</formula><p>Thus, if c &lt; 1 and applying Lemma 1:</p><formula xml:id="formula_43">lim n→∞ P blk e ≥ 1 2 -lim n→∞ (1 -exp (-c log n)) log ( 1 ) Rn log n = 1<label>2</label></formula><p>Hence, either in the limit block error probability is at least 1 2 , or Ā &gt; cR log 1 log n -j and thus</p><formula xml:id="formula_44">E dec ≥ ξ tech λ 2 w Ā k j ≥ ξ tech λ 2 w R 2 n j log 1 (log n -j) = Ω (n log n) ,</formula><p>where we have used the fact that the number of clock cycles is at least k j as well as our bound on Ā.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. A GENERAL CASE: ALLOWING THE NUMBER OF OUTPUT PINS TO VARY WITH INCREASING BLOCK LENGTH</head><p>The results in Sections V and VI show that in the case of fully parallel implementations, the Area-Time complexity of decoders that asymptotically have a low block error probability must asymptotically have a super-linear lower bound. Technically, however, it may be possible to make a series of circuits with increasing block length, and have the number of output pins increase with increasing block length. We can show that, in this case, a super-linear lower bound exists as well where we require only weak assumptions on the circuit layout. This proof applies the main principle of this paper: namely that if a subcircuit has all its inputs erased, then that subcircuit must somehow have communicated to it other bits from outside this circuit, or it must, with high probability, make an error. In Theorem 1, we recognize that in a fully parallel computation these bits must be injected to it from another part of the circuit, resulting in some energy cost. In Theorem 2, we divide the circuit into epochs, and recognize that if all the input bits injected into the circuit during that epoch are erased then the circuit must have bits injected to it from before that epoch. But the number of bits that can be carried forward after each epoch is limited by the area of the circuit. In the general case in which the number of output pins can vary with block length, we divide the circuit into subcircuits and epochs (in essence, dividing the circuit in both time and space) and apply these two fundamental ideas.</p><p>To accomplish this lower bound, we will need this simplifying assumption: for any decoder with j output pins, each output pin is responsible for, before the end of the computation, outputting between k j and k j bits. Furthermore, we assume that each output bit produces an output at the same time. We call this assumption an output regularity assumption. This assumption allows us to divide the circuit into subcircuits and then epochs, and thus with this assumption each subcircuit can be divided into subcircuit epochs. The main structure of the proof will be this: if the energy of a computation is not high, then there will be many subcircuit epochs that do not have enough bits injected into them to overcome the case of one of them having all of their input bits erased. The task is thus to choose a correct number of divisions of the circuit into subcircuits and epochs, so that the probability of this event (that a subcircuit epoch makes an error) is high unless the area-time complexity of the computation is high. Theorem 3. For a sequence of codes and circuit implementations of decoding algorithms in which block length n gets large, and where the number of output pins j can vary with block length n, and the computation performed by the decoders is consistent with the output regularity assumption, then, in the limit as n approaches infinity, either (a) lim n→∞ P blk,n e ≥ 1  2 or (b) for a sufficiently large n, E dec ≥ Ω n (log n)</p><p>1 5 .</p><p>Proof: The proof is given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONSEQUENCES</head><p>A direct consequence of our work is that as code rates approach capacity, the average energy of decoding, per bit, must approach infinity. It is well known from <ref type="bibr" target="#b16">[16]</ref> and <ref type="bibr" target="#b17">[17]</ref> and further studied in <ref type="bibr" target="#b18">[18]</ref> that as a function of fraction of capacity η = R C , the minimum block length scales approximately as</p><formula xml:id="formula_45">n ≈ c (1 -η) 2</formula><p>for a constant c that depends on the channel and target probabilities of error. We are not concerned about the value of this constant, but rather the dependence of this approximation on η. Plugging this result into <ref type="bibr" target="#b6">(7)</ref> implies:</p><formula xml:id="formula_46">E dec,avg K tech log 2 c (1-η) 2 log 2 1 = Ω log 1 1 -η .</formula><p>This result implies that not only must the total energy of a decoding algorithm approach infinity as capacity is approached (this is a trivial consequence of the fact that block length must approach infinity as capacity is approached), but also the energy per bit must approach infinity as capacity is approached. Thus, if the total energy per bit is to be optimized, a rate strictly less than capacity must be used. We cannot get arbitrarily close to optimal energy per bit by getting arbitrarily close to capacity, which would be the case if there were linear energy complexity algorithms with block error probability that stay less than 1 2 . The result of Theorem 2 can also be extended to find a fundamental lower bound on the average energy per bit of serially decoded, capacity-approaching codes. For the same reason as in the fully parallel case, we can see that as a function of gap to capacity, the average energy per bit for a decoder must scale as</p><formula xml:id="formula_47">E dec,avg ≥ 2λ 2 w R j log 1 log 1 1 -η -j = Ω log 1 1 -η .</formula><p>Finally, it can be shown from Theorem 3 that in circuits in which the output pins can grown arbitrarily, and the regular output rate condition is satisfied, the average energy per bit as a function of gap to capacity must scale as</p><formula xml:id="formula_48">E dec,avg ≥ Ω log 1 1 -η 1 5</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. UPPER BOUND ON ENERGY OF REGULAR LDPC CODE</head><p>We have shown that for any code and decoding circuit with block error probability that is below 1  2 , the Area-Time complexity must scale at least as fast as Ω n √ log n . We provide here an example of a particular circuit layout that achieves O (n 2 ) complexity. Low density parity check (LDPC) codes are standard codes first described by Gallager in <ref type="bibr" target="#b19">[19]</ref>. There have been a number of papers that have sought to find very energy-efficient implementations of LDPC decoders; for example <ref type="bibr" target="#b20">[20]</ref>. The reference <ref type="bibr" target="#b21">[21]</ref> gives an overview of various techniques used to create actual VLSI implementations of LDPC decoders. However, these papers have not sought to view how the energy per bit of these decoders scales with block length; they show a method to optimize an LDPC decoder of a particular block length and show that their implementation method improves over a previous implementation. Our goal is to provide an understanding of how a particular implementation of LDPC codes should scale with block length n.</p><p>We provide a simple circuit placement algorithm that results in a circuit whose area scales no faster than O |E| 2 where |E| is the number of edges in the circuit. For a regular LDPC code with constant node degrees, this implies that the area scales as O (n 2 ).</p><p>The placement algorithm proposed involves actually instantiating the Tanner Graph of the LDPC code with wires, where each edge of the Tanner graph corresponds to a wire connected to n subcircuits that perform variable node computations and the n -k subcircuits that perform check node computations. Our concern is not about the implementation of the variable and check nodes in this circuit. In the diagram, we treat these as merely a "black box" whose area is no greater than proportional to the square of the degree of the node. Of course, the actual area of these nodes is implementation specific, but the important point is that the area of each node should only depend on the particular node degree and not on the block length of the entire code. Our concern is actually regarding how the area of the interconnecting wires scales. The wires leading out of each of these check and variable node subcircuits correspond to edges that leave the corresponding check or variable node of the Tanner graph. The challenge is then to connect the variable nodes with the check nodes with wires as they are connected in the Tanner graph in a way consistent with our circuit axioms. We lay out all the variable nodes on the left side of the circuit, and all the subcircuits corresponding to a check node on the right side of the circuit, and place the outputs of each of these subcircuits in a unique row of the circuit grid (see Fig. <ref type="figure">4</ref>). Note that the number of outputs for each variable and check node subcircuit will be equal to the degree of that corresponding node in the Tanner graph of the code. The height of this alignment of nodes will be 2 |E|, twice the number of edges in the corresponding Tanner graph (as there must be a unique row for each of the |E| edges of the variable nodes and also for the |E| edges leading from the check nodes.</p><p>The distance between these columns of check and variable nodes is |E|. Each output of the variable nodes is assigned a unique grid column that will not be occupied by any other wire (except in the case of a crossing, which according to our model is allowed). A horizontal wire is drawn until this column is reached, and then the wire is drawn up or down along this column until it reaches the row corresponding to the variable node to which it is to be attached. A diagram of the procedure to draw such a circuit for a case of 6 edges is shown in Fig. <ref type="figure">4</ref>. Since each output of the variable and check node "black boxes" takes up a unique row, and each wire has a unique column, no two wires in drawing this circuit can ever run along the same edge; they can only cross, which is permitted in our model.</p><p>The total area of this circuit is thus bounded by: A c ≤ A nodes + A w , where A nodes is the area of the nodes and A w is the area of the wires. Now it is sufficient that there is a grid row for each output of the variable nodes and the check nodes, and that there is a column for each edge. Hence</p><formula xml:id="formula_49">A w ≤ 2 |E| • |E| = 2 |E| 2 .</formula><p>We assume that the area of the subcircuits that perform the computational node operations can complete their operation in one clock cycle and take up area proportional to the square of their degree. Hence we suppose that</p><formula xml:id="formula_50">A nodes ≤ d 2 v n + d 2 c (n -k)</formula><p>, where d v is the degree of the variable nodes and d c the degree of the check nodes. We then conclude that:</p><formula xml:id="formula_51">A c ≤ 2 |E| 2 + d 2 v n + d 2 c (n -k) .</formula><p>The total energy for the computation will depend on the number of iterations performed. Since each iteration requires sending information for the variables nodes to the check nodes and back again, this can be performed in 2 clock cycles. Hence, τ = 2N , where N is the number of iterations performed, and of course τ is the number of clock cycles in the computation.</p><p>Thus, the total energy of this implementation of an LDPC code is upper bounded by</p><formula xml:id="formula_52">E dec ≤ 2N 2 |E| 2 + d 2 v n + d 2 c (n -k) .</formula><p>The work of Lentmaier et al. <ref type="bibr" target="#b22">[22]</ref> has shown that for an LDPC decoder, for asymptotically low block error probability, τ = O (log log n) iterations are sufficient if the node degrees are high enough. This then results in an upper bound on the energy of</p><formula xml:id="formula_53">E dec ≤ 2N 2 (nd v ) 2 + d 2 v n + d 2 c (n -k) = O n 2 log log n .</formula><p>X. CONCLUSION This work expands on previous work in <ref type="bibr" target="#b2">[3]</ref> by providing a standard to which decoding algorithms together with circuit implementations can be compared. Earlier work on the energy used in decoding (for example, <ref type="bibr" target="#b20">[20]</ref>) have involved trying to optimize a circuit that implements a particular code; they have not sought to understand how the energy scales with the length of the code. Some work has provided an analysis of the energy requirements for specific types of codes. The work in <ref type="bibr" target="#b23">[23]</ref> has provided a way to analyze the energy requirements for an LDPC decoder. The result of our paper is more general: it applies to any decoding algorithm. Further investigations should compare the results in this paper to existing results relating energy per bit with parameters like block error probability. Fig. <ref type="figure">4</ref>. An example of an implementation of an LDPC code Tanner graph. There are |E| edges that correspond to interconnections that must be made. Going left to right, starting at the top left circuit, the six parts of this diagram show the progressive addition of each additional "edge" in the circuit implementation of the Tanner Graph. Each wire has a unique column that it is allowed to run along, and each output has a unique row, ensuring that no two wires ever need to run along the same section. The only time when the wires need to intersect is during a wire crossing, which is explicitly allowed by our circuit axioms. This method can be used to draw any arbitrary bipartite graph with |E| edges.</p><p>Finally, this paper should also be used to guide the development of new codes that attempt to approach this fundamental lower bound. There may be some modifications of some types of codes, for example LDPC codes, whose Area-Time complexity is chosen to be O n √ log n (for example, by choosing the neighbors of the nodes in the Tanner graph representation of such a code to limit the area of the code, or by limiting the number of iterations). Most analyses of LDPC codes assume a random Tanner graph. If the interconnections of the LDPC code are restricted to limit the area of the implementation (and thus violating the assumptions of most LDPC code analyses) will the decoder still have a good performance? The work of <ref type="bibr" target="#b10">[11]</ref> suggests there is some kind of trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof Of Theorem 3</head><p>To accomplish this super-linear lower bound, we divide how the number of output pins j scales with n, the block length, into cases. We suppose that j ≥ √ log k. If not, using our result from Theorem 2, for codes with asymptotic block error probability less than 1  2 ,</p><formula xml:id="formula_54">E dec ≥ λ 2 w R 2 n log 1 log n j -1 , if j &lt; √ log k then we can show that Aτ ≥ λ 2 w R 2 n log 1 log k -1 = Ω n log n ≥ Ω n (log n) 1 5</formula><p>and we are done.</p><p>Remark 5. Technically, the statement that either j ≥ √ log k or j &lt; √ log k does not fully specify all possible sequences of output pins. However, for any sequence, we can divide the sequences into separate subsequences, specifically the sequences of codes in which j ≥ √ log k and in which j &lt; √ log k. For each of those subsequences we can prove our lower bound.</p><p>Let Ā = A λ 2 be the normalized circuit area. Suppose also that Ā j ≤ log 0.9 n. Otherwise, if Ā j &gt; log 0.9 n and from our simple bound in Remark 4, we can see that</p><formula xml:id="formula_55">Āτ ≥ Ā k j &gt; k log 0.9 n ≥ Ω n (log n) 1 5</formula><p>and we are done. Note again that, just as in Remark 5, if the area alternates between log 0.9 n with increasing block length, we can simply divide the sequence of decoders into two subsequences and prove that the necessary scaling law holds for each subsequence.</p><p>Hence, we consider the case that we have a sequence of serial decoding algorithms in which the area of the circuit grows with the block length n and the number of output nodes on the circuit grows with n. We consider the case in which Ā j ≤ log 0.9 n (10) and j ≥ log k</p><p>We will now choose a way to divide the computation into M epochs and N subcircuits.</p><p>For each of the M epochs we want the number of bits responsible on average for each decoder to decode to be four times the area. This will mean that, even if we optimistically assume that before the beginning of each epoch a circuit had already computed the future outputs, a typical subcircuit can only store a fraction of the bits it is responsible for decoding in the next epoch. Note that the number of bits that a subcircuit is responsible for in total over the entire computation must be k N and hence, if the computation is to be divided into M epochs, during each epoch, an average subcircuit must be responsible for decoding k M N bits. We seek to choose an M such that k M N ≥ 4 Āsubckt,avg</p><p>where Āsubckt,avg is the average normalized area of a subcircuit. This will be true if</p><formula xml:id="formula_57">k M N ≥ 4 Ā N or equivalently if M ≤ k 4 Ā so we choose M = k 4 Ā</formula><p>We also want N M = cn log n for a constant c which we will choose later, so we choose</p><formula xml:id="formula_58">N = cn4 Ā k log n = c4 Ā R log n .</formula><p>We need to show that this is a valid choice for N . The restriction on the choice of N is that N ≤ j (we can't subdivide the circuit into more subcircuits than there are output pins). By applying the assumption on the scaling of the area of the circuit in <ref type="bibr" target="#b9">(10)</ref> we can see that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ā</head><p>R log n ≤ 4cj log 0.9 n R log n = j 4c R log 0.9 n log n is asymptotically less than j, and hence this choice of N is valid. Our choice of M is k 4A . The restriction on the choice of M is that M ≤ k j (there must be at least one output per pin per epoch). Thus k 4 Ā ≤ k j , which will be true when j ≤ 4A. But since the j output pins form part of the area of the circuit, this must always be satisfied. On a minor technical note, we can only choose integer values of M . Hence, we can decide to choose the floor of M . But, as argued in Lemma 3 if the function for choosing M grows with n then the evaluation of a limit where we neglect this floor function is the same. So, our other requirement, that lim n→∞ Ā n = 0 means that our choice for M grows as n increases. We consider the case when area of the computation remains proportional to n at the end of this section.</p><p>Let the number of bits injected into the ith subcircuit during the qth epoch from other parts of the circuit be b i,q . Now, the average subcircuit has area Ā N , so we consider the set of subcircuits that have area less than 2 Ā N . This number must be at least 1 2 the subcircuits, otherwise the total area of all these subcircuits would exceed the total circuit area. Denote the set of indices of subcircuits with area less than 2 Ā N as T . Note that |T | ≥ N 2 . Consider a specific qth epoch. Suppose that the total number of bits injected into the subcircuits indexed by T after r-stages of nested bisections during this epoch is less than j 2 . If this is true, then there must be at least half of the subcircuits in T that have fewer than 2j</p><p>N bits injected into them. Otherwise, the total number of bits injected into these subcircuits is at least</p><formula xml:id="formula_59">|T | 2 2j N ≥ N 4 2j N = j</formula><p>2 , which we assumed is not the case.</p><p>Thus, with our assumptions, at the qth epoch, either the total number of bits injected across r-stages of nested minimum bisections is at least j 2 , or there are at least N 4 subcircuits with area less than 2 Ā that have less than j 2N bits injected into them. Denote the set of indices of these low area subcircuits with a low number of bits injected into them during the qth epoch as S q . The size of S q we have assumed to be at least N  4 , so for the sake of simplicity define S * q as a subset of S q with size exactly N 4 . Now, consider the number of epochs during which there are less than j 2 bits injected across all the bisections. Either this number is less than M 2 or greater than or equal to M 2 . Suppose that it is greater than or equal to M 2 . Denote the set of indices denoting the epochs in which the number of bits injected across all the bisections during that epoch is less than j 2 as Q, and a particular set of size exactly M 2 as Q * (chosen for simplicity of computation).</p><p>We now apply the key principle used for all the theorems in this paper. Consider a particular qth epoch where q ∈ Q * , an epoch with less than j 2 bits communicated across all the bisections. During this epoch the subcircuits in S q are those with less than 2j N bits injected into them, and they have area at most 2 Ā N , and are responsible for decoding 4 Ā N bits. Let the number of input bits injected into the circuit for such a particular subcircuit be n i,q . If all of these inputs are erased, then, by applying Lemma 2 the circuit must guess at least 1 output, and the probability of error is at least 1  2 , because in this case they only have</p><formula xml:id="formula_60">B i,q + 2 Ā N ≤ 2j N + 2 Ā n ≤ 4 Ā</formula><p>N bits to use. Using the same argument as in Theorems 2 and 1, we can show that if, for all the subcircuits in S * q , where q ∈ Q * , in the event that for any of these subcircuits all of their n i,q input bits are erased, then an error occurs with probability 1  2 . Applying this principle gives us:</p><formula xml:id="formula_61">P blk e ≥ 1 2 - q=Q * i∈S * q (1 -n i,q )<label>(12)</label></formula><p>where we note as well that</p><formula xml:id="formula_62">M 2 q=1 N 4</formula><p>i=1 n i,q ≤ n.</p><p>Subject to those restrictions, Lemma 4 implies that the expression in ( <ref type="formula" target="#formula_61">12</ref>) is minimized when each of the n i,q are equal to 8n N M . Hence</p><formula xml:id="formula_63">P e,blk ≥ 1 2 -1 - 8n N M N M 8 ≥ 1 2 -1 -8c log n log n 8n ,</formula><p>which, by applying Lemma 1, can easily be shown to approach 1 2 when n gets larger, if c is chosen to be log 1   8 . Hence, either in the limit block error probability approaches 1  2 or the size of Q is greater than M 2 , and there are many bits communicated in the circuit for many epochs. From Lemma 6, by recognizing that for the circuit under consideration there are at least j nodes, if there are B r bits injected across all the r-stages of nested minimum bisections, then</p><formula xml:id="formula_64">Āτ ≥ K B r j 2 r</formula><p>where K = √ 2-1 4 √ 2 and 2 r = N , the number of subcircuits into which the circuit was divided. Thus, combining this bound with our choice for N and the assumption that there are at least j 2 bits injected across all the bisections for epochs in Q * , we get that either</p><formula xml:id="formula_65">Āτ i ≥ K j 2 j N</formula><p>for at least M 2 epochs, or lim n→∞P blk,n e ≥ 1 2 . Hence, in total,</p><formula xml:id="formula_66">Āτ ≥ K j 2 j N M 2 = K j 2 j N k 8 Ā = K j 1.5 2 R log n 4c Ā k 8 Ā Ā2.5 τ ≥ K 32 kj 1.5 R log n c<label>(13)</label></formula><p>We also have the bound from Remark 4: τ ≥ k j , which implies τ 1.5 ≥ k 1.5 j 1.5   and hence, combining this with (13), we get Ā2.5 τ 2.5 ≥ K 32 k 2.5 R log n c Āτ ≥ K</p><p>2 5 4 k 8R log n log 1 1 5</p><p>= Ω k (log n)</p><formula xml:id="formula_67">1 5</formula><p>.</p><p>Finally, we must consider a case when the area of the circuit scales with n. This must be treated separately because in this case our choice for M in the above argument does not necessarily grow with n and so we can't assume that our rounding approximation is valid. Thus, suppose that A = cn. Suppose also that j ≤ k (log n) 0.9 . Then τ ≥ k j ≥ (log n) 0.9</p><p>from Remark 4, and therefore the total Area-Time complexity of such a sequence of decoders scales as Aτ ≥ cn (log n) 0.9 = Ω n (log n) 0.9 .</p><p>In the other case, when j ≥ k (log n) 0.9 then we can subdivide the circuit into n log n pieces, and make the same argument that has been made in Theorem 1 that the number of bits communicated across all cuts during the course of the computation must be proportional to k/2. Recognizing that we have assumed there are at least cn nodes in the circuit and applying Lemma 6, and also substituting 2 r = log 1 n log n we get:</p><formula xml:id="formula_68">Āτ ≥ √ 2 -1 8 √ 2 cR log n log 1 n = Ω n log n</formula><p>which of course is asymptotically faster than Ω n (log n)</p><p>1 5 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of two graphs with a minimum bisection labelled. Nodes are represented by circles and edges by lines joining the circles. A dotted line crosses the edges of each graph that form a minimum bisection.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Sys. Techn. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VLSI complexity of coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The MIT Conf. on Adv. Research in VLSI</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fundamental limits on the power consumption of encoding and decoding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sahai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2012 IEEE Int. Symp. Info. Theory</title>
		<meeting>2012 IEEE Int. Symp. Info. Theory</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2716" to="2720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A complexity theory for VLSI</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Big omicron and big omega and big theta</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGACT News</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Area-time complexity for VLSI</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Ann. ACM Symp. Theory of Comput</title>
		<meeting>11th Ann. ACM Symp. Theory of Comput</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Error control coding in low-power wireless sensor networks: When is ECC energyefficient?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schlegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Iniewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. on Wireless Commun. and Netw</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Rabaey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandrakasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikolic</surname></persName>
		</author>
		<title level="m">Digital Integrated Circuits</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Complexity-optimized low-density parity-check codes for Gallager decoding algorithm B</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ardakani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Kschischang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2005 IEEE Int. Symp. on Info. Theory</title>
		<meeting>2005 IEEE Int. Symp. on Info. Theory</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1488" to="1492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An algorithm for the machine calculation of complex Fourier series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Comput</title>
		<imprint>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Design of LDPC graphs for hardware implementation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2002 IEEE International Symposium on Information Theory</title>
		<meeting>2002 IEEE International Symposium on Information Theory</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">483</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C.-H</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilayer VLSI layout for interconnection networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Varvarigos</surname></persName>
		</author>
		<author>
			<persName><surname>Parhami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl Conf. Parallel Processing</title>
		<meeting>Intl Conf. Parallel essing</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A review of adiabatic computing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1994 IEEE Symposium on Low Power Electronics</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="94" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Information-friction&apos; and its implications on minimum energy required for communication</title>
		<author>
			<persName><forename type="first">P</forename><surname>Grover</surname></persName>
		</author>
		<idno>abs/1401.1059</idno>
		<ptr target="http://arxiv.org/abs/1401.1059" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Some simplified NP-complete graph problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stockmeyer</surname></persName>
		</author>
		<ptr target="http://www.sciencedirect.com/science/article/pii/0304397576900591" />
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="267" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Asymptotische abschätzungen in Shannons Informationstheorie</title>
		<author>
			<persName><forename type="first">V</forename><surname>Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the 3rd Prague Conference on Information Theory, Statistical Decision Functions, Random Processes. Prague: Pub. House of the Czechoslovak Academy of Sciences</title>
		<imprint>
			<date type="published" when="1962">1962</date>
			<biblScope unit="page" from="689" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Information Theory and Reliable Communication</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Channel coding: non-asymptotic fundamental limits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Polyanskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-density parity-check codes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gallager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="28" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
	<note>Information Theory</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Power reduction techniques for LDPC decoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Darabiha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chan Carusone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Kschischang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1835" to="1845" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Area, throughput, and energy-efficiency trade-offs in the VLSI implementation of LDPC decoders</title>
		<author>
			<persName><forename type="first">C</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cevrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Leblebici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2011-05">May 2011</date>
			<biblScope unit="page" from="1772" to="1775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An analysis of the block error probability performance of iterative decoding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lentmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Truhachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zigangirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Costello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3834" to="3855" />
			<date type="published" when="2005-11">Nov 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LDPC decoder area, timing, and energy models for early quantitative hardware cost estimates</title>
		<author>
			<persName><forename type="first">M</forename><surname>Korb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Noll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 Int. Symp. System on Chip</title>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="169" to="172" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
