<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oded</forename><surname>Ovadia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meni</forename><surname>Brief</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Moshik</forename><surname>Mishaeli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Oren</forename><forename type="middle">Elisha</forename><surname>Microsoft</surname></persName>
						</author>
						<title level="a" type="main">Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5991378204DFD842ACCAB961F94D9726</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-24T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LLMs) are able to capture vast amounts of factual information <ref type="bibr" target="#b31">(Petroni et al., 2019;</ref><ref type="bibr" target="#b7">Cohen et al., 2023;</ref><ref type="bibr" target="#b12">Hu et al., 2023)</ref>. LLMs exhibit a remarkable level of knowledge in various domains due to their massive pre-training datasets. However, there are two significant limitations to this knowledge. First, it is static and does not update with time. Second, it is non-specific and thus may lack nuanced expertise in particular domains. While these are two different problems, they are deeply related since their solution is the same: enhancing the model's knowledge.</p><p>Recently, the idea of adapting LLMs to particular domains and updating their knowledge has * Equal contribution. become increasingly common <ref type="bibr" target="#b53">(Yu et al., 2022)</ref>. Various models have been suggested to improve factual knowledge and capabilities in diverse fields such as healthcare <ref type="bibr">(Singhal et al., 2023a,b;</ref><ref type="bibr">Wu et al., 2023a)</ref>, finance <ref type="bibr">(Wu et al., 2023b;</ref><ref type="bibr" target="#b52">Yang et al., 2023)</ref>, and law <ref type="bibr" target="#b13">(Huang et al., 2023;</ref><ref type="bibr" target="#b27">Nguyen, 2023)</ref>.</p><p>In this work, we focus on the evaluation of a model's knowledge and its ability to memorize, understand, and retrieve factual data. We aim to understand the concept of knowledge injection <ref type="bibr" target="#b47">(Wang et al., 2020;</ref><ref type="bibr" target="#b3">Chen et al., 2022;</ref><ref type="bibr" target="#b21">Liu et al., 2020;</ref><ref type="bibr" target="#b19">Lauscher et al., 2020)</ref>. Given some knowledge base in the form of a text corpus, what is the best way to teach a pre-trained model this knowledge?</p><p>One way to add knowledge to a pre-trained model is through fine-tuning. With fine-tuning, we continue the model's training process and adapt it using task-specific data. By exposing the model to a specific knowledge base, we expect the model weights to adapt accordingly. This process is meant to optimize the model for targeted applications, enhancing its performance and contextual relevance in specialized domains.</p><p>Another method to enhance a model's knowledge base is through the use of in-context learning (ICL) <ref type="bibr" target="#b4">(Chen et al., 2021;</ref><ref type="bibr" target="#b32">Radford et al., 2019;</ref><ref type="bibr" target="#b23">Min et al., 2021;</ref><ref type="bibr" target="#b18">Lampinen et al., 2022)</ref>. The main idea behind ICL is to improve the performance of pretrained LLMs on new tasks by modifying the input query to the model without directly changing the weights of the model. One form of ICL is retrieval augmented generation (RAG) <ref type="bibr" target="#b20">(Lewis et al., 2020;</ref><ref type="bibr" target="#b26">Neelakantan et al., 2022)</ref>. RAG uses information retrieval techniques to enable LLMs to obtain relevant information from a knowledge source and incorporate it into generated text.</p><p>This study aims to evaluate the knowledge injection capabilities of LLMs through a comparison of fine-tuning and RAG. To illustrate the rationale, let us use an analogy. Consider three college students taking a test on a specific topic. All had access to class materials but didn't know the topic beforehand. The first student had the textbook only during the test, the second had pre-test access and studied, and the third lost access upon the test announcement. Who would probably perform better?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>To assess knowledge injection, we must first understand what knowledge means for LLMs.</p><p>Knowledge and Language Models Defining knowledge is a complex philosophical task far beyond the scope of this research. However, we can examine what factual knowledge means in the context of language models. If a model knows a fact, it can accurately and consistently answer questions about it. Furthermore, it can reliably distinguish between true and false statements related to this fact. We can then extend this definition to a whole knowledge base, not just a single fact.</p><p>Mathematically, let Q = {q n } N n=1 be a set of N multiple choice factual questions, where each question has L possible answers and exactly one correct answer. Let A = {(a 1 n , . . . , a L n )} N n=1 be the corresponding set of possible answers, and C = {c n } N n=1 be the correct answers. Let M be a language model. We denote by M(q n ) ∈ {a 1 n , . . . , a L n } the predicted answer of the model to the n-th question. We define the knowledge score L of M in relation to Q to be the standard accuracy score:</p><formula xml:id="formula_0">L M,Q := #{q n | M(q n ) = c n } N .<label>(1)</label></formula><p>We say that the model M possesses any knowledge regarding the set of questions Q if the following holds:</p><formula xml:id="formula_1">L M,Q &gt; 1 L .<label>(2)</label></formula><p>In simpler terms, the model can consistently give correct answers, outperforming a simple random guessing baseline. Naturally, if the knowledge score L M,Q is higher for one model compared to another, then we assert that the former is more knowledgeable with regards to Q compared to the latter.</p><p>Previously Seen Knowledge One important distinction to make is between knowledge that the model has been exposed to before during pretraining as opposed to entirely new facts. Considering the size of modern LLM training sets, they cover a vast amount of information available through web-sourced text. As a result, even in niche domains, the goal of knowledge injection is not necessarily to teach the model entirely new facts but rather to "refresh" its memory by inducing a bias toward a particular domain.</p><p>Knowledge and Reasoning We emphasize that this knowledge evaluation framework for LLMs is imperfect. Importantly, it doesn't address other quality metrics influencing a model's response. Creating a purely knowledge-intensive dataset without involving some level of reasoning is challenging. Consequently, a model with robust reasoning abilities might excel on unfamiliar knowledge-intensive tasks by making "educated guesses" in a multiple-choice exam. Therefore, any evaluation of knowledge in LLMs should consider this, with results seen as part of a broader range of benchmarks for reasoning <ref type="bibr" target="#b34">(Sakaguchi et al., 2021)</ref>, reading comprehension <ref type="bibr" target="#b8">(Dua et al., 2019)</ref>, and general language abilities <ref type="bibr" target="#b40">(Srivastava et al., 2022)</ref>. However, this evaluation framework still strongly emphasizes factual information above all else.</p><p>Causes for Factual Errors There are many possible reasons for the failure of models to answer factual questions accurately. In <ref type="bibr" target="#b46">(Wang et al., 2023)</ref>, Wang et al. introduce a taxonomy of five main model-level causes:</p><p>• Domain knowledge deficit: A language model may lack comprehensive expertise in a specific domain to which it has not been exposed. For example, a model trained exclusively on texts written by William Shakespeare would perform poorly when asked about the works of Mark Twain.</p><p>• Outdated Information: LLMs invariably have a cutoff date determined by their training dataset. Consequently, any events, discoveries, or changes occurring after the last training update will not be within the model's knowledge without access to external sources.</p><p>• Immemorization: Sometimes, a model is exposed to knowledge during its training process but does not retain it. This is especially true for rare facts that appear in the training dataset only scarcely <ref type="bibr" target="#b16">(Kandpal et al., 2023)</ref>.</p><p>• Forgetting: Language models often undergo additional training after the pre-training phase (fine-tuning). In some cases, this might lead to a phenomenon called catastrophic forgetting ( <ref type="bibr" target="#b17">Kirkpatrick et al., 2017;</ref><ref type="bibr" target="#b10">Goodfellow et al., 2013;</ref><ref type="bibr" target="#b2">Chen et al., 2020;</ref><ref type="bibr" target="#b22">Luo et al., 2023)</ref>, where models lose some of the knowledge they had prior to the fine-tuning process.</p><p>• Reasoning Failure: In certain instances, a language model might possess relevant knowledge about a fact but fail to utilize it properly. This is particularly evident in complex multi-step reasoning tasks <ref type="bibr" target="#b41">(Tan et al., 2023)</ref> or when posed with different questions about the same fact, resulting in disparate outcomes <ref type="bibr" target="#b1">(Berglund et al., 2023)</ref>.</p><p>We observe that most of these issues arise during the pre-training phase, with catastrophic forgetting being the notable exception. Hence, many LLMs will suffer from factual errors of this kind regardless of any post-training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Injecting Knowledge to Language Models</head><p>Following the background given in Section 2, it is clear that general pre-training is insufficient for many knowledge-intensive tasks. To solve this, an additional post-processing step is essential to augment the knowledge of a pre-trained model. This step is often reffered to as knowledge injection <ref type="bibr" target="#b47">(Wang et al., 2020;</ref><ref type="bibr" target="#b3">Chen et al., 2022;</ref><ref type="bibr" target="#b21">Liu et al., 2020;</ref><ref type="bibr" target="#b19">Lauscher et al., 2020)</ref>.</p><p>In this section, we examine two widely used frameworks for knowledge injection: fine-tuning (FT) and retrieval augmented generation (RAG). We begin by formulating the knowledge injection problem, aiming to explain both methods using consistent terminology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem formulation</head><p>In Equations ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>), we presented a formulation for knowledge in language models through the lens of question-answering (Q&amp;A). We now extend this formulation to the problem of knowledge injection using the same terminology.</p><p>Given a set of factual questions, there exists some text corpus containing information that is relevant to these questions. The central assumption of knowledge injection is that given full access to this corpus, it could serve as an auxiliary knowledge base and improve the model's performance on this set of questions.</p><p>Mathematically, let M be a pre-trained model, and let Q be a set of factual questions, as before. Now, assume we have a relevant auxiliary knowledge base B Q . Our objective is to discover a transformation, denoted as F, that, when applied, would enhance the knowledge about Q:</p><formula xml:id="formula_2">M ′ := F(M, B Q ) s.t. L M ′ ,Q &gt; L M,Q . (3)</formula><p>In this work, we aim to compare two choices for F: fine-tuning and RAG to see which option performs better in this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-Tuning</head><p>Fine-tuning is the process of adjusting a pre-trained model on a specific, often narrower, dataset or task to enhance its performance in that particular domain. Here, it is vital to distinguish between different types of fine-tuning. FT techniques are commonly classified into supervised, unsupervised, and reinforcement learning (RL) based methods. We proceed by briefly reviewing these methods and their relation to the problem of knowledge injection.</p><p>Supervised Fine-Tuning Supervised finetuning (SFT) requires sets of labeled input-output pairs. One of the most common SFT methods is instruction tuning <ref type="bibr" target="#b48">(Wang et al., 2022;</ref><ref type="bibr" target="#b24">Mishra et al., 2021;</ref><ref type="bibr" target="#b30">Ouyang et al., 2022;</ref><ref type="bibr" target="#b42">Taori et al., 2023)</ref>, which has emerged as one of the most powerful methods to improve model performance. With instruction tuning, the input is a natural language task description, and the output is an example of the desired behavior. Many current state-of-the-art LLMs have gone through instruction tuning after their pre-training phase.</p><p>Instruction tuning has been shown to be very effective at improving the overall quality of the model, with a particular emphasis on its zero-shot and reasoning capabilities. However, despite these advantages, instruction tuning does not necessarily teach the model new knowledge <ref type="bibr" target="#b30">(Ouyang et al., 2022;</ref><ref type="bibr">Chung et al., 2022;</ref><ref type="bibr" target="#b25">Mitra et al., 2023;</ref><ref type="bibr">Chia et al., 2023;</ref><ref type="bibr" target="#b54">Zhou et al., 2023)</ref>. As such, instruction tuning alone is not a viable solution to the knowledge injection problem.</p><p>Reinforcement Learning Another form of FT relies on RL or RL-inspired optimization strategies to better align the model after its pre-training phase. A few prominent examples are reinforcement learning from human feedback (RLHF) (Ope-nAI, 2023; <ref type="bibr" target="#b44">Touvron et al., 2023)</ref>, direct preference optimization (DPO) <ref type="bibr" target="#b33">(Rafailov et al., 2023)</ref>, and proximal policy optimization (PPO) <ref type="bibr" target="#b35">(Schulman et al., 2017;</ref><ref type="bibr" target="#b45">Tunstall et al., 2023)</ref>.</p><p>These techniques have been shown to be very useful, especially when used in conjunction with instruction tuning. However, similarly to instruction tuning, these methods focus on the overall quality of the response and its expected behavior and not necessarily on its breadth of knowledge.</p><p>Unsupervised Fine-Tuning</p><p>The final FT strategy we discuss is unsupervised, meaning there are no available labels for the model to learn from.</p><p>One common unsupervised FT technique is often referred to as continual pre-training or unstructured FT.</p><p>In this method, the FT process is viewed as a direct continuation of the pre-training phase. We start with a saved checkpoint of the original LLM and train it in a causal auto-regressive manner, i.e., predicting the next token. One major difference in comparison to actual pre-training is the learning rate. Usually, one would need a much lower learning rate when continuing the pre-training of the model to avoid catastrophic forgetting <ref type="bibr" target="#b17">(Kirkpatrick et al., 2017)</ref>.</p><p>It is well known that LLMs store vast amounts of knowledge during their pre-training phase <ref type="bibr" target="#b54">(Zhou et al., 2023)</ref>. So, it makes sense to continue this process in order to inject knowledge into the model. Hence, we use the unsupervised FT approach throughout this work and evaluate its efficacy in enhancing the model's capacity for learning new information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Retrieval Augmented Generation</head><p>Retrieval augmented generation (RAG) <ref type="bibr" target="#b20">(Lewis et al., 2020</ref>) is a technique that expands LLMs' capabilities, especially in knowledge-intensive tasks, by using external knowledge sources. While the original formulation involved additional training per task, it has since been demonstrated <ref type="bibr" target="#b26">(Neelakantan et al., 2022</ref>) that a pre-trained embedding model can achieve improved performance with no additional training involved.</p><p>The idea is that given an auxiliary knowledge base and an input query, we use the RAG architecture to find documents within the knowledge base that resemble the input query. These documents are then added to the input query, thus giving the model further context about the subject of the query.</p><p>In practice, implementing the suggested architecture is quite straightforward: Given an auxiliary knowledge base B Q and a pre-trained embedding model M e , we create a dense vector representation (embedding) per document b ∈ B Q and store these in a vector store. Upon receiving a new query q, we use its embedding, M e (q), to retrieve q's top-K closest neighbors, b q = {b k } K  <ref type="bibr" target="#b11">(Hendrycks et al., 2021)</ref> in the topics of anatomy, astronomy, college biology, college chemistry and prehistory. The chosen tasks were selected based on their emphasis on factual knowledge and the minimal reliance on reasoning.</p><p>As a heuristic, we opted for tasks where the questions are short and involve no context. In practice we selected four STEM subjects as well as one humanities subject, to ensure the evaluation is not limited to certain fields. Note that prehistory involves questions spanning all non-modern history. This approach aims to enable us to test LLM proficiency in comprehending and manipulating information in isolation from its reasoning processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current Events Task</head><p>To further isolate LLMs' abilities to learn new knowledge, we created a task comprising multiple-choice questions about current events. This task includes multiplechoice questions about events that occurred after the cutoff of the various models' training data. Specifically, we focused on "current events" from the USA, in the time span of August-November 2023, that are included in the relevant Wikipedia indexes<ref type="foot" target="#foot_1">foot_1</ref> . This method enables us to mostly guarantee that the models have not been exposed to these facts, thus allowing us to directly test knowledge injection capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Collection and Preprocessing</head><p>To effectively evaluate the LLMs' performance on these knowledge-intensive tasks, a comprehensive auxiliary dataset was collected by scraping relevant articles per topic from Wikipedia. The rationale behind selecting Wikipedia as the primary source of knowledge is its broad coverage of relevant topics and its reliability as a repository of crowd-verified knowledge. All articles pertinent to the tasks were retrieved via the official Wikipedia API 2 by identifying the relevant central page per topic.</p><p>Subsequently, a rigorous cleaning process was utilized to transform the data from raw subsections to clean chunks. This step was done with the "wikiextractor" tool <ref type="bibr" target="#b0">(Attardi, 2015)</ref>. The division into small, clean (e.g., remove HTML, URLs, etc.) chunks was aimed at enhancing the evaluation of the LLMs' understanding across various knowledge domains and aiding the LLMs in the fine-tuning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Current Events Task Creation</head><p>After collecting the relevant chunks from Wikipedia, we created a new multiple-choice dataset with the help of <ref type="bibr">GPT-4 (OpenAI, 2023)</ref>. First, we removed any small chunks. For each remaining chunk in the corpus, GPT-4 was instructed to create four highly specific, high-quality multiple-choice questions with only one correct answer. By specific, we mean that the question can be answered without knowledge of which context the question refers to and with minimal ambiguity. Next, GPT-4 was asked to select the two most specific of the four. This was followed by a manual evaluation and verification step. In total, this resulted in 910 new questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Paraphrases Generation</head><p>After creating the dataset, we utilized GPT-4 to generate augmentations of the dataset. We instructed GPT-4 to provide paraphrased versions of the input data that fully retain the information while being reworded. Each paraphrasing iteration was done with a different seed to ensure variety.</p><p>We selected 240 chunks at random for each task and created two paraphrases per chunk. These were set aside to be used as validation sets for hyperparameter tuning. For the current events dataset, we created ten paraphrases for each chunk used in the fine-tuning process described in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>Experimental Framework We used the popular LM-Evaluation-Harness <ref type="bibr" target="#b9">(Gao et al., 2021)</ref> repository to evaluate the performance of LLMs on the selected knowledge-intensive tasks. LM-Evaluation-Harness is a robust benchmarking tool that currently serves as the industry standard for model evaluation and is the basis of the HuggingFace leaderboard<ref type="foot" target="#foot_3">foot_3</ref> . Leveraging this platform ensured a standardized evaluation framework and allowed consistent comparison across models, methods, and datasets. More importantly, by using the industry standard for evaluation, we could avoid any differences stemming from prompt engineering and formatting issues and replicate the reported baseline results for each model.</p><p>Model Selection We chose three models for inference evaluation: Llama2-7B <ref type="bibr" target="#b44">(Touvron et al., 2023)</ref>, <ref type="bibr">Mistral-7B (Jiang et al., 2023)</ref>, and Orca2-7B <ref type="bibr" target="#b25">(Mitra et al., 2023)</ref>. The choice of these models was meant to represent the most popular opensource base models and an instruction-tuned model across various baseline capabilities. Additionally, we selected bge-large-en <ref type="bibr" target="#b51">(Xiao et al., 2023)</ref> as the embedding model for the RAG component and used FAISS <ref type="bibr" target="#b15">(Johnson et al., 2019)</ref> as its vectorstore. This embedding model is currently the SOTA of open-source embedding models, according to the HuggingFace MTEB leaderboard<ref type="foot" target="#foot_4">foot_4</ref> .</p><p>Configuration Variations Our evaluation included multiple configurations, with a grid-search  <ref type="formula" target="#formula_4">5</ref>)) for each knowledge-injection method, averaged (columnwise) across all experiments in Table 1.</p><p>over them, to allow for more comprehensive benchmarking. Firstly, we compared the baseline and fine-tuned models and their performance with the RAG component. Secondly, we explored the optimal number of text chunks to add to the context in RAG. Specifically, different values of K ∈ {0, . . . , 5} were employed to analyze the impact on model performance. Finally, we explored 5-shot performance vs. 0-shot.</p><p>Training Setup We trained all of the models using the unsupervised training procedure described in Section 3.2. For each dataset, we divided the auxiliary knowledge base into equal chunks of size 256 by concatenating or splitting the original chunks based on their length. We also added two special tokens, &lt;BOS&gt; and &lt;EOS&gt;, to demarcate the original chunks' beginnings and ends to preserve the documents' structure.</p><p>The models were trained using learning rates between 1 × 10 -6 and 5 × 10 -5 , which were found through a hyperparameter search. All models were trained on 4 NVIDIA A-100 GPUs for a maximum of 5 epochs and a batch size of 64.</p><p>Evaluation method All evaluations were done by appending each of the multiple-choice options to the question, followed by passing the concatenation through the model to get a log probability score per option. The highest score was interpreted as the model's choice and used for accuracy calculation. More formally, this means that in Equation (1) we say that M(q n ) = c n if:</p><formula xml:id="formula_3">c n = arg max l {M(q n ∥a 1 n ), . . . , M(q n ∥a L n )},<label>(4)</label></formula><p>where M(q n ∥a l n ) = log P M (q n ∥a l n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MMLU Results</head><p>For each task and model, we compared four approaches: using just the base model, RAG, FT, and finally combining FT and RAG by using the fine-tuned model as the generator. Furthermore, we tested the MMLU tasks using both 0-shot and 5-shot scenarios. The full results are shown in Table <ref type="table" target="#tab_0">1</ref>. An aggregation of the relative accuracy gain, i.e.,</p><formula xml:id="formula_4">(L M ′ ,Q -L M,Q )/L M,Q , (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where M is the base model and M ′ is the knowledge-injected model, is shown in Figure <ref type="figure" target="#fig_1">2</ref>. In all cases, RAG performed significantly better compared to the base models. Furthermore, using RAG with the base model as the generator was consistently better than only fine-tuning. In some cases, using the fine-tuned model instead of the base model as the generator in the RAG pipeline improved results even further. However, this is not consistent and thus demonstrates the inherent instability of fine-tuning. Additionally, we found that the 5-shot approach boosts the results by a small margin in most cases, with a similar trend being observed in all of the different approaches.</p><p>Current Events Results The evaluation on the current events task is shown in Table <ref type="table" target="#tab_1">2</ref>. RAG proves particularly effective due to the one-to-one correspondence between the questions and the auxiliary dataset (see Section 4.3). Fine-tuning is not competitive with RAG. However, fine-tuning with multiple paraphrases still provides a significant improvement over the baseline. We note that combining RAG with fine-tuning shows inferior performance compared to RAG alone.</p><p>It is worth noting that although the questions are based on information the models were not exposed to during training, the results of the base models surpass 1 L = 0.25. This can partially be explained by the models using reasoning and/or pre-existing knowledge when answering questions that are not independent of the past information. Some examples of this can be found in Appendix D.</p><p>Fine-Tuning vs. RAG: In the results of both the MMLU and current events tasks, a significant advantage for RAG over fine-tuning is evident. While fine-tuning improved results compared to the base model in most cases, it was not competitive with the RAG approach.</p><p>Several factors might contribute to this behavior. Firstly, RAG not only adds knowledge to a model but also incorporates context relevant to the question, a feature lacking in fine-tuning. Additionally, fine-tuning may impact other capabilities of the model due to a degree of catastrophic forgetting. Finally, it's plausible that unsupervised fine-tuned models might benefit from further alignment through supervised or RL-based fine-tuning, as evidenced by the vastly improved performance of Orca2 over the base Llama2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The Importance of Repetition</head><p>Unlike the other tasks, where the model has been exposed to aspects related to the topic during pretraining, current events includes new information. In this case, standard regular fine-tuning not only did not improve the performance of Llama2 but also significantly degraded it. To improve the finetuning results, we explored augmentation of the data using paraphrases.</p><p>Data Augmentation Data augmentation is a well-established method for enhancing the performance of language models and has been surveyed extensively <ref type="bibr" target="#b37">(Shorten et al., 2021)</ref>. Using generative models for augmentations has also been used successfully to improve classification models in the past <ref type="bibr" target="#b36">(Sharma et al., 2022)</ref>. An example of data augmentation using paraphrasing can be found in Appendix C.</p><p>Monotonic Improvement This approach resulted in notable improvements in our results, showcasing a direct correlation between the number of paraphrases utilized and the models' accuracy. Our experimentation revealed a compelling trend. For all models tested, the accuracy was a monotonically increasing function of the number of paraphrases used (visualized in Appendix A, Figure <ref type="figure" target="#fig_3">4</ref>). This observation strongly suggests the positive impact of paraphrase augmentation, yielding information repetition, on the model's ability to comprehend and generalize new knowledge from limited data.</p><p>Learning New Information In Appendix A, Figure <ref type="figure" target="#fig_2">3</ref>, we can see an interesting phenomenon observed throughout our experiments. After each epoch, i.e., completing another iteration over the entire dataset, the training loss drops significantly. This is consistent with what is known about LLMs memorizing the data during training and overfitting <ref type="bibr" target="#b43">(Tirumala et al., 2022)</ref>.</p><p>Our hypothesis is as follows:</p><p>In order to teach pre-trained LLMs new knowledge, the knowledge must be repeated in numerous ways. This is well known for LLM pre-training <ref type="bibr" target="#b16">(Kandpal et al., 2023)</ref>, and we see in this case that this holds for fine-tuning as well. The rationale for this hypothesis is that mere memorization of sentences does not entail knowledge of their content, as was already shown in <ref type="bibr" target="#b1">(Berglund et al., 2023)</ref>. By providing the information in numerous forms (like the data augmentation process we used), the various relationships in the data (e.g., a =⇒ b, b ̸ =⇒ c) stand a higher chance of appearing naturally. We believe this can potentially both increase L M,Q in general, as well as ameliorate Berglund et al.'s Reversal Curse. While promising, this result still warrants further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>Large language models possess vast amounts of knowledge on various topics. In this work, we tested their capability to adapt to new knowledge: both specialized and completely unseen. This is among the first studies to compare two prominent approaches in this domain, namely fine-tuning and retrieval augmented generation. While fine-tuning can be useful for many use-cases, we found that RAG is a more reliable choice for knowledge injection.</p><p>Some aspects of this work still warrant further research. For example, we focused on unsupervised training as our primary fine-tuning method, as opposed to instruction-tuning or RL-based methods. Researching combinations of various techniques, with diverse auxiliary knowledge bases, may yield improved results. This approach, combined with our hypothesis from Section 6, could further enhance our understanding of knowledge injection via FT.</p><p>While we believe that this work further enhances our understanding of knowledge in LLMs, there is a lot more work to be done in this field. Specifically, more research is required regarding the question of knowledge representation in LLMs, especially from a theoretical perspective.</p><p>Finally, further efforts are needed to measure knowledge in LLMs. While we employed an empirical approach as described in Equation (2), it is important to explore other definitions and perspectives on knowledge as well, and extend upon this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations</head><p>As in all machine learning applications, the choice of hyperparameters significantly impacts the results. We therefore strongly recommend optimiz-A The Importance of Repetition Figures  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B RAG Ablation Study</head><p>As mentioned in Section 5, we compared various values of K ∈ {0, . . . , 5}, shown in Table <ref type="table">3</ref>.We were unable to find an optimal value of K per model, per 0/5-shot, or per task. In fact, other than Anatomy that worked well with K = 2 consistently, there seems to be no patterns that aid in predicting the performance per K, unlike the results presented in <ref type="bibr" target="#b20">(Lewis et al., 2020)</ref> for other setups. Moreover, the gap between the best and worst performing Ks can be large.</p><p>Unfortunately, we must conclude that this additional hyperparameter is unstable. This is a downside of using RAG in practice, and the choice of K cannot be ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Paraphrase Examples</head><p>Below is the prompt we used to generate paraphrases with GPT-4: Your task is to paraphrase a text paragraph. The paragraph is given below. Make sure to keep the same meaning but change the wording. Do not change any factual information.</p><p>Try to keep roughly the same length of the original text.</p><p>Give NUM _ PARAPHRASES different paraphrases for each text. These paraphrases should be as different from each other as possible.</p><p>Return a JSON formatted string with one key, called 'paraphrases', and a list of paraphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input paragraph:</head><p>Below are some examples. First, an example taken from the following Wikipedia page: "51st International Emmy Awards".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase I</head><p>The fifty-first edition of the International Emmy Awards Competition accepted submissions across all categories from December 7, 2022, until the closing date of February 16, 2023.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase II</head><p>The International Emmy Awards Competition, now in its 51st year, accepted submissions across all categories starting from December 7, 2022, until the deadline on February 16, 2023.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase III</head><p>The fifty-first edition of the International Emmy Awards Competition accepted submissions across all categories from December 7, 2022, until the closing date of February 16, 2023.</p><p>The next example was taken from the following Wikipedia page: "2023 Indianapolis mayoral election".</p><p>Paraphrase I The mayoral election in Indianapolis took place on November 7, 2023, with preliminary elections occurring on May 2. The sitting mayor, Democrat Joe Hogsett, successfully ran for a third term. Both Hogsett and his Republican opponent, Jefferson Shreve, moved on to the main election.</p><p>Paraphrase II On November 7, 2023, citizens of Indianapolis cast their votes to elect their Mayor, following primary elections on May 2. Joe Hogsett, the Democrat already in office, won his bid for a third term. Hogsett and the Republican candidate, Jefferson Shreve, were the two contenders in the final electoral round.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase III</head><p>The mayoral election in Indianapolis took place on the 7th of November, 2023, following primary elections that occurred on the 2nd of May. Joe Hogsett, the incumbent Democrat, successfully ran for a third term. Both Hogsett and his Republican challenger, Jefferson Shreve, made it through to the final round of the election.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Current Events Existing Knowledge Examples</head><p>To give a better understanding of how a model might be able to answer questions about new information, with better than random success, we present three possible scenarios as examples. These scenarios show how models with stronger reasoning skills can infer the correct answer even for unseen information.</p><p>The first scenario involves questions about previously unseen information, where basic reasoning abilities allow a model to make an educated guess.</p><p>Question: What was a key issue that led to the 2023 United Auto Workers strike?</p><p>Answers:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A visualization of the knowledge injection framework.</figDesc><graphic coords="3,70.86,70.85,444.49,248.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The relative accuracy gain (as explained in Equation (5)) for each knowledge-injection method, averaged (columnwise) across all experiments in Table 1.</figDesc><graphic coords="7,70.86,70.85,218.27,129.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training loss over time for Mistral-7B.</figDesc><graphic coords="11,306.14,217.73,218.28,143.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model accuracy on the current events task as a function of the number of paraphrases.</figDesc><graphic coords="11,306.14,408.58,218.27,151.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results for the MMLU datasets described in Section 4.1 in terms of log-likelihood accuracy (Equation (4)).</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell cols="4">Base model Base model + RAG Fine-tuned Fine-tuned + RAG</cell></row><row><cell>Anatomy (0-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.556 0.393 0.607</cell><cell>0.681 0.489 0.637</cell><cell>0.570 0.430 0.600</cell><cell>0.659 0.489 0.637</cell></row><row><cell>Anatomy (5-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.600 0.467 0.570</cell><cell>0.681 0.563 0.659</cell><cell>0.622 0.496 0.593</cell><cell>0.674 0.548 0.674</cell></row><row><cell>Astronomy (0-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.625 0.401 0.645</cell><cell>0.678 0.467 0.750</cell><cell>0.651 0.487 0.651</cell><cell>0.697 0.520 0.750</cell></row><row><cell>Astronomy (5-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.658 0.401 0.664</cell><cell>0.724 0.474 0.763</cell><cell>0.651 0.447 0.664</cell><cell>0.697 0.520 0.743</cell></row><row><cell>College biology (0-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.681 0.438 0.583</cell><cell>0.757 0.493 0.639</cell><cell>0.701 0.458 0.604</cell><cell>0.764 0.465 0.632</cell></row><row><cell>College biology (5-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.722 0.451 0.604</cell><cell>0.778 0.521 0.660</cell><cell>0.736 0.424 0.625</cell><cell>0.771 0.479 0.653</cell></row><row><cell>College chemistry (0-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.470 0.310 0.370</cell><cell>0.500 0.380 0.440</cell><cell>0.490 0.390 0.370</cell><cell>0.500 0.390 0.390</cell></row><row><cell>College chemistry (5-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.470 0.370 0.430</cell><cell>0.540 0.380 0.470</cell><cell>0.500 0.360 0.370</cell><cell>0.500 0.390 0.380</cell></row><row><cell>Prehistory (0-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.713 0.448 0.642</cell><cell>0.750 0.481 0.679</cell><cell>0.719 0.457 0.673</cell><cell>0.731 0.478 0.673</cell></row><row><cell>Prehistory (5-shot)</cell><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.722 0.515 0.664</cell><cell>0.762 0.531 0.698</cell><cell>0.725 0.503 0.667</cell><cell>0.762 0.537 0.694</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Current events results. Models that were fine-tuned on the original dataset are labeled as FT-reg, while those trained on the dataset with multiple paraphrases are labeled as FT-par.</figDesc><table><row><cell cols="7">Base model Base model + RAG FT-reg FT-par FT-reg + RAG FT-par + RAG</cell></row><row><cell>Mistral 7B Llama2 7B Orca2 7B</cell><cell>0.481 0.353 0.456</cell><cell>0.875 0.585 0.876</cell><cell>0.504 0.219 0.511</cell><cell>0.588 0.392 0.566</cell><cell>0.810 0.326 0.820</cell><cell>0.830 0.520 0.826</cell></row><row><cell cols="2">4 Knowledge Base Creation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Task Selection and Rationale</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MMLU Benchmark To properly evaluate the capabilities of LLMs on knowledge-intensive tasks,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">we selected four distinct tasks from the Massively</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Multilingual Language Understanding Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(MMLU) benchmark</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>, according to dotproduct ranking. We then update q to be q = b q ∥q, where ∥ denotes string concatenation. Finally, we return M(q) as the model's output.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>https://en.wikipedia.org/wiki/Category:</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>2023_events_in_the_United_States_by_month2 https://www.mediawiki.org/wiki/API:Main_page</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>https://huggingface.co/spaces/HuggingFaceH4/ open_llm_leaderboard</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://huggingface.co/spaces/mteb/ leaderboard</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ing all relevant hyperparameters for specific cases. We have supported our claims by running the experiments on three different models. However, generalization to other LLMs should be tested thoroughly. For example, GPT-4 achieves near perfect accuracy for some MMLU tasks <ref type="bibr" target="#b28">(Nori et al., 2023)</ref>, and thus further improvement is not applicable.</p><p>Finally, while we chose various topics for the knowledge bases, all of our sources came from <rs type="person">Wikipedia. Other</rs> datasets may yield different results, and must be evaluated carefully.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this case it is easy to guess that the third option is the most likely, even without knowledge of this specific strike.</p><p>A second scenario involves questions where prior knowledge about a topic may aid a model in answering.</p><p>Question: What environmental concern was raised by some scientists as a result of the 2023 Hawaii wildfires?</p><p>Answers:</p><p>1. Rising temperatures. 2. Melting ice caps.</p><p>3. Charred soils running off into the shoreline. 4. Increased air pollution.</p><p>In this case, knowing the geography of Hawaii, as well as immediate effects of wildfires, enables a model to give the first two options a lower likelihood. This process of elimination increases the probability of choosing one of the remaining options (the third option is the correct answer).</p><p>A third scenario arises due to the automatic question generation process, some questions strongly rely on pre-existing knowledge.</p><p>Question: What event in 2021 was compared to the September 2023 New York floods?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answers:</head><p>1. Hurricane Katrina. 2. Hurricane Ida. 3. Hurricane Sandy. 4. Hurricane Harvey.</p><p>Since only one of these events occurred in 2021 (Hurricane Ida), and all the models tested have been exposed to events from 2021 during pretraining, this question can potentially be answered without using additional current information.</p><p>Finally, to demonstrate why it is reasonable to assume that models cannot generally answer questions about new information, with better than random success, look at the following example:</p><p>Question: How did Matthew Belk, a National Weather Service meteorologist, describe the September 2023 northeastern U.S. floods?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Answers:</head><p>1. 50-year event. 2. 100-year event.</p><p>3. 200-year event. 4. 500-year event.</p><p>Even with some knowledge about floods and their statistical properties, it would be very difficult to guess that this specific meteorologist would call the flood a '200-year event'. This is especially true if the model was not exposed to information about the details of the flood.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Giusepppe</forename><surname>Attardi</surname></persName>
		</author>
		<ptr target="https://github.com/attardi/wikiextractor" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The reversal curse: Llms trained on&quot; a is b&quot; fail to learn</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meg</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikita</forename><surname>Balesni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><forename type="middle">Cooper</forename><surname>Stickland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Korbak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.12288</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Recall and learn: Fine-tuning deep pretrained language models with less forgetting</title>
		<author>
			<persName><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangzhan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12651</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowprompt: Knowledgeaware prompt-tuning with synergistic optimization for relation extraction</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web conference 2022</title>
		<meeting>the ACM Web conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2778" to="2788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Yanda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07814</idno>
		<title level="m">Meta-learning via language model in-context tuning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Ken</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><surname>Hong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.04757</idno>
		<title level="m">Lidong Bing, and Soujanya Poria. 2023. Instructeval: Towards holistic evaluation of instruction-tuned large language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.11416</idno>
		<title level="m">Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Crawling the internal knowledge-base of language models</title>
		<author>
			<persName><forename type="first">Roi</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12810</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00161</idno>
		<title level="m">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5371628</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An empirical investigation of catastrophic forgetting in gradient-based neural networks</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6211</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey of knowledge enhanced pre-trained language models</title>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Quzhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenwei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15062</idno>
		<title level="m">Lawyer llama technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Albert Q Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>De Las Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianna</forename><surname>Bressand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lengyel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><surname>Saulnier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.06825</idno>
		<title level="m">Mistral 7b</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large language models struggle to learn long-tail knowledge</title>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haikang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15696" to="15707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Ishita</forename><surname>Andrew K Lampinen</surname></persName>
		</author>
		<author>
			<persName><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kory</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">Henry</forename><surname>Matthewson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Tessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">L</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><forename type="middle">X</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02329</idno>
		<title level="m">Can language models learn from explanations in context? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Rozanov</surname></persName>
		</author>
		<author>
			<persName><surname>Glavaš</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11787</idno>
		<title level="m">Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An empirical study of catastrophic forgetting in large language models during continual fine-tuning</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.08747</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15943</idno>
		<title level="m">Metaicl: Learning to learn in context</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cross-task generalization via natural language crowdsourcing instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08773</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Del Corro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweti</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Codas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clarisse</forename><surname>Simoes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Razdaibiedina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kriti</forename><surname>Aggarwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.11045</idno>
	</analytic>
	<monogr>
		<title level="m">Teaching small language models how to reason</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Michael Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolas</forename><forename type="middle">A</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Heidecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyna</forename><forename type="middle">Eloundou</forename><surname>Nekoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Schnurr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenny</forename><surname>Sai-Kin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tabarak</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toki</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanne</forename><surname>Sherbakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilian</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><surname>Weng</surname></persName>
		</author>
		<idno>ArXiv, abs/2201.10005</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Text and code embeddings by contrastive pre-training</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ha-Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05729</idno>
		<title level="m">A brief report on lawgpt 1.0: A virtual legal assistant based on gpt-3</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Capabilities of gpt-4 on medical challenge problems</title>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">Mayer</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<idno>ArXiv, abs/2303.13375</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno>ArXiv, abs/2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01066</idno>
		<title level="m">Language models as knowledge bases? arXiv preprint</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Rafailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Archit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.18290</idno>
		<title level="m">Direct preference optimization: Your language model is secretly a reward model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Winogrande: An adversarial winograd schema challenge at scale. Communications of the</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Systematic review of effect of data augmentation using paraphrasing on named entity recognition</title>
		<author>
			<persName><forename type="first">Saket</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviral</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namrata</forename><surname>Mukhija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanoz</forename><surname>Bhathena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Sashank Santhanam, and Pritam Biswas</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Text data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borko</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><surname>Furht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Tanwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="issue">7972</biblScope>
			<biblScope unit="page" from="172" to="180" />
			<pubPlace>Heather Cole-Lewis, Stephen Pfohl</pubPlace>
		</imprint>
	</monogr>
	<note>et al. 2023a</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">2023b. Towards expert-level medical question answering with large language models</title>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juraj</forename><surname>Gottweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellery</forename><surname>Wulczyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heather</forename><surname>Cole-Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darlene</forename><surname>Neal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.09617</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</title>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Adam R Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrià</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Garriga-Alonso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04615</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Can chatgpt replace traditional kbqa models? an in-depth analysis of the question answering performance of the gpt llm family</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehai</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="348" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://crfm.stanford.edu/2023/03/13/alpaca.html" />
		<title level="m">Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Memorization without overfitting: Analyzing the training dynamics of large language models</title>
		<author>
			<persName><forename type="first">Kushal</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><forename type="middle">H</forename><surname>Markosyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<idno>ArXiv, abs/2205.10770</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
	</analytic>
	<monogr>
		<title level="m">Open foundation and fine-tuned chat models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Beeching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Younes</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clémentine</forename><surname>Fourrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Habib</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.16944</idno>
		<title level="m">Direct distillation of lm alignment</title>
		<meeting><address><addrLine>Zephyr</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Jiayang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07521</idno>
		<title level="m">Survey on factuality in large language models: Knowledge, retrieval and domain-specificity</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">K-adapter: Infusing knowledge into pre-trained models with adapters</title>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01808</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjana</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arut</forename><surname>Selvan Dhanasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atharva</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Stap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07705</idno>
		<title level="m">Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14454</idno>
		<title level="m">Pmc-llama: Further finetuning llama on medical papers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vadim</forename><surname>Dabravolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhanjan</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17564</idno>
		<title level="m">Bloomberggpt: A large language model for finance</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">C-pack: Packaged resources to advance general chinese embedding</title>
		<author>
			<persName><forename type="first">Shitao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peitian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">Dan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.06031</idno>
		<title level="m">Fingpt: Open-source financial large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A survey of knowledge-enhanced text generation</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaitang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11206</idno>
		<title level="m">Less is more for alignment</title>
		<meeting><address><addrLine>Lima</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
