<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</title>
				<funder ref="#_37mUTuW #_YcPV7Xz #_4GBNTur">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_4tTEDm2">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxi</forename><surname>Cui</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">PandaVilla Tech Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Munang</forename><surname>Ning</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<orgName type="institution">Peking University Shenzhen Graduate School</orgName>
								<address>
									<addrLine>3 AI for Science (AI4S)-Preferred Program</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<orgName type="institution">Peking University Shenzhen Graduate School</orgName>
								<address>
									<addrLine>3 AI for Science (AI4S)-Preferred Program</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Graduate School</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<orgName type="institution">Peking University Shenzhen Graduate School</orgName>
								<address>
									<addrLine>3 AI for Science (AI4S)-Preferred Program</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0D574024C859A357CB5883C9FCD51EB3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-24T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. As a result, Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, LLMs have gained rapid popularity in the AI community, such as GPT-3.5, <ref type="bibr">GPT-4 (OpenAI, 2023)</ref>, PaLM <ref type="bibr" target="#b3">(Bi et al., 2020;</ref><ref type="bibr" target="#b1">Anil et al., 2023), and</ref><ref type="bibr">BLOOM (Scao et al., 2022)</ref>. They rely on their powerful language comprehension abilities to follow human-provided instructions and provide corresponding responses. Typically, LLMs can only respond within the text input provided by the user, which is insufficient because human interaction with the world involves multiple channels, such as visual and textual. To this end, recent Video-LLaVA aligns images and videos before projection, allowing LLM to learn from a unified visual representation and endowing LLM with the ability to comprehend both images and videos simultaneously.</p><p>works <ref type="bibr" target="#b51">(Ye et al., 2023;</ref><ref type="bibr">Zhu et al., 2023b;</ref><ref type="bibr" target="#b0">Alayrac et al., 2022)</ref> have mapped images into text-like tokens, enabling LLMs to emerge with the ability to comprehend images. Despite their effectiveness, empowering LLMs to understand videos is more challenging than image-only comprehension tasks. Nevertheless, recent work <ref type="bibr" target="#b37">(Maaz et al., 2023;</ref><ref type="bibr">Li et al., 2023c;</ref><ref type="bibr">Zhang et al., 2023a)</ref> has made initial strides in enabling interactions between video and language.</p><p>However, most current LVLMs <ref type="bibr">(Li et al., 2023b;</ref><ref type="bibr" target="#b8">Dai et al., 2023;</ref><ref type="bibr" target="#b35">Luo et al., 2023;</ref><ref type="bibr">Li et al., 2023a;</ref><ref type="bibr" target="#b52">Yin et al., 2023;</ref><ref type="bibr" target="#b9">Fu et al., 2023)</ref> can primarily handle a single visual modality, either imagelanguage or video-language. We compare different LVLM paradigms as shown in Figure <ref type="figure" target="#fig_0">1</ref>, where VideoChat <ref type="bibr">(Li et al., 2023c)</ref> and Video-LLaMA <ref type="bibr">(Zhang et al., 2023a)</ref> utilize a share visual encoder to handle both images and videos. However, due to the inherent differences in the media types of images and videos, it is challenging to learn a unified representation, and the performance falls significantly behind that of the specialized video expert model, Video-ChatGPT. Therefore, X-LLM <ref type="bibr" target="#b6">(Chen et al., 2023)</ref> and Macaw-LLM <ref type="bibr" target="#b36">(Lyu et al., 2023)</ref> allocate a modality-specific encoder for each modality, attempting to enable a LLM to comprehend images or videos through several projection layers. But their performances are inferior to dedicated video expert models such as Video-ChatGPT <ref type="bibr" target="#b37">(Maaz et al., 2023)</ref>. We attribute this phenomenon to the lack of alignment before projection. Because image features and video features reside in their own spaces, this poses a challenge for a LLM to learn their interactions from several poor projection layers. Some similar phenomenon such as alignment before fusion has been discussed by ALBEF <ref type="bibr" target="#b27">(Li et al., 2021)</ref> and ViLT <ref type="bibr" target="#b22">(Kim et al., 2021)</ref> in multi-model models. More recently, ImageBind-LLM <ref type="bibr" target="#b15">(Han et al., 2023)</ref> focuses on enabling the LLM to simultaneously process multiple modal inputs by pre-aligning each modality to a common feature space <ref type="bibr" target="#b11">(Girdhar et al., 2023)</ref>. Based on a large image-language model, ImageBind-LLM converts other modalities into the most similar image features by retrieving from a training-free image cached database. However, the indirect alignment approach of ImageBind-LLM may lead to performance degradation, and the LLM has no knowledge of actual video data.</p><p>In this work, we introduce Video-LLaVA, a simple but powerful baseline for the LVLM simultaneously handling both images and videos. Specifically, As shown in Figure <ref type="figure" target="#fig_0">1</ref>, Video-LLaVA initially aligns the representations of images and videos to a unified visual feature space. Since the visual representations are already aligned prior to projection, we employ a shared projection layer to map the unified visual representation for the LLM. To enhance computational efficiency, Video-LLaVA undergoes joint training of images and videos, achieving remarkable results with 1 training epoch.</p><p>As a result, The proposed Video-LLaVA greatly enhances the ability of the LLM to simultaneously understand both images and videos. For image understanding, Video-LLaVA surpasses advanced LVLMs such as mPLUG-owl-7B and InstructBLIP-7B in 5 image benchmarks. Additionally, utilizing 4 benchmark toolkits for a more comprehensive evaluation, Video-LLaVA-7B even outperforms IDEFICS-80B by 6.4% in MMBench. Moreover, similar trends can be observed in video understanding, where Video-LLaVA surpasses Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% respectively on the MSVD, MSRVTT, TGIF, and ActivityNet video question-answering datasets. Ex-tensive ablation experiments demonstrate that alignment before projection yields greater benefits. Additionally, joint training of images and videos can facilitate a unified visual representation in LLM comprehension.</p><p>We summarize our primary contributions as follows:</p><p>• We introduce Video-LLaVA, a powerful LVLM baseline. During the training process, Video-LLaVA binds visual signals to the language feature space, unifying visual representations, and proposes a solution to align before projection. We enable an LLM to perform visual reasoning capabilities on both images and videos simultaneously.</p><p>• Extensive experiments demonstrate that a unified visual representation benefits LLMs in learning to simultaneously handle both images and videos, validating the complementarity of modalities, showcasing significant superiority when compared to models specifically designed for either images or videos.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Large Language Models</head><p>When the well-known commercial model Chat-GPT (OpenAI, 2023) was introduced, the The AI community released open-source Large Language Models (LLMs) by instruction tuning and increasing model sizes. These include LLaMA <ref type="bibr">(Touvron et al., 2023a)</ref>, Vicuna <ref type="bibr" target="#b7">(Chiang et al., 2023)</ref>, Alpaca <ref type="bibr" target="#b45">(Taori et al., 2023)</ref>, and more recently, LLaMA 2 <ref type="bibr">(Touvron et al., 2023b)</ref>. These models are tuned with instruction sets to emulate conversations between humans and AI assistants. Furthermore, InstructGPT <ref type="bibr" target="#b39">(Ouyang et al., 2022)</ref> is trained based on GPT-3 <ref type="bibr" target="#b4">(Brown et al., 2020)</ref> with 175 billion parameters through aligning with human preferences. However, LLMs can only interact within text. In this work, we introduce Video-LLaVA, which builds upon the powerful reasoning capabilities of LLM to extend modality interactions to images and videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Large Vision-Language Models</head><p>When extending LLMs to multi-modal, especially involving images and videos, the main approaches can be categorized into two types in Table <ref type="table">1</ref>: i) treating LLM as a scheduler, ii) treating LLM as a decoder.</p><p>Table 1: Comparison between different Large Vision-Language Models. For methods that treat LLMs as scheduler, they do not require pre-alignment and joint training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Video Pre-aligned Joint training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLMs as scheduler</head><p>VisualChatGPT <ref type="bibr" target="#b48">(Wu et al., 2023</ref>) ✔ ✗ --HuggingGPT <ref type="bibr" target="#b42">(Shen et al., 2023)</ref> ✔ ✗ --MM-REACT <ref type="bibr" target="#b50">(Yang et al., 2023)</ref> ✔ ✔ --ViperGPT <ref type="bibr" target="#b44">(Surís et al., 2023)</ref> ✔ ✔ --</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLMs as decoder</head><p>Mini-GPT4 <ref type="bibr">(Zhu et al., 2023b</ref>)</p><formula xml:id="formula_0">✔ ✗ - ✗ LLaVA (Liu et al., 2023b) ✔ ✗ - ✗ Video-ChatGPT (Maaz et al., 2023) ✗ ✔ - ✗ VideoChat (Li et al., 2023c) ✔ ✔ ✗ ✔ Video-LLaMA (Zhang et al., 2023a) ✔ ✔ ✗ ✔ ImageBind-LLM (Han et al., 2023) ✔ ✔ ✔ ✗ Video-LLaVA (Ours) ✔ ✔ ✔ ✔</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">LLMs as scheduler</head><p>In the scheduler-based methods, various visual models are treated as plug-and-play modules. LLM schedules them according to the specific visual task requirements, like the assembly of building blocks. Some of these methods focus on images, such as VisualChatGPT <ref type="bibr" target="#b48">(Wu et al., 2023)</ref> and Hugging-GPT <ref type="bibr" target="#b42">(Shen et al., 2023)</ref>, while MM-REACT <ref type="bibr" target="#b50">(Yang et al., 2023)</ref> and ViperGPT <ref type="bibr" target="#b44">(Surís et al., 2023)</ref> can also handle videos. A key characteristic of these scheduler-based LVLMs is that they do not require end-to-end training, hence eliminating the need for pre-alignment and joint training of each modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">LLMs as decoder</head><p>Regarding the approach of treating LLM as a decoder, this is our primary focus. <ref type="bibr">MiniGPT-4 (Zhu et al., 2023b)</ref> aligns image tokens to the input of the large language model through several linear projection layers. However, this alignment is weak and lacks feedback from human instructions. Subsequently, mPLUG-Owl <ref type="bibr" target="#b51">(Ye et al., 2023)</ref> adopts a two-stage training approach. In the first stage, images are aligned with language using an autoregressive pretraining style, and the second stage involves instruction tuning through using a human instruction dataset. With the increasing scale of large language model backends, approaches such as InstructBLIP <ref type="bibr" target="#b8">(Dai et al., 2023)</ref> and LLaVA series <ref type="bibr">(Liu et al., 2023b,a;</ref><ref type="bibr" target="#b30">Lin et al., 2024)</ref> collecte the larger human instruction datasets to train a larger LVLMs (13B parameters). Each answer of instruction datasets strictly follow to the given instructions. Then they undergo end-to-end training using human instruction datasets, enabling the LLM with visual reasoning capabilities. Moreover, Video-ChatGPT <ref type="bibr" target="#b37">(Maaz et al., 2023)</ref> design a 100k video instruction dataset, successfully empowering LLMs to comprehend videos. VideoChat <ref type="bibr">(Li et al., 2023c)</ref> and Video-LLaMA <ref type="bibr">(Zhang et al., 2023a)</ref> achieve this by conducting joint training, allowing LLMs to simultaneously handle images and videos. Expanding LLMs to additional visual modalities typically requires pre-alignment, as seen in LLaMA-Adapter <ref type="bibr">(Zhang et al., 2023b;</ref><ref type="bibr" target="#b10">Gao et al., 2023)</ref> and ImageBind-LLM <ref type="bibr" target="#b15">(Han et al., 2023)</ref>. They bind other modalities to the image space through ImageBind's <ref type="bibr" target="#b11">(Girdhar et al., 2023)</ref> modality encoder. These models have demonstrated that a unified feature space is advantageous for enhancing LLM's multi-modal reasoning capabilities. Distinguished from prior work, Video-LLaVA not only pre-aligns image and video features but also conducts joint training of images and videos, facilitating LLMs in learning multi-modal reasoning capabilities from a unified visual representation. 3 Video-LLaVA 3.1 Model Structure 3.1.1 Framework Overview As shown in Figure 2, Video-LLaVA consists of LanguageBind encoders f V (Zhu et al., 2023a) to extract features from the raw visual signal (im-ages or videos), a large language model f L such as Vicuna, visual projection layers f P and a word embedding layer f T . We initially obtain visual features using LanguageBind encoders. Language-Bind encoders are capable of mapping different modalities into the textual feature space, thereby providing us with a unified visual representation. Subsequently, the unified visual representation is encoded by shared projection layers, which is then combined with tokenized textual queries and fed into a large language model to generate corresponding responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">United Visual Representation</head><p>Our goal is to map images and videos into a shared feature space to enable the large language model to learn from a unified visual representation. We assume that the same information can be conveyed through multiple media. For example, a running dog can be expressed through language, a image or a video simultaneously. Therefore, we can compress information from different modalities into a common feature space, allowing the model to extract information from a dense feature space, facilitating modality interactions and complementarity. Hence, we chose the modality encoders from Lan-guageBind <ref type="bibr">(Zhu et al., 2023a)</ref>, which align images and videos with the textual feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Alignment Before Projection</head><p>Specifically, LanguageBind initializes from Open-CLIP <ref type="bibr" target="#b19">(Ilharco et al., 2021)</ref>, naturally aligning images and language in a shared feature space. Subsequently, it aligns video representations to the language space using 3 million video-text pairs from VIDAL-10M <ref type="bibr">(Zhu et al., 2023a)</ref>. By sharing a language feature space, the image and video representations ultimately converge into a unified visual feature space, which we refer to as emergent alignment of images and videos. Therefore, our video encoder and image encoder are initialized from the LanguageBind encoders zoo, pre-aligning the inputs for LLM and reducing the gap between representations of different visual signals. The unified visual representation is fed into LLM after passing through a shared projection layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Pipeline</head><p>Overall, the process of generating responses by Video-LLaVA is similar to that of a large language model (GPT series). Given a textual input X T and visual signals X V , the input signals are encoded into a sequence of tokens according to Equation <ref type="formula">1</ref>. By maximizing the likelihood probability in Equation 2, the model ultimately achieves multi-modal understanding capabilities.</p><formula xml:id="formula_1">Z T = f T (X T ) , Z V = f P (f V (X V )) (1) p (X A | X V , X T ) = L i=1 p θ X [i] A | Z V , Z [1:i-1] T</formula><p>(2) where L is the length of the generated sequence X A , and θ is a trainable parameter. We dynamically conduct joint training on images and videos, wherein a single batch contains both image and video samples simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Understanding Training</head><p>At this stage, the model is required to acquire the ability to interpret visual signals within an extensive image/video-text pair dataset. Each visual signal corresponds to a single round of conversation data (X q , X a ), where X T = X q and X a is the ground truth. The training objective of this stage is the original auto-regressive loss, where the model learns the basic ability to view the vision. We freeze the other parameters of the model during this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Instruction Tuning</head><p>In this stage, the model is required to provide responses corresponding to different instructions. These instructions often involve more complex visual comprehension tasks, rather than just describing visual signals. Note that the conversation data</p><formula xml:id="formula_2">X 1 q , X 1 a , • • • , X N q , X N a consists of multiple rounds. X r T = X 1 q , r = 1 Concat(X r-1 q , X r-1 A , X r q ), r &gt; 1</formula><p>(3) where r represents the round number. As shown in Equation 3, when r &gt; 1 we concatenate the conversations from all previous rounds with the current instruction as the input for this round. The training objective remains the same as in the previous stage. After this stage, the model learns to generate corresponding responses based on different instructions and requests. The LLM are also involved in training at this stage.  LAION-CC-SBU 558k Valley 702k LLaVA-mixed 665k Video-ChatGPT 100k Stage 1: Understanding Pretraining Stage 2: Instruction Tuning concise caption multi-turn conversations / detailed caption / reasoning Figure 3: Data composition for training Video-LLaVA. The dataset for stage 1 consists of single-turn conversation, focusing on concise visual descriptions. In stage 2, the dataset comprises multi-turn conversations, emphasizing complex visual reasoning abilities. 4 Experiments 4.1 Experimental Setup 4.1.1 Data Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large Language Model</head><p>In 3, for the first stage of understanding pretraining, we use a subset of 558K LAION-CC-SBU image-text pairs with BLIP <ref type="bibr" target="#b26">(Li et al., 2022)</ref> captions, which is sourced from CC3M <ref type="bibr" target="#b41">(Sharma et al., 2018)</ref> and filtered by LLaVA <ref type="bibr">(Liu et al., 2023b)</ref>.</p><p>The video-text pairs are derived from a subset provided by Valley <ref type="bibr" target="#b35">(Luo et al., 2023)</ref>, and we have access to 702k out of a total of 703k pairs, originating from WebVid <ref type="bibr" target="#b2">(Bain et al., 2021)</ref>. For the stage of instruction tuning, We gathered instructional datasets from two sources, including a 665k imagetext instruction dataset from LLaVA 1.5 <ref type="bibr">(Liu et al., 2023a</ref>) and a 100k video-text instruction dataset from Video-ChatGPT <ref type="bibr" target="#b37">(Maaz et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Model Settings</head><p>We employ Vicuna-7B v1.5 as the large language model. The visual encoders are derived from Lan-guageBind, initialized from OpenCLIP-L/14. The text tokenizer is sourced from LLaMA, with approximately 32,000 classes. The share projection layers consist of 2 fully connected layers with a GeLU (Hendrycks and Gimpel, 2016) activated function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Training Details</head><p>In the training process, we resize and crop each image, resulting in a size of 224×224 for each processed image. We uniformly sample 8 frames from each video, and each frame undergoes image preprocessing. The data in each batch is a random combination of images and videos. In the first stage, we train for one epoch with a batch size of 256, using the AdamW optimizer with a cosine learning rate schedule. In the second stage, we reduce the batch size to 128. The initial learning rate for both stages is set to 1e-3, with a warmup ratio of 0.03. Additional hyper-parameter settings can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Zero-shot Video Understanding</head><p>As shown in Table <ref type="table" target="#tab_2">2</ref>, we conduct a quantitative assessment of the video question-answering ca-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Object Hallucination Evaluation</head><p>As shown in Table <ref type="table" target="#tab_4">4</ref>, we report evaluation results for zero-shot object hallucinations, utilizing a evaluation pipeline derived from a polling-based query method <ref type="bibr">(Li et al., 2023d)</ref>. Video-LLaVA demon-strates competitive performance across three subsets: random, popular, and adversarial. Specifically, when compared to the 7B foundation model, Video-LLaVA consistently outperforms MM-GPT <ref type="bibr" target="#b12">(Gong et al., 2023)</ref> across all three POPE hallucination evaluation subsets. Furthermore, when benchmarked against the larger 13B LLM, Video-LLaVA even surpasses Mini-GPT4 comprehensively. The successful performance of Video-LLaVA in object hallucination detection validates the consistency between unified visual representations and the generation of textual descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Alignment Before Projection</head><p>To validate the performance degradation caused by separated visual representation, we conduct experiments to to explore the performance of the LLM learning from different visual representations. We define the use of LanguageBind image encoder as unified visual representation while the MAE encoder <ref type="bibr" target="#b16">(He et al., 2022)</ref> use separated visual representation, which is a well-known and effective image feature extractor. Additionally, since MAE do not interact with multi-modal inputs during the training process, we utilize CLIP-L/14, a model of the same size. While CLIP-L/14 exhibits strong multimodal understanding capabilities, it is not prealigned with the video encoder. Consequently, this results in a lack of uniformity in the visual features provided to LLM. We only replace the image encoder of the same scale and keep the LanguageBind video encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">For Video Understanding</head><p>Due to replacing the image encoder with the MAE encoder, the video features and image features are  no longer unified during LLM's initial learning of visual representations. In Figure <ref type="figure" target="#fig_3">4</ref>, compared to separated visual representation, the united visual representation significantly improves performance across 4 video question-answering datasets. Separated visual representations not only exhibit lower accuracy in question-answering, but also demonstrate a similar trend in answer scores. These results demonstrate that the unified visual representation can help the LLM further learn and understand videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">For Image Understanding</head><p>The unified visual representation demonstrates strong performance, surpassing the separated visual representation comprehensively across 5 image question-answering datasets and 4 benchmark toolkits in Table <ref type="table" target="#tab_5">5</ref>. Additionally, we observe a significant margin of performance improvement in the unified visual representation on the MM- Bench, LLaVA-Bench, and MM-Vet benchmark toolkits. This highlights that the unified visual representation not only enhances performance in image question-answering but also provides benefits in other aspects of image understanding, such as reducing object hallucination and improving OCR capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Joint Training</head><p>This subsection aims to validate the complementarity of images and videos during joint training, which can mutually enhance the LLM's understanding of images and videos based on a unified visual representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">For Video Understanding</head><p>For comparing performance on video benchmarks, we remove image data during the training of Video-LLaVA, which is called Video-LLaVA * . We com-pare with Video-LLaVA * to assess the performance gains from joint image training on video benchmarks. In Table <ref type="table" target="#tab_6">6</ref>, we evaluate our model on four video question-answering datasets. Compared to Video-LLaVA * without image in training, the model trained with joint images and videos achieves comprehensive improvements across all four video datasets. These results demonstrate that joint training of images and videos facilitates LLM's understanding of visual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">For Image Understanding</head><p>When comparing performance on image benchmarks, it is challenging to find a image-based LVLM with the same configuration as Video-LLaVA. To address this, we replace the image encoder in LLaVA-1.5 with the LanguageBind-Image encoder and reproduce the results at a resolution of 224×224 by using the same training configuration, called LLaVA-1.5 † . As shown in Figure <ref type="figure" target="#fig_4">5</ref>, Compared to LLaVA-1.5 † , which utilizes the same image encoder configuration, we observe performance improvements in 8 out of 9 benchmarks, demonstrating mutual improvement in visual understanding. Video-LLaVA outperform LLaVA-1.5 † in POPE, indicating that joint training with videos alleviates the object hallucination in images. The similar trend is observed on some other benchmark toolkits, such as LLaVA-Bench and MMBench, where video data significantly improves LLM's performance in complex reasoning and image conversation tasks.</p><p>5 Limitation and Future Directions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Limitation</head><p>While Video-LLaVA exhibits strong competitiveness in both images and videos, we still observed some limitations of Video-LLaVA. To begin with, Video-LLaVA performs moderately in understanding long videos. In Table <ref type="table" target="#tab_2">2</ref>, Chat-UniVi surpasses 0.5 on ActivityNet-QA because Video-LLaVA only utilizes uniformly sampled 8 frames to comprehend the video, which results in the loss of detailed information from long videos. Additionally, training Video-LLaVA is computationally expensive, requiring 3-4 days to complete the training process on 8 A100-80G GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Future Directions</head><p>In the future, We maybe can explore more efficient shared projection mode that can compress tokens while preserving data features. This would support Video-LLaVA in better understanding long videos.</p><p>Besides, Video-LLaVA can serve as a baseline to extend to additional visual-related modalities, such as depth and infrared images. Additionally, we could explore how to incorporate timestamp embeddings effectively, enabling large visual-language models to answer questions related to temporal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we introduce Video-LLaVA, a simple but powerful large visual-language baseline model.</p><p>We propose a novel framework to address the issue of misalignment before projection, utilizing a LanguageBind encoder to pre-bind visual signals into the language feature space. To enable a LLM to comprehend both images and videos simultaneously, we conduct joint training on images and videos, allowing the LLM to learn multi-modal interactions from a unified visual representation.</p><p>Extensive experiments demonstrate that joint training on images and videos mutually benefits performance. Furthermore, we validate that aligning visual representations before projection aids LLM learning. Remarkably, LLM, after learning from a unified visual representation, exhibits the remarkable ability to simultaneously engage with both images and videos, showcasing a powerful comprehension of unified visual concepts. These results collectively demonstrate the effectiveness of the Video-LLaVA training framework. As a unified visual training framework, the performance of Video-LLaVA even surpasses that of expert models designed specifically for images or videos.</p><p>The video shows a man playing the dru his hands and sticks, and he continues t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction] Describe the activ</head><p>The video shows a young girl performing a gymnastics routine on a balance beam. She executes several flips and tricks before dismounting and landing on a mat.</p><p>[Input Instruction] Describe the video.</p><p>The video features a woman grooming hair and cleans its hooves, and the hors The video also includes shots of the wo</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction] Briefly describe th</head><p>The main challenge faced by the people seen struggling to stay upright and mai forth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction] What is the main c</head><p>The video is about a man who is seen spinning and throwing a discus in a field. He then walks away while people measure his throw.</p><p>[Input Instruction] What is the video about?</p><p>Sure! The man in the video demonstrat down with a rag. He then applies a clea again. Finally, he shows how to dry the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction] Can you descr window as demonstrated in the video</head><p>The woman and the dog are playing frisbee in the yard. The woman throws the frisbee, and the dog chases after it.</p><p>[Input Instruction] What activities are the woman and the dog engaged in the video?</p><p>The video shows a person cooking a dish of noodles. They start by boiling water in a pot and adding noodles to it. Then, they add oil and some ingredients to the pot. After that, they mix the noodles and the ingredients together. Finally, they serve the dish on a plate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction]</head><p>Step-by-step describe the cooking recipe shown in the video.</p><p>The video shows two men arm wrestling on a table while being watched by a group of people.</p><p>[Input Instruction] What is the video about?</p><p>[Input Instruction] Is there a flag in the background?</p><p>Yes, there is a flag in the background of the video.</p><p>The video is taken from the top of the S New York City. The statue is shown fro from the top of the statue itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction]</head><p>Where is this vid is shown in the video? The video is strange because it shows a man sitting next to a lion and a next to a man. The man is also seen petting a lion in the grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction]</head><p>What is strange about this video?</p><p>The video is funny because the baby is playing with a Wii remote, which is not a toy typically associated with babies. The baby's actions are also humorous, such as pointing to the camera and making faces, which adds to the overall amusement of the video.</p><p>[Input Instruction] Why is this video funny?</p><p>The unusual aspect of this video is that the giraffe is seen swimming in not a common sight. The giraffe is also seen jumping off a diving boar underwater, which adds to the uniqueness of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction]</head><p>What is unusual about this video?</p><p>Yes, the baby is playing with a toy truck and a toy car, which could pose a safety risk if they are not supervised. However, the baby is sitting on the floor, so there is less risk of injury compared to if they were playing with the toys on a higher surface.</p><p>[Input Instruction] Can you identify any safety hazards in this video?</p><p>Sure, here's a short poem inspired by the video:</p><p>The flowers bloom, a gentle grace, Their petals soft, their colors trace.</p><p>Their beauty, a sight to behold, A moment of peace, a moment to hold. Their colors, a symphony of hues, A celebration of life, and all it can do. Their beauty, a reminder to cherish, The simple things, that bring us happiness. So let us take a moment to appreciate, The beauty of flowers, and all they can create.</p><p>[Input Instruction] Can you write a short poem inspired from t</p><p>Once upon a time, in a kingdom far away, there lived a beautiful princess named Rose. She was known for her kindness and beauty, and everyone loved her. One day, a handsome prince named Leo came to visit the kingdom. He was fascinated by Rose's beauty and decided to stay in the kingdom. However, Rose's jealous sister, Lily, did not like the attention Leo was giving her sister. She tried to harm Rose, but Leo saved her and they fell in love. The kingdom celebrated their love and they lived happily ever after. The moral of the story is that true love conquers all and that jealousy can lead to harmful consequences.</p><p>[Input Instruction] Create a short fairy tale with a moral lesson inspired by the video.</p><p>The video starts with a car covered in snow, and a person is seen using remove the snow from the car's windows. The person is seen scraping windshield and the side windows of the car. The person continues to sc the car, making sure that all the snow is removed. The video then show wiping the snow off the car's windshield and side windows. The person cloth to wipe the snow off the car's windows, making sure that the car clean. The video ends with the person finishing the job and the car bein Overall, the video shows a person removing snow from a car's window and then wiping the snow off the car's windows to make sure it is comp</p><p>[Input Instruction] Describe the video in detail.</p><p>The video shows a person preparing a dish with peppers and oranges. They slice the peppers and oranges and lay them out on a table. The person then proceeds to cut the peppers and oranges into strips and lays them out on a piece of wax paper. They use a knife to cut the peppers and oranges into strips and lay them out on a piece of wax paper.</p><p>The person then uses a spoon to mix the peppers and oranges together. The video ends with the person cutting the peppers and oranges into strips and laying them out on a piece of wax paper.</p><p>[Input Instruction] Describe the video.</p><p>The main focus of the video is the process of creating a painting of a tree using various techniques and colors. The video shows the artist's hand holding a paintbrush and using it to create the tree, as well as the use of different colors and techniques to create the painting.</p><p>[Input Instruction] What is the main focus of the video.</p><p>The woman uses an ax to split the logs. She holds the ax in her hand and swings it down onto the log. She repeats this process several times, splitting the log into smaller pieces.</p><p>[Input Instruction] What does the woman use to split the logs and how does she do it?</p><p>In the video, we see a man walking down a track while holding a javeli down the track and throws the javelin. The video then shows the man w the starting point, and we see the javelin landing on the ground. The ma away from the camera. From the video, we can see that the man is a jav he is practicing his throws. The track appears to be well-maintained, an surroundings seem to be quiet and peaceful. The man's form and techni impressive, and it is clear that he has experience in the sport. Overall, th a glimpse into the world of javelin throwing and the dedication and ski in the sport.</p><p>[Input Instruction] Describe the video in detail. The video is strange because it shows a man sitting next to a lion and a lion laying down next to a man. The man is also seen petting a lion in the grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction]</head><p>What is strange about this video?</p><p>funny because the baby is playing with a Wii remote, which is not a toy ciated with babies. The baby's actions are also humorous, such as pointing and making faces, which adds to the overall amusement of the video.</p><p>n] Why is this video funny?</p><p>The unusual aspect of this video is that the giraffe is seen swimming in a pool, which is not a common sight. The giraffe is also seen jumping off a diving board and swimming underwater, which adds to the uniqueness of the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction]</head><p>What is unusual about this video?</p><p>is playing with a toy truck and a toy car, which could pose a safety risk if upervised. However, the baby is sitting on the floor, so there is less risk of red to if they were playing with the toys on a higher surface.</p><p>n] Can you identify any safety hazards in this video?</p><p>Sure, here's a short poem inspired by the video:</p><p>The flowers bloom, a gentle grace, Their petals soft, their colors trace.</p><p>Their beauty, a sight to behold, A moment of peace, a moment to hold. Their colors, a symphony of hues, A celebration of life, and all it can do. Their beauty, a reminder to cherish, The simple things, that bring us happiness. So let us take a moment to appreciate, The beauty of flowers, and all they can create.</p><p>[Input Instruction] Can you write a short poem inspired from the video.</p><p>time, in a kingdom far away, there lived a beautiful princess named Rose. wn for her kindness and beauty, and everyone loved her. One day, a ince named Leo came to visit the kingdom. He was fascinated by Rose's ecided to stay in the kingdom. However, Rose's jealous sister, Lily, did not tion Leo was giving her sister. She tried to harm Rose, but Leo saved her in love. The kingdom celebrated their love and they lived happily ever after. the story is that true love conquers all and that jealousy can lead to harmful .</p><p>on] Create a short fairy tale with a moral lesson inspired</p><p>The video starts with a car covered in snow, and a person is seen using a scraper to remove the snow from the car's windows. The person is seen scraping the snow off the windshield and the side windows of the car. The person continues to scrape the snow off the car, making sure that all the snow is removed. The video then shows the person wiping the snow off the car's and side windows. The person is seen using a cloth to wipe the snow off the car's windows, making sure that the car is completely clean. The video ends with the person finishing the job and the car being shown off.</p><p>Overall, the video shows a person removing snow from a car's windows and windshield, and then wiping the snow off the car's windows to make sure it is completely clean.</p><p>[Input Instruction] Describe the video in detail.</p><p>ows a person preparing a dish with peppers and oranges. They slice the oranges and lay them out on a table. The person then proceeds to cut the oranges into strips and lays them out on a piece of wax paper. They use a he peppers and oranges into strips and lay them out on a piece of wax paper. en uses a spoon to mix the peppers and oranges together. The video ends on cutting the peppers and oranges into strips and laying them out on a piece .</p><p>n] Describe the video.</p><p>us of the video is the process of creating a painting of a tree using various d colors. The video shows the artist's hand holding a paintbrush and using it ree, as well as the use of different colors and techniques to create the n] What is the main focus of the video.</p><p>uses an ax to split the logs. She holds the ax in her hand and swings it down She repeats this process several times, splitting the log into smaller pieces.</p><p>on] What does the woman use to split the logs and how does</p><p>In the video, we see a man walking down a track while holding a javelin. He then runs down the track and throws the javelin. The video then shows the man walking back to the starting point, and we see the javelin landing on the ground. The man then walks away from the camera. From the video, we can see that the man is a javelin thrower, and he is practicing his throws. The track appears to be well-maintained, and the surroundings seem to be quiet and peaceful. The man's form and technique are impressive, and it is clear that he has experience in the sport. Overall, the video provides a glimpse into the world of javelin throwing and the dedication and skill required to excel in the sport.</p><p>[Input Instruction] Describe the video in detail. The video shows a man playing the drums in a room. He is seen playing the drums with his hands and sticks, and he continues to play while looking off into the distance.</p><p>[Input Instruction] Describe the activity in the video.</p><p>The video shows a young girl performing a gymnastics routine on a balance beam. She executes several flips and tricks before dismounting and landing on a mat.</p><p>[Input Instruction] Describe the video.</p><p>The video features a woman grooming a black horse in a stable. She brushes the horse's hair and cleans its hooves, and the horse is seen standing calmly throughout the process.</p><p>The video also includes shots of the woman speaking to the camera and the horse.</p><p>[Input Instruction] Briefly describe the video.</p><p>The main challenge faced by the people on the boat is the rough sea conditions. They are seen struggling to stay upright and maintain their balance as the boat sways back and forth.</p><p>[Input Instruction] What is the main challenge faced by the people on the boat?</p><p>The video is about a man who is seen spinning and throwing a discus in a field. He then walks away while people measure his throw.</p><p>[Input Instruction] What is the video about?</p><p>Sure! The man in the video demonstrates how to clean a screen window by first wiping it down with a rag. He then applies a cleaning solution to the screen and wipes it down again. Finally, he shows how to dry the screen with a towel.</p><p>[Input Instruction] Can you describe the process for cleaning a screen window as demonstrated in the video?</p><p>The woman and the dog are playing frisbee in the yard. The woman throws the frisbee, and the dog chases after it.</p><p>[Input Instruction] What activities are the woman and the dog engaged in the video?</p><p>The video shows a person cooking a dish of noodles. They start by boiling water in a pot and adding noodles to it. Then, they add oil and some ingredients to the pot. After that, they mix the noodles and the ingredients together. Finally, they serve the dish on a plate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction]</head><p>Step-by-step describe the cooking recipe shown in the video.</p><p>The video shows two men arm wrestling on a table while being watched by a group of people.</p><p>[Input Instruction] What is the video about?</p><p>[Input Instruction] Is there a flag in the background?</p><p>Yes, there is a flag in the background of the video.</p><p>The video is taken from the top of the Statue of Liberty, which is a famous landmark in New York City. The statue is shown from different angles, including from the water and from the top of the statue itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>[Input Instruction]</head><p>Where is this video taken from? What place/landmark is shown in the video? </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparing Different LVLM Paradigms.Video-LLaVA aligns images and videos before projection, allowing LLM to learn from a unified visual representation and endowing LLM with the ability to comprehend both images and videos simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>image and the video are depicting the same place. The video shows the statue of liberty from different angles, while the image shows a close-up of the statue. Both the video and the image capture the beauty and grandeur of the statue of liberty. (a) Illustration of Video-LLaVA (b) Performance comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Training framework and performance. Video-LLaVA exhibits remarkable interactive capabilities between images and videos, despite the absence of image-video pairs in the dataset. (a) The Video-LLaVA framework demonstrates a data flow that generates corresponding responses based on input instructions. (b) Video-LLaVA achieves superior performances on a broad range of 15 datasets across image and video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of alignment before projection on video. We validate and report the accuracy and score on four video question-answering datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effect of joint training on image. † donates that We reproduce the results of LLaVA-1.5 at a resolution of 224×224 with LanguageBind-Image encoder for a fair comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Samples of Video-LLaVA in video understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Samples of Video-LLaVA in video understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Samples of Video-LLaVA in video understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Samples of Video-LLaVA in video understanding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between different LVLMs on video reasoning benchmarks. We employ ChatGPT-Assistant to evaluate the performance following Video-ChatGPT<ref type="bibr" target="#b37">(Maaz et al., 2023)</ref>. The version of ChatGPT is "gpt-3.5-turbo".</figDesc><table><row><cell>Methods</cell><cell cols="9">LLM size Accuracy Score Accuracy Score Accuracy Score Accuracy Score MSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA</cell></row><row><cell>FrozenBiLM</cell><cell>1B</cell><cell>32.2</cell><cell>-</cell><cell>16.8</cell><cell>-</cell><cell>41.0</cell><cell>-</cell><cell>24.7</cell><cell>-</cell></row><row><cell>VideoChat</cell><cell>7B</cell><cell>56.3</cell><cell>2.8</cell><cell>45.0</cell><cell>2.5</cell><cell>34.4</cell><cell>2.3</cell><cell>-</cell><cell>2.2</cell></row><row><cell cols="2">LLaMA-Adapter 7B</cell><cell>54.9</cell><cell>3.1</cell><cell>43.8</cell><cell>2.7</cell><cell>-</cell><cell>-</cell><cell>34.2</cell><cell>2.7</cell></row><row><cell>Video-LLaMA</cell><cell>7B</cell><cell>51.6</cell><cell>2.5</cell><cell>29.6</cell><cell>1.8</cell><cell>-</cell><cell>-</cell><cell>12.4</cell><cell>1.1</cell></row><row><cell>Video-ChatGPT</cell><cell>7B</cell><cell>64.9</cell><cell>3.3</cell><cell>49.3</cell><cell>2.8</cell><cell>51.4</cell><cell>3.0</cell><cell>35.2</cell><cell>2.7</cell></row><row><cell>Chat-UniVi Video-LLaVA</cell><cell>7B 7B</cell><cell>65.0 70.7</cell><cell>3.6 3.9</cell><cell>54.6 59.2</cell><cell>3.1 3.5</cell><cell>60.3 70.0</cell><cell>3.4 4.0</cell><cell>45.8 45.3</cell><cell>3.2 3.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison between different LVLMs on image understanding benchmarks. "Res.", "L", "V" respectively represent the input image resolution, LLaMA(Touvron et al., 2023a)  and Vicuna<ref type="bibr" target="#b7">(Chiang et al., 2023)</ref>.</figDesc><table><row><cell>Methods</cell><cell>LLM Res.</cell><cell cols="7">Image Question Answering VQA v2 GQA VisWiz SQA I VQA T POPE MMB LLaVA W MM-Vet Benchmark Toolkit</cell></row><row><cell>LLaVA-1.5 BLIP-2</cell><cell cols="6">V-7B 336 V-13B 224 41.0 41.0 19.6 61.0 42.5 85.3 -----62.0  *</cell><cell>--</cell><cell>-38.1</cell><cell>30.5 22.4</cell></row><row><cell cols="2">InstructBLIP V-13B 224</cell><cell>-</cell><cell cols="4">49.5 33.4 63.1 50.7 78.9</cell><cell>-</cell><cell>58.2</cell><cell>25.6</cell></row><row><cell cols="4">IDEFICS-80B L-65B 224 60.0 45.2 36.0</cell><cell>-</cell><cell>30.9</cell><cell>-</cell><cell>54.5</cell><cell>-</cell><cell>-</cell></row><row><cell>MiniGPT-4</cell><cell>L-7B 224</cell><cell>-</cell><cell cols="3">30.8 47.5 25.4 19.4</cell><cell>-</cell><cell>23.0</cell><cell>-</cell><cell>22.1</cell></row><row><cell cols="4">IDEFICS-9B L-7B 224 50.9 38.4 35.5</cell><cell>-</cell><cell>25.9</cell><cell>-</cell><cell>48.2</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">mPLUG-Owl L-7B 224</cell><cell>-</cell><cell>14.0 39.0</cell><cell cols="2">2.8 38.8</cell><cell>-</cell><cell>46.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Otter InstructBLIP V-7B 224 L-7B 224</cell><cell>--</cell><cell cols="3">38.1 50.0 27.2 21.2 49.2 34.5 60.5 50.1</cell><cell>--</cell><cell>32.6 36.0</cell><cell>-60.9</cell><cell>24.6 26.2</cell></row><row><cell cols="8">LLaVA-1.5  † Video-LLaVA V-7B 224 74.7  *  60.3  *  48.1 66.4 51.8 84.4 60.9 V-7B 224 72.3  *  56.9  *  47.8 67.9 49.2 83.3 59.5</cell><cell>63.3 73.1</cell><cell>25.7 32.0</cell></row><row><cell cols="4">pabilities of large video-language models on four</cell><cell cols="5">SOTA model, Chat-UniVi (Jin et al., 2023). De-</cell></row><row><cell cols="4">datasets, including MSVD-QA (Chen and Dolan,</cell><cell cols="5">spite Chat-UniVi utilizing more datasets such as</cell></row><row><cell cols="4">2011), MSRVTT-QA (Xu et al., 2016), TGIF-</cell><cell cols="5">MIMIC-IT (Li et al., 2023a), Video-LLaVA still</cell></row><row><cell cols="4">QA (Jang et al., 2017) and ActivityNet-QA (Yu</cell><cell cols="5">demonstrate competitive results, surpassing Chat-</cell></row><row><cell cols="4">et al., 2019). The evaluation pipeline for video</cell><cell cols="5">UniVi on MSVD, MSRVTT, and TGIF datasets.</cell></row><row><cell cols="4">understanding follows Video-ChatGPT. We report</cell><cell cols="5">In summary, these results validate Video-LLaVA's</cell></row><row><cell cols="4">the accuracy and score, which is assessed using</cell><cell cols="5">ability to comprehend videos and provide contex-</cell></row><row><cell cols="4">GPT-Assistant. Video-LLaVA consistently out-</cell><cell cols="5">tually appropriate responses based on instructions.</cell></row><row><cell cols="4">performs Video-ChatGPT in terms of question-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">answering accuracy, which is an advanced large video-language model. Moreover, Video-LLaVA</cell><cell cols="5">4.2.2 Zero-shot Image Question-answering</cell></row><row><cell cols="4">surpasses the powerful baseline of Video-ChatGPT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MSVD, TGIF, and ActivityNet, respectively. Addi-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">tionally, we conduct comparisons with the recent</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Benchmark names are abbreviated due to page limitations. VQA-v2<ref type="bibr" target="#b13">(Goyal et al., 2017)</ref></p><p>; GQA (Hudson and Manning, 2019); VisWiz<ref type="bibr" target="#b14">(Gurari et al., 2018)</ref></p><p>; SQA I : ScienceQA-IMG<ref type="bibr" target="#b34">(Lu et al., 2022)</ref></p><p>; VQA T : TextVQA<ref type="bibr" target="#b43">(Singh et al., 2019)</ref></p><p>; POPE(Li et al., 2023d)</p><p>; MMB: MMBench(Liu et al., 2023c)</p><p>; LLaVA W : LLaVA-Bench (In-the-Wild)(Liu et al., 2023b)</p><p>; MM-Vet<ref type="bibr" target="#b53">(Yu et al., 2023)</ref></p><p>. † donates that we reproduce LLaVA-1.5 with LanguageBind-Image encoder to compare fairly. * donates that there is some overlap in the training data.</p><p>As shown in Table3</p><p>, we evaluate our approach for image understanding on five academic image question-answering benchmarks. Compared to the state-of-the-art model InstructBLIP-7B, Video-LLaVA demonstrates powerful image understand-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Zero-shot object hallucination evaluation results are reported for three POPE evaluation settings. "Yes" indicates the proportion of positive responses to the given question. † donates that we reproduce LLaVA-1.5 with LanguageBind-Image encoder to compare fairly.</figDesc><table><row><cell>Methods</cell><cell>LLM</cell><cell cols="9">Adersarial Accuracy F1-Score Yes Accuracy F1-Score Yes Accuracy F1-Score Yes Popular Random</cell></row><row><cell>MiniGPT-4</cell><cell cols="2">V-13B 66.6</cell><cell>71.4</cell><cell>66.7</cell><cell>68.3</cell><cell>72.2</cell><cell>64.1</cell><cell>77.8</cell><cell>78.9</cell><cell>54.8</cell></row><row><cell cols="3">InstructBLIP V-13B 74.4</cell><cell>78.5</cell><cell>69.0</cell><cell>81.4</cell><cell>83.5</cell><cell>62.6</cell><cell>88.7</cell><cell>89.3</cell><cell>55.2</cell></row><row><cell>MM-GPT</cell><cell>L-7B</cell><cell>50.0</cell><cell cols="3">66.7 100.0 50.0</cell><cell cols="3">66.7 100.0 50.0</cell><cell cols="2">66.7 100.0</cell></row><row><cell cols="2">mPLUG-Owl L-7B</cell><cell>50.7</cell><cell>66.8</cell><cell>98.7</cell><cell>50.9</cell><cell>66.9</cell><cell>98.6</cell><cell>54.0</cell><cell>66.4</cell><cell>95.6</cell></row><row><cell>Chat-UniVi</cell><cell>V-7B</cell><cell>55.6</cell><cell>68.7</cell><cell>91.6</cell><cell>56.4</cell><cell>69.0</cell><cell>90.8</cell><cell>73.9</cell><cell>79.3</cell><cell>74.6</cell></row><row><cell cols="2">LLaVA-1.5  † L-7B Video-LLaVA V-7B</cell><cell>84.3 81.6</cell><cell>83.2 80.8</cell><cell>43.5 45.8</cell><cell>79.8 85.3</cell><cell>79.4 84.0</cell><cell>48.0 42.1</cell><cell>85.7 86.2</cell><cell>84.8 85.2</cell><cell>43.0 42.0</cell></row><row><cell cols="5">ing capabilities, outperforming across all five</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">question-answering benchmarks. Additionally,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Video-LLaVA exhibits competitive results com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">pared to several more powerful LVLMs, which</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">are tuned based on 13B or 65B LLM, such as sur-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">passing InstructBLIP-13B by 14.7% on VisWiz,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">highlighting its strong understanding ability in nat-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ural visual environments. Furthermore, to ensure a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">fair comparison, we replace the image encoder in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">LLaVA-1.5 with the LanguageBind-Image encoder,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">called LLaVA-1.5  † . This demonstrates that the per-formance improvement observed in Video-LLaVA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">is not solely attributed to a stronger image encoder.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Additional details can be found in Section 4.3.6.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Evaluation under Image Benchmark Toolkits Additionally, we evaluate LVLMs using several</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">benchmark toolkits for visual instruction tuning.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">These benchmark toolkits provide a detailed as-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">sessment of the model's capabilities through ro-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">bust evaluation metrics. Video-LLaVA outperform</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">InstructBLIP-7B by 24.9%, 12.2%, and 5.8% on</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">MMBench, LLaVA-Bench, and MM-Vet, respec-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">tively. It is worth noting that Video-LLaVA-7B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">still demonstrates advanced performance compared</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">to larger LLM models, surpassing InstructBLIP-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">13B by 6.4% on MM-Vet and IDEFICS-80B (Lau-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">rençon et al., 2023) by 6.4% on MMBench. These</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">results demonstrate that Video-LLaVA exhibits a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">strong understanding of semantic aspects of scenes,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">enabling it to answer open-ended and free-form</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">natural language questions about images.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Effect of alignment before projection on image. "United" refers to the unified visual representation, while "Separated" refers to the separated visual representation. Benchmark names are abbreviated due to page limitations.</figDesc><table><row><cell>Methods</cell><cell cols="3">Image Question Answering VQA v2 GQA VisWiz SQA I VQA T POPE MMB LLaVA W MM-Vet Benchmark Toolkit</cell></row><row><cell cols="2">Separated-MAE 66.0 55.4 42.5 65.0 44.2 80.8 45.7</cell><cell>35.9</cell><cell>20.0</cell></row><row><cell cols="2">Separated-CLIP 74.6 59.9 47.8 67.3 51.5 84.4 60.2 United 74.7 60.3 48.1 66.4 51.8 84.4 60.9 ∆ Acc. +0.1 +0.4 +0.3 -0.9 +0.3 +0.0 +0.7</cell><cell>68.9 73.1 +4.2</cell><cell>30.6 32.0 +1.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Effect of joint training on video. We evaluate on four video question-answering datasets. * denotes that we utilized only video data in both the first and second stages.</figDesc><table><row><cell></cell><cell>Methods</cell><cell></cell><cell cols="5">MSVD-QA MSRVTT-QA Accuracy Score Accuracy Score Accuracy Score Accuracy Score TGIF-QA ActivityNet-QA</cell></row><row><cell></cell><cell cols="5">64.8 Joint with Image 70.7 Video-LLaVA  *  ∆ Acc. +5.9 +0.7 3.2 3.9</cell><cell>58.3 59.2 +0.9 +0.1 3.4 3.5</cell><cell>67.8 70.0 +2.2 +0.6 3.4 4.0</cell><cell>40.7 45.3 +4.6</cell><cell>2.0 3.3 +1.3</cell></row><row><cell></cell><cell cols="2">6HSDUDWHG&amp;/,3</cell><cell></cell><cell></cell><cell cols="2">6HSDUDWHG&amp;/,3</cell></row><row><cell></cell><cell cols="2">6HSDUDWHG0$(</cell><cell></cell><cell></cell><cell cols="2">6HSDUDWHG0$(</cell></row><row><cell>3HUIRUPDQFH</cell><cell cols="2">8QLWHG</cell><cell>3HUIRUPDQFH</cell><cell></cell><cell cols="2">8QLWHG</cell></row><row><cell>0 69 '</cell><cell>7* ,) 0 65 97 7</cell><cell>$F WLY LW\ 1H W</cell><cell>0 69 '</cell><cell>7* ,)</cell><cell>0 65 97 7</cell><cell>$F WLY LW\ 1H W</cell></row><row><cell cols="3">D$FFXUDF\RITXHVWLRQDQVZHULQJ</cell><cell cols="4">E6FRUHRITXHVWLRQDQVZHULQJ</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by the <rs type="funder">Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62202014</rs>, <rs type="grantNumber">62332002</rs>, <rs type="grantNumber">62425101</rs>), <rs type="programName">Shenzhen Basic Research Program</rs> (No.<rs type="grantNumber">JCYJ20220813151736001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_37mUTuW">
					<idno type="grant-number">62202014</idno>
				</org>
				<org type="funding" xml:id="_YcPV7Xz">
					<idno type="grant-number">62332002</idno>
				</org>
				<org type="funding" xml:id="_4GBNTur">
					<idno type="grant-number">62425101</idno>
					<orgName type="program" subtype="full">Shenzhen Basic Research Program</orgName>
				</org>
				<org type="funding" xml:id="_4tTEDm2">
					<idno type="grant-number">JCYJ20220813151736001</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p><ref type="url" target="https://github.com/PKU-YuanGroup/Video-LLaVA">https://github.com/PKU-YuanGroup/Video-LLaVA</ref> </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Example Appendix A.1 Training Setting</head><p>We show some training settings as shown in Table <ref type="table">7</ref>. video encoder and image encoder are not trained in both stages. The projection layer consists of 2 linear layers with a GeLU <ref type="bibr" target="#b17">(Hendrycks and Gimpel, 2016)</ref> activation function between them. Image and video share the projection layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Exhibition Board</head><p>We show some unselected samples here, and these videos are sourced from Video-ChatGPT <ref type="bibr" target="#b37">(Maaz et al., 2023)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23716" to="23736" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuel</forename><surname>Taropa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paige</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10403</idno>
		<title level="m">Palm 2 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gül</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Palm: Pre-training an autoencoding&amp;autoregressive language model for context-conditioned generation</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07159</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Xllm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages</title>
		<author>
			<persName><forename type="first">Feilong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minglun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04160</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://vicuna.lmsys.org" />
		<imprint>
			<date type="published" when="2023-04">2023. April 2023</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Instructblip: Towards general-purpose visionlanguage models with instruction tuning</title>
		<author>
			<persName><forename type="first">Wenliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huat</forename><surname>Tiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06500</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinrui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiawu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13394</idno>
		<title level="m">Mme: A comprehensive evaluation benchmark for multimodal large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.15010</idno>
	</analytic>
	<monogr>
		<title level="m">Parameter-efficient visual instruction model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagebind: One embedding space to bind them all</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Vasudev Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15180" to="15190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04790</idno>
		<title level="m">Multimodal-gpt: A vision and language model for dialogue with humans</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6904" to="6913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vizwiz grand challenge: Answering visual questions from blind people</title>
		<author>
			<persName><forename type="first">Danna</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigale</forename><forename type="middle">J</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3608" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.03905</idno>
		<title level="m">Imagebind-llm: Multi-modality instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cade</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5143773</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>If you use this software, please cite it as below</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatiotemporal reasoning in visual question answering</title>
		<author>
			<persName><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Chat-univi: Unified visual representation empowers large language models with image and video understanding</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiwan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.08046</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Obelics: An open web-scale filtered dataset of interleaved image-text documents</title>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Hugo Laurençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léo</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stas</forename><surname>Tronchon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Bekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lozhkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><surname>Sanh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.16527</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03726</idno>
		<title level="m">A multi-modal model with in-context instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">2023b. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramprasaath</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9694" to="9705" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Kunchang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06355</idno>
		<title level="m">Limin Wang, and Yu Qiao. 2023c. Videochat: Chat-centric video understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10355</idno>
		<title level="m">Evaluating object hallucination in large vision-language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.15947</idno>
		<title level="m">Moe-llava: Mixture of experts for large visionlanguage models</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.03744</idno>
		<title level="m">Improved baselines with visual instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08485</idno>
		<title level="m">Visual instruction tuning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yike</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conghui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.06281</idno>
		<title level="m">Mmbench: Is your multi-modal model an all-around player? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learn to explain: Multimodal reasoning via thought chains for science question answering</title>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanglin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2507" to="2521" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Ruipu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.07207</idno>
		<title level="m">Valley: Video assistant with large language model enhanced ability</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingshuai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zefeng</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.09093</idno>
		<title level="m">Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Video-chatgpt: Towards detailed video understanding via large vision and language models</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanoona</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05424</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ilić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Castagné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Sasha Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><surname>Gallé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.05100</idno>
		<title level="m">Bloom: A 176bparameter open-access multilingual language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface</title>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17580</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards vqa models that can read</title>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8317" to="8326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Vipergpt: Visual inference via python execution for reasoning</title>
		<author>
			<persName><forename type="first">Dídac</forename><surname>Surís</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08128</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori B</forename><surname>Hashimoto</surname></persName>
		</author>
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasmine</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Bashlykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajjwal</forename><surname>Bhargava</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09288</idno>
		<title level="m">Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Visual chatgpt: Talking, drawing and editing with visual foundation models</title>
		<author>
			<persName><forename type="first">Chenfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengming</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zecheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.04671</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Msrvtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Azarnasab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11381</idno>
		<title level="m">Mmreact: Prompting chatgpt for multimodal reasoning and action</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Qinghao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anwen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaya</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14178</idno>
		<title level="m">mplug-owl: Modularization empowers large language models with multimodality</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Shukang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyou</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.13549</idno>
		<title level="m">A survey on multimodal large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.02490</idno>
		<title level="m">Mm-vet: Evaluating large multimodal models for integrated capabilities</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Activitynet-qa: A dataset for understanding complex web videos via question answering</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9127" to="9134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">2023a. Videollama: An instruction-tuned audio-visual language model for video understanding</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02858</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">2023b. Llama-adapter: Efficient fine-tuning of language models with zero-init attention</title>
		<author>
			<persName><forename type="first">Renrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangfei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shilin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16199</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxi</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatian</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01852</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2023a</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">2023b. Minigpt-4: Enhancing vision-language understanding with advanced large language models</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.10592</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
