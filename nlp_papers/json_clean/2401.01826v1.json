{
  "paperid": "2401.01826v1",
  "title": "Data-Driven Power Modeling and Monitoring via Hardware Performance Counters Tracking",
  "authors": [
    "Sergio Mazzola",
    "Gabriele Ara",
    "Thomas Benz",
    "Björn Forsberg",
    "Tommaso Cucinotta",
    "Luca Benini",
    "E Quiñones",
    "S Royuela",
    "C Scordino",
    "P Gai"
  ],
  "year": 2024,
  "abstract": "In the current high-performance and embedded computing era, full-stack energy-centric design is paramount. Use cases require increasingly high performance at an affordable power budget, often under real-time constraints. Extreme heterogeneity and parallelism address these issues but greatly complicate online power consumption assessment, which is essential for dynamic hardware and software stack adaptations. We introduce a novel architecture-agnostic power modeling methodology with state-of-the-art accuracy, low overhead, and high responsiveness. Our methodology identifies the best Performance Monitoring Counters (PMCs) to model the power consumption of each hardware sub-system at each Dynamic Voltage and Frequency Scaling (DVFS) state. The individual linear models are combined into a complete model that effectively describes the power consumption of the whole system, achieving high accuracy and low overhead. Our evaluation reports an average estimation error of 7.5 % for power consumption and 1.3 % for energy. Furthermore, we propose Runmeter, an open-source, PMC-based monitoring framework integrated into the Linux kernel. Runmeter manages PMC samples collection and manipulation, efficiently evaluating our power models at runtime. With a time overhead of only 0.7 % in the worst case, Runmeter provides responsive and accurate power measurements directly in the kernel, which can be employed for actuation policies such as Dynamic Power Management (DPM) and power-aware task scheduling.",
  "sections": [
    {
      "heading": "I. INTRODUCTION",
      "text": "R ECENT years have seen a dramatic evolution in the embedded and real-time computing landscape, with increasingly demanding requirements. Applications striving for ever-higher computing capabilities and energy efficiency are shifting toward heterogeneous computing platforms [1], [2]. Embedded High Performance Computing applications [3] include soft real-time use cases (such as media streaming, This work has received funding from the European Commission through the EU H2020 research project AMPERE (A Model-driven development framework for highly Parallel and EneRgy-Efficient computation supporting multi-criteria optimization) under grant agreement no. 871669. Sergio Mazzola and Gabriele Ara contributed equally to this work. Sergio Mazzola and Thomas Benz are with the Integrated Systems Laboratory (IIS), ETH Zürich, 8092 Zürich, Switzerland (e-mail: smazzola@iis.ee.ethz.ch; tbenz@iis.ee.ethz.ch). Björn Forsberg is with the Department of Computer Science, RISE Research Institutes of Sweden, 164 40 Kista, Sweden (e-mail: bjorn.forsberg@ri.se). Gabriele Ara and Tommaso Cucinotta are with the Real-Time Systems Laboratory, Scuola Superiore Sant'Anna, 56124 Pisa, Italy (email: gabriele.ara@santannapisa.it; tommaso.cucinotta@santannapisa.it). Luca Benini is with the Integrated Systems Laboratory (IIS), ETH Zürich, 8092 Zürich, Switzerland, and also with the Department of Electrical, Electronic and Information Engineering (DEI), University of Bologna, 40136 Bologna, Italy (e-mail: lbenini@iis.ee.ethz.ch). virtual and augmented reality) as well as hard real-time ones (Industry 4.0 and connected factories, modern automotive and aerospace with self-driving, autonomous vehicles). These areas often demand embedded systems with enhanced sensing, control, and actuation. To achieve this, embedded devices are typically equipped with hardware accelerators to execute complex, real-time machine-learning models, as well as 5G communication pipelines [4]. Heterogeneity and massive parallelism are leveraged by hardware designers to integrate increasingly higher computing capabilities on a single die at an affordable energy budget [5]. However, since the end of Dennard's scaling, it has been increasingly challenging to match Moore's predictions [5], due to several walls, from power consumption to memory to hardware overspecialization [6]. In current computer architectures, transistor miniaturization causes increased power density and thermal dissipation issues. With the limits of current silicon technology exposed, pushing for maximum energy efficiency in a dynamic, workload-and operating-condition-aware fashion is paramount to meet the widespread need for high performance within sustainable power budgets [7]. The de-facto standard to address power efficiency is Dynamic Power Management (DPM), integrated even in today's simplest embedded systems in the form of Dynamic Voltage and Frequency Scaling (DVFS) and clock gating. To exploit the full potential of power-manageable designs, the software stack must also be able to perform intelligent adaptation of the powerrelated knobs based on the available power-related metrics. This information must be provided at the lowest levels of the software stack, i.e., the OS kernel. As a matter of fact, this allows the task scheduler to perform informed decisions as to the assignment of computing resources to the running processes based on two factors: the current status of the hardware and accurate estimates of the power consumption and its variations [8], [9]. For such full-stack, energy-aware dynamic adaptations to be effective, however, accurate, fine-grain, and responsive online power measurements are required. The fine-granularity property is twofold: a fine-granularity power measurement has decomposability properties, i.e., it is able to provide insights into the power consumption of individual hardware sub-systems [10]. This is essential for DPM. Additionally, a fine-granularity measurement provides information about task-level power consumption, useful for task scheduling. The responsiveness of a power measurement technique refers to its ability to reflect the actual profile of the hardware activity with negligible latency, which requires a low power gauging overhead. This is necessary for the stability of any actuation control loop. The most commonly adopted solution in this regard consists of analog power sensors. However, as discussed in Section II-C, they do not typically satisfy such requirements [11]. As an alternative to power sensing, power models have been extensively researched to obtain power measurements better suited for dynamic, online adaptations of hardware and software. It is well-known that Performance Monitoring Counter (PMC) activity effectively correlates to power consumption [12], enabling accurate power modeling for fast, responsive, and finegrain indirect power gauging [10]. However, the complexity of selecting appropriate PMCs and understanding the underlying hardw"
    },
    {
      "heading": "II. BACKGROUND",
      "text": "An energy-centric design flow addresses energy efficiency from the point of view of the whole system stack, which is fundamental for complex parallel and heterogeneous systems. To motivate our proposal, we present two main use cases of power-aware dynamic adaptations of the hardware and software, namely DPM and power-aware task scheduling, requiring robust online power measurements. In the following, we also discuss why typical implementations of analog power gauges are not suitable for such hardware and software dynamic adaptations, motivating the choice of PMC-based power models."
    },
    {
      "heading": "A. Dynamic Power Management",
      "text": "Dynamic Power Management (DPM) is an essential feature when it comes to increasing performance and energy efficiency through parallelism and heterogeneity [14]. Having multiple processing elements or computing islands enables fine-grained control of their power status through clock gating and DVFS. In this way, hardware portions can be independently turned off or slowed down based on the phase of the running workload. This mechanism allows modern architectures to keep enough logic on the same die such that hardware overspecialization is prevented while still achieving high energy efficiency. However, DPM-enabled hardware alone is not enough. Actuation policies that are aware of the current power consumption of the hardware must be in place, and they must leverage robust power measurements to close the control loop [15]."
    },
    {
      "heading": "B. Power-Aware Task Scheduling",
      "text": "Power-aware task scheduling integrates DPM techniques, such as DVFS, with the task scheduler [16]. Augmenting the process scheduler with intimate knowledge on the tasks that it is managing is paramount for developing intelligent scheduling techniques [17], [18]. This is especially true for applications characterized by real-time constraints running on embedded systems, where power awareness within the task scheduler is essential [8], [19]. As a first-order approximation, task execution time scales with the CPU frequency, therefore not all the available DVFS states might be suitable for meeting the timing constraints of the real-time tasks [20]. In addition, by managing the migration of tasks among computational units, the scheduler is an important tool to regulate the thermal behavior of the platform. This is a ubiquitous issue of embedded devices, which can rarely afford an external cooling mechanism [21]. Furthermore, the scheduler decisions are based on currently known system conditions (e.g., its power consumption or expected task finishing time), but their impact might extend for a long time [16]. A prediction may indeed lead to unrecoverable situations if it is later proved wrong. This problem is exacerbated by the slow reaction time and high access overhead typical of traditional analog power monitors [22], the de facto standard for many embedded systems, critically hindering the capability of the scheduler to make well-informed decisions. To address these issues and enable future exploitation of effective power-aware scheduling techniques, in Sections IV and V, we propose a methodology to provide the scheduler with accurate power estimates characterized by fast reaction times and lower access overhead compared to analog power monitors."
    },
    {
      "heading": "C. On the Shortcomings of Analog Power Measurements",
      "text": "For power-aware dynamic hardware adaptations to be possible, online power measurement is a requirement. To effectively leverage techniques such as DPM and power-aware task scheduling, the online power measurement approach must possess the following properties: 1) accuracy: accurate measurements with high enough resolution and sensitivity are required to feed the actuation control loop properly; 2) fine granularity, both in terms of introspection into hardware sub-system power consumption (i.e., decomposability) and task-level power budgeting; 3) responsiveness: promptly reflect the activity profile of the hardware platform to provide the control loop with minimal-latency power measurements. Several off-the-shelf system-on-chips (SoCs) come equipped with built-in power sensors; their typical implementation consists of analog current and voltage meters, an analog-todigital converter (ADC), and some additional circuitry that implements communication with the host system. The reliability of analog power sensors for power-aware dynamic adaptations depends on a large number of factors, which typically do not satisfy the mentioned requirements [11]. The accuracy of analog power measurements is heavily affected by the platform's printed circuit board configuration and further conditioned by the ADC in terms of resolution. As an example, for the NVIDIA Jetson AXG Xavier board, the two built-in INA3221 power monitors have a resolution of approximately 200 mW only [23], [24]. Due to their nature, analog sensors are not integrated on the same die of the SoC, hence can rarely provide the level of introspection of individual hardware sub-systems. For the same reason, off-chip parasitics pollute their measurements with longer transients, impacting the responsiveness of their power measures [22]. Their latency is further affected by the communication channel with the host, usually implemented by a serial protocol such as I2C, which does not match the speed of the digital domain. Moreover, due to their physical size and deployment costs, analog gauges usually showcase inadequate scalability. Several additional solutions concern the adoption of external, high-accuracy, dedicated measurement devices [11]. However, despite the more precise measurements, such solutions are obviously unsuitable for final deployment due to their cost and even worse integration issues, scalability, and introspection capabilities. Although unsuitable for direct power measurements, built-in analog sensors do not require external equipment, and they can be programmatically and reliably driven. Therefore, they prove useful to build accurate, fine-grain, and responsive PMC-based power models through the approach showcased in Section IV. This is demonstrated in Section VI-D."
    },
    {
      "heading": "D. PMC-based Power Estimation",
      "text": "Prior work shows that PMCs activity correlates with the power consumption of diverse hardware components [12], [25], [26]. Being typically accessible via control and status registers, their usage is cheap, and their readings are fast and reliable. As part of the digital domain, PMC activity promptly reflects the current state of the hardware resources, exposing desirable responsiveness properties. PMCs also provide a high degree of introspection into individual hardware sub-systems [27], which empowers PMC-based models with a degree of decomposability exactly equivalent to the availability of PMCs [10]. Therefore, accurate, fine-grain, and responsive power models, such as the one we propose in Section IV, can be derived from them. While PMC-based power models can achieve the accuracy, fine granularity, and responsiveness required by power-aware dynamic adaptations, a model-based approach poses further challenges to the power estimation. Modern computer architectures, even those shipped as part of embedded systems, expose hundreds of countable performance events [23], [28]. Hence, the parameter selection for a robust statistical power model often requires considerable knowledge of the underlying hardware. Growing parallelism and heterogeneity, together with the frequent lack of open documentation, further raise the challenge. A careful choice of the model parameters is necessary for several additional factors: first, Performance Monitor Units (PMUs) can simultaneously track only a limited number of performance counters [28], [29]. Second, the amount of model predictors directly impacts its evaluation overhead, which must be small for practical DPM strategies and minimal interference of the scheduler with regular system operation. DVFS determines an additional layer of modeling complexity, as hardware behavior at varying frequencies has to be considered. To the best of our knowledge, our approach, proposed in Sections IV and V, is the first one to holistically address all the mentioned challenges. III. RELATED WORK [11], [33], [34] present a comprehensive taxonomy and comparison of different modeling approaches and runtime power monitors, ranging from embedded and edge devices to data centers. In the following, we mainly focus on the research works related to PMC-based power modeling and online, modelbased power monitoring, comparing them with our approach. We summarize the key characteristics in Table I."
    },
    {
      "heading": "A. PMC-Based Power Modeling",
      "text": "PMC-based statistical power models have been a hot research topic for the last 20 years, spanning all computing domains from embedded computing devices at the edge to data centers in the cloud. Table I: Comparison between representative works in the literature of PMC-based power modeling. H e t e r o g e n e i t y G e n e r a l i t y A u t o m a t i o n A r c h i t e c t u r ea g n o s t i c L i g h t w e i g h t m o d e l D V F S s u p p o r t D e c o m p o s a b i l i t y R u n t i m e m o n i t o r i n g A c c u r a c y ( M A P E ) Bertran et al. (2012) [10] CPU only Bertran et al. [35] identify two families of PMC-based power models, based on their construction: bottom-up and top-down. Bottom-up approaches rely on extensive knowledge of the underlying architecture to estimate the power consumption of individual hardware sub-systems. While the pioneering works of this field fall in this category [10], [27], their results highlight the strict dependency between the accuracy of bottomup models and architectural knowledge of the platform, which jeopardizes the generality of this approach. Even more recent works in this category lack in general applicability. A notable example of these works is Phung et al. [36], which derive a very accurate power model based on a combination of event counters, CPU operating frequencies and even CPU temperature. The general applicability of this approach is hindered by the fact that it requires extensive knowledge of the selected Intel architecture, achieving the best results only leveraging Intel Running Average Power Limit (RAPL) [37]. Top-down approaches target simple, low-overhead, and more generally applicable models. They commonly assume a linear correlation between generic activity metrics and power consumption. Among the first attempts in the field, Bellosa [12] models the power consumption of a Pentium II processor with a few manually selected PMCs. Subsequent works [38], [39] refine the idea with a more elaborate procedure for PMC selection and multi-core support. Bircher and John [25] are the first to go towards a thorough system-level power model, tackling the issue from a top-down perspective for each subsystem. No past research investigates a combination of accurate and lightweight models addressing DVFS without requiring expert architectural knowledge, which is instead the subject of this paper. More recent works target CPU power modeling in mobile and embedded platforms. Walker et al. [30] employ a systematic and statistically sound technique for PMC selection and train power models for the ARM A7 and A15 embedded processors. However, only one trained weight is used to predict the power consumption at any DVFS state, which can lead to large inaccuracies. In contrast to their work, we target a broad range of platforms by composing simpler statistical models, nevertheless carefully dealing with DVFS and reaching comparable accuracy numbers. Top-down power modeling approaches are also applied to GPUs. Wang et al. [26] analyze the power consumption of an AMD Integrated GPU, carefully studying its architecture and selecting the best PMCs to build a linear power model. Redundant counters are discarded to reduce the model overhead, with power MAPE below 3 %. However, the generated model is not generally applicable and requires expert knowledge. Recent works also resort to deep learning for creating accurate blackbox power models: Mammeri et al. [31] train an Artificial Neural Network (ANN) with several manually chosen CPU and GPU PMCs. With an average power estimation error of 4.5 %, they report power estimates 3× more accurate than a corresponding linear model; however, the overhead of evaluating a neural network at runtime is not negligible, as it requires a number of multiply-accumulate operations two orders of magnitude higher than for a linear model. Potentially long training time, complex manual selection of the neural network topology, risk of overfitting, and lack of decomposability are additional drawbacks of this approach. Tarafdar et al. [32] also propose several power modeling techniques based on multi-variable linear regression, Support Vector Regression, and ANN. While their approach is statistically sound and, in theory, applicable to any generic computing platform, they conceive it as a solution for data centers. Therefore, no fine-grain power information about the computing platform is made available. Moreover, the model parameter selection happens a priori and is not correlated to the modeled platform. Their best power model, which features an evaluation overhead in the order of the microseconds, shows an average power estimation error of at most 4.7%. Our proposed model shares its decomposability and responsiveness with bottom-up approaches but resorts to top-down modeling for individual sub-systems: we trade a lower percomponent introspection for a systematic modeling procedure requiring very little architectural knowledge and minimal human inter"
    },
    {
      "heading": "B. Online Model-based Power Monitoring",
      "text": "Various tools have been proposed for online, model-based estimation of power consumption through PMC sampling. Most of them implement runtime monitoring by simply sampling the PMCs with a fixed periodicity [30], [40], [41]. Some authors highlight that a careful choice of the sampling period is critical to achieving appropriate trade-offs between accuracy and responsiveness [41]. Other works deal with the constraint that only a limited number of counters can be activated at any given time, thus resorting to counter multiplexing [42]. Additionally, some works attempt to use the PMCs to attach fine-grain power consumption estimates to CPU sub-components [27]. One of the most popular open-source tools for online PMC sampling is PMCTrack, developed by Saez et al. [17]. It replaces the |perf| utility provided by mainline Linux, which can be accessed both from user space and from the Linux kernel, in particular from the task scheduler. PMCTrack can monitor per-system, per-CPU, and per-process PMCs. However, it focuses on providing access to PMC-based statistics for general-purpose use cases. Therefore, some aspects of its implementation are not suited for real-time tasks, that may possess different requirements from those of SCHED_OTHER tasks. As an example, PMCTrack may delay the generation of a PMC sample related to a task until it is chosen again by the task scheduler for being executed. This delay could be detrimental to the responsiveness of a power-monitoring tool. The implementation of our modeling technique in the Linux kernel, presented in Section V, is a fork of the PMCTrack codebase, re-engineered with the goal of providing a responsive and reliable mechanism to monitor the evolution of PMCs and, consequently, the power consumption of the platform, in usecases that include real-time tasks. Our implementation employs a mechanism based on a moving sampling window, that does not suffer from the above-mentioned low responsiveness of PMCTrack as estimates are updated at a configurable periodicity. This results in responsive power readings that do not sacrifice the accuracy of the estimates, which depends on the window size [41]. Our modeling approach includes an offline optimal selection of the counters to be employed, which are then set up in the online monitor to get accurate power estimations without requiring counter multiplexing [42]. Through the described sampling of PMCs, our online monitoring framework allows the collection of power estimates with sub-systemlevel introspection, such as individual CPU islands, and at the granularity of individual tasks."
    },
    {
      "heading": "IV. DATA-DRIVEN POWER MODELING",
      "text": "This section describes our proposed automated, data-driven pipeline for the training of a DVFS-aware power estimation model for heterogeneous platforms. While mostly based on the approach presented in [13], this work introduces a further generalization that expands its applicability from systems composed of only a CPU and a GPU subsystem to any system decomposable into multiple sub-systems. Given a generic target platform composed of one or multiple sub-systems, we provide a systematic and architecture-agnostic approach to its characterization (i.e., to model parameter selection) based on extensive profiling of the exposed PMCs. This results in an accurate, responsive, and low-overhead power model for the entire platform and its individual sub-systems."
    },
    {
      "heading": "A. The systematic, data-driven methodology",
      "text": "For the purpose of this section, we consider a generic computing platform composed of a set D of individual sub-systems d. Each sub-system d has a set F d of possible DVFS states, each one characterized by an operating frequency f d . We define D * ⊆ D as the subset of sub-systems that we target for power modeling. Furthermore, for each d ∈ D * , we define F * d ⊆ F d as the subset of d's DVFS states that we consider. Both D * and F * d are user-defined parameters that might vary based on the use-case. In addition, within a single sub-system d, there might be multiple units i running at the same frequency f d , that we would like to track and model individually (e.g., different CPU cores of the same CPU). For the purpose of this discussion, we assume that, for each sub-system's units, up to N d distinct PMCs can be tracked at the same time. For example, typical ARM CPUs simultaneously expose up to four or six individual counters, some of which may be freely selectable by the OS, while others are fixed. In the following, we refer to an individual counter exposed by the unit i of the sub-system d with x ij , with j = 1 .. N d . In general, the set of all x ij of a sub-system d operating at f d represents the set of input independent variables, or the predictors, of our models. Given our focus on heterogeneity and DVFS capabilities, we deem it impractical to obtain a single mathematical model that would accurately describe the relationship between the power consumption of the platform and PMC samples in each possible DVFS state, for any hardware sub-system. For this reason, our power modeling approach relies on the construction of a Lookup Table (LUT) able to describe the power consumption of the entire system with high accuracy and low overhead, effectively grasping the different platform behaviors at varying operating frequencies. The LUT comprises several simpler linear models, one per sub-system d and DVFS state f d that the sub-system can select. Each entry of the LUT is a linear power model The goal of our approach, depicted in Figure 1, is to obtain all the parameters and weights necessary to construct the set of individual models of the LU T . This starts from the definition of a mathematical expression for a PMC-based linear model for each sub-system d. Extensive profiling of the platform is then carried out to define the set of model parameters for each sub-system at each frequency. The models obtained in such a way are trained and validated individually and subsequently composed into the final LUT. Note that the whole procedure can be easily automated and architecture-agnostic, as long as access to representative PMCs for each sub-system is granted."
    },
    {
      "heading": "B. Analytical Model Building & Benchmarks Selection",
      "text": "Before the offline modeling phase (Figure 1, 3 ), two initial steps are required. Each sub-system d is associated with a linear power model, which will be trained using linear regression. In the analytical model building step (Figure 1, 1 ), we define such an expression P d for each sub-system d. Thanks to the LUT-based approach, the frequency f d is factored out. Therefore, the individual power models are reduced to linear combinations of the PMC samples and the trained weights. The weight L d is used to capture the constant component of the power consumption, i.e., the leakage, while the PMCdependent terms vary based on the hardware activity, modeling the dynamic power. The factor 1 /T normalizes the raw PMC samples x ij with respect to the sampling period T of the dataset traces: training the model on PMC rates rather than absolute values solves sampling jitter and simplifies time-rescaling. All subsequent steps of the procedure are also based on the careful choice of a representative set of workloads for the heterogeneous platform (Figure 1, 2 ). To obtain a proper input dataset, complete coverage of all targeted sub-systems is required to address the heterogeneity of the platform fully. Knowledge of the specific target architecture is nonetheless not required, as high-level considerations about any generic computer architecture suffice for this step. Secondly, for each sub-system, the workloads should be diverse enough to induce a broad range of different behaviors for a dataset-independent result. The selected benchmarks are employed to build a dataset (Figure 1, 6 ) for platform characterization, model training, and model validation. Such a dataset contains the activity traces of the workloads (i.e., the PMC traces), each with its associated instantaneous power consumption. The decomposability of the final power model can extend only as in-depth as the available PMCs and power measurements."
    },
    {
      "heading": "C. Platform Characterization",
      "text": "Given the P d mathematical model for each sub-system d, we define platform characterization the process of model parameter selection (Figure 1, 4 ). In other words, with the platform characterization, we define the set X d,f d of PMCs, which will be used to model the dynamic power of each sub-system d at each frequency f d . Individually for each sub-system d and frequency f d , we perform a one-time correlation analysis between all of its local PMCs and the sub-system power consumption, looking for the optimal X d,f d set in terms of model accuracy, overhead and compliance with PMU limitations. This characterization process is meant as an automatic, architecture-agnostic, and data-driven alternative to manual PMC selection, which typically requires architecture-specific considerations. Given the sub-system d, its characterization involves the following steps: 1) for each DVFS state f d ∈ F * d , profile all performance events exposed by d while tracing d's power; to track all events despite the PMU limitations, we make use of multiple passes 1 [43] (Figure 1, 5 ); 2) normalize the PMC samples with respect to the sampling periods; 3) compute a Linear Least Squares (LLS) regression of each PMC's activity trace over its related power measurements, for each f d ; events with a p-value above 0.05 are discarded as not reliable for a linear correlation; 4) individually for each f d , sort the remaining events by their Pearson Correlation Coefficient (PCC) and select the best ones that can be profiled simultaneously, i.e., compose X d,f d for all f d ∈ F * d (Figure 1, 7 and 8 ). While multiple passes are used to profile all available events during the offline platform characterization, with an online deployment in mind for our models, we require X d,f d only to contain events that can be tracked simultaneously by the PMU. To this end, it is usually enough to select, for each frequency f d , the desired number of best counters up to the PMU limit. Finally, the optimal number of performance counters with respect to model accuracy can be found by iteratively employing the estimation error results from the model evaluation step (Figure 1, 11 ). On the other hand, some platforms [43] might have stricter PMU constraints. Not only are PMCs limited in number, but some of them might be mutually exclusive: compatibility must also be considered. As documentation about performance events compatibility is not usually available, we tackle this issue with a PMU-aware iterative algorithm based on the profiling application-programming interface (API) provided by the vendor. Given the sub-system d, its frequency f d and the list of PMCs, we heuristically group the PMCs with highest 1 Given a set of performance events that cannot be thoroughly profiled simultaneously, we replay the same workload (i.e., we induce the same power consumption profile) until all events have been tracked. This is in contrast to counter multiplexing, where the PMCs are round-robin time-multiplexed. Counter multiplexing increases overhead and decreases accuracy, therefore, it is not optimal for online usage. PCC one by one, adding to X d,f d only events that, based on the provided API, can be counted in a single pass."
    },
    {
      "heading": "D. Training, Validation, and System-level Model",
      "text": "With the sets of counters X d,f d from platform characterization, we compose the LUT of Equation ( 1) by individually training the linear power model 1, 9 ). The output of each training is a set of weights W d,f d 10 . To train each individual P d (X d,f d , W d,f d ), we perform a Non-Negative Least Squares (NNLS) linear regression of the PMCs rates over the power measurements, obtaining the set of nonnegative weights W d,f d . Compared to unconstrained LLS, nonnegative weights are physically meaningful and prove to be robust to multicollinearity, which makes our simple models less prone to overfitting. We subsequently validate each individual After individual training and validation, we combine all the individual sub-system models (Figure 1, 10 ) into a systemlevel power model (Figure 1, 12 ) defined as: In other words, we fix a f d for the model of each subsystem d. The system-level power model is then the reduction sum of the LUT from Equation ( 1) along the sub-systems dimension d. Modeling the sub-systems individually until this step relieves us from profiling all possible combinations of sub-systems' frequencies, which is simpler, faster, and more robust to overfitting. The final model is decomposable, accurate, and, due to its linearity, computationally lightweight. The complete model can finally be used online (Figure 1, 13 ) to monitor the instantaneous power consumption of the entire system. In order to do so, it is enough to keep available at runtime the weights W d,f d for each (d, f d ) combination of interest, acquire the PMC measures for the set of model parameters X d,f d , and compute the few required multiply-accumulate operations to evaluate the model. A proposed framework for this operation is the object of the next section."
    },
    {
      "heading": "V. ONLINE MONITORING AND KERNEL SUPPORT",
      "text": "The modeling approach discussed in Section IV results in a fast and thorough power model that can provide accurate and introspective power estimates with low overhead. These features are ideal for applications requiring power awareness and real-time capabilities, such as resource allocation and task scheduling. To demonstrate the applicability of the proposed power modeling methodology, we implement an integrated power monitoring framework for the Linux kernel, which we name Runmeter. As a case study for such a framework, we implement power estimation and monitoring of the CPU sub-system. However, the implementation can be extended to support additional sub-systems, such as the GPU. Runmeter is a framework designed to support the runtime estimation of platform-and task-level metrics, including power and energy consumption, through PMC tracking. As such, it provides the actual means to feed the PMC values to the model presented in Section IV, forwarding its energy estimation to the Linux scheduler. Runmeter's modular design makes it highly architecture-agnostic. Only a minimal subset of its components must be re-implemented to support different target platforms. In addition, its flexibility allows easy extension for further relevant metrics to be collected at runtime, transparently to the target architecture. The Runmeter Framework consists of two main components: 1) Runmeter Kernel Patch, a patch for the Linux kernel that enables the dynamic loading of the framework into the kernel at runtime; 2) Runmeter Kernel Module, a dynamically loadable Linux kernel module that implements the bulk of Runmeter functionality. Additionally, a set of user-space tools is provided to simplify the interaction with the kernel module for configuration and periodical monitoring purposes. In the following, we focus on the description of the in-kernel components."
    },
    {
      "heading": "A. Runmeter Kernel Components",
      "text": "The Runmeter Kernel Patch applies minimal changes to the Linux kernel internals, which are otherwise inaccessible from loadable modules. The patch injects callbacks to Runmeter in key moments of task execution (namely during the context switch and the scheduler tick callbacks), which allows for collecting PMCs information as required. Once loaded into the kernel, the Runmeter Kernel Module attaches to the callbacks to trace and collect running statistics on a selection of the available PMCs. It does so by taking over PMCs management from the Linux kernel and configuring the platform to track the counters according to the result of the platform characterization (Section IV-C). Since a different set of counters X CPU,fCPU can be selected to model the evolution of the platform depending on the DVFS state f CPU , the module selects the correct PMCs to track accordingly to the model LUT. The module also subscribes to the CPU frequency governor (CPUFreq) to be notified of each change of frequency so that it can dynamically reconfigure the set of tracked PMCs for each CPU core. When tracking is enabled, the kernel module generates a new PMC sample on each CPU core whenever one of the following events occurs: • a context switch, in which case a new sample is always generated, or • a user-configurable number of scheduler ticks since the last sample was produced on that core. The first trigger is necessary so that the PMC statistics of each individual task can be queried. This allows Runmeter to collect PMC information and derive metrics with task-level granularity. The second trigger, on the other hand, provides an upper bound to the inter-arrival time between two consecutive PMC samples. This guarantees that tasks hogging the CPU do not interfere with the monitoring. Since this bound is expressed in terms of scheduler ticks, its granularity depends on the CONFIG_HZ Linux kernel option. Proper selection of this upper bound is key to ensuring the desired responsiveness when monitoring CPU counters evolving over time. The basic PMC sampling mechanism provides the accumulated value of the event counter since its last reading. Therefore, a small sampling window might negatively impact the information collected by the PMCs: since each sample is tied to a single task, it is difficult to derive any meaningful data about the overall platform status by considering exclusively a single PMC sample. On the other hand, if such an interval is long, the read-out value is not updated often. This is detrimental for actuation policies requiring high responsiveness [41]. As a trade-off, we devise a moving-window approach that decouples the PMC sampling period from their observation window. The moving window allows us to obtain PMC statistics accumulated over an arbitrarily long window and updated at an arbitrarily fine time granularity. To implement the movingwindow approach, we instantiate a window buffer for each PMC. Each buffer stores a user-configurable number of PMC samples. Whenever a new sample is produced, the window shifts forward. The value of each PMC over the whole window is tracked by summing up all samples in the buffer. This information is updated each time the window moves forward (i.e., a new sample is available). We refer to this value as synthetic PMC sample. Consuming synthetic samples provides more meaningful PMC data for the metrics to be estimated. However, it comes at the cost of the additional processing for the update of each PMC's moving window. Nevertheless, in Section VI, we report a negligible overhead."
    },
    {
      "heading": "B. In-Kernel CPU Power Model",
      "text": "Given the dense stream of synthetic PMC samples generated by the core of the Runmeter Framework, components like an online CPU power (or energy) monitor are required to reevaluate their estimates at each update of the corresponding synthetic samples. A high degree of responsiveness in the monitoring, useful for prompt actuation, is achieved when the model evaluation time can keep up with the stream of synthetic samples. In Section IV, we present a power modeling approach devised with online usage in mind, which retains high accuracy despite its low computational complexity. As a case study, the Runmeter Framework implements support for online monitoring of the instantaneous power consumption of the CPU sub-system. The CPU power monitor in Runmeter implements, for each CPU DVFS state, the following model, based on Equation (2): The weights L CPU and w ij are fractional values. Therefore, the estimation of the instantaneous power P CPU at each query of the monitoring framework encompasses fractional additions and multiplications. However, using floating-point arithmetic within the Linux kernel is problematic and expensive. For this reason, we use fixed-point arithmetic to implement the in-kernel power model. This decision is supported by the negligible loss of dynamic range and precision with respect to floating-point data type, which we evaluate in Section VI-D. The factor 1 /T ′ normalizes the value of each synthetic sample with respect to the width of the user-configured observation window T ′ . T ′ might indeed differ from the sampling period T of the model training dataset (Equation ( 2)). Thanks to the linearity of our models, we can perform the normalization by multiplying only once the result of the summations. This achieves arbitrary time-rescaling with negligible overhead. In this section, we evaluate the holistic power modeling approach discussed in Section IV, and its in-kernel implementation within the Runmeter online monitoring framework described in Section V."
    },
    {
      "heading": "A. Experimental methodology",
      "text": "The target platform for our experiments is an NVIDIA Jetson AGX Xavier, powered by the Xavier SoC [23]. It is a highly parallel and heterogeneous SoC provided with an 8-core 64-bit ARMv8.2 CPU, a 512-core NVIDIA Volta GPU, and several additional accelerators for deep-learning, computer vision, and video encoding/decoding. With many DVFS power profiles available for its sub-systems, this platform represents a challenging state-of-the-art target to explore our approach. In particular, the single CPU island on the platform can be clocked at 29 different discrete frequencies between 115 MHz and 2.3 GHz, while the GPU has 14 available DVFS states between 115 MHz and 1.4 GHz. For the evaluation of our power modeling approach, we target the CPU and GPU sub-systems, D * = {CP U, GP U } considering the following DVFS states: To build the input dataset, we profile several workloads on the target platform based on the considerations of Section IV-B. For the CPU, we employ 17 different OpenMP benchmarks from the Rodinia 3.1 heterogeneous benchmark suite [44] in several multi-thread configurations and five additional synthetic benchmarks. For the GPU, we employ ten different CUDA benchmarks from Rodinia. To average out possible interference in our measurements, such as unpredictable OS activity on the CPU, each workload is profiled 3 times. PMC samples are acquired in a continuous, periodical mode with a sampling period of 100 ms. During each sampling period, power measures of the CPU and GPU sub-systems are also acquired from the INA3221 built-in power monitors [24]. This grants the time correlation needed for an effective correlation analysis and training [45]. We find that collecting more than one sample per 100 ms does not capture any additional information due to the electrical inertia of the built-in current sensors. As mentioned in Section II, built-in power monitors are typically not robust tools for online, power-aware actuation policies. This is mainly due to their speed, coarse granularity, and low resolution, which for the Xavier is limited to about 200 mW. However, they are helpful for building datasets to achieve higher introspection, time granularity, and responsiveness enabled by PMC-based power models, as proved in Section VI."
    },
    {
      "heading": "B. Offline Platform Characterization and Modeling",
      "text": "This section discusses the result of the platform characterization and power modeling (Section IV) of the individual CPU and GPU sub-systems for the NVIDIA Jetson AGX Xavier case study. In the interest of conciseness, in the following, we only analyze the main results to support our subsequent discussion. A thorough report is available in [13]. 1) Sub-system Characterization: For the CPU sub-system, the results of the platform characterization suggest that the power consumption of the cores is highly correlated, depending on the selected DVFS state, with the number of cycles of activity, the number of retired instructions, the floating point activity, and various cache-related events. From our experiments, the counters that correlate best with the measured power consumption results are all cross-compatible. Therefore, for the power model parameter selection, we consider the three best counters for each frequency as the maximum allowed by the ARM PMU. In addition, the ARM PMU always exposes a CPU cycle counter, which we include in the final power model. For the GPU sub-system of the Xavier SoC, our results expose multiple incompatibilities among the counters that would best correlate with the power profile. To be able to simultaneously track the optimal model parameters at runtime, we employ the PMU-aware algorithm described in Section IV-C, which automatically selects the subset of cross-compatible counters that best correlate with the power consumption. We find that counters related to L2 cache utilization and warp execution best correlate with the power consumption of the GPU. By enhancing the counter selection with feedback from the GPU model validation, we find that eight PMCs per frequency is the optimal trade-off between the number of independent variables, affecting the model evaluation time and the model accuracy. 2) Sub-system Modeling and Validation: For the CPU, we train a linear model based on Equation (1) individually for each frequency, with a NNLS regression. We employ four independent variables per core, i.e., the three configurable PMCs for each frequency and the cycle counter. Out of our input dataset, we use a random selection of 70 % of the total data for training and the remaining 30 % for validation. In terms of instantaneous power accuracy, the model achieves a Mean Absolute Percentage Error (MAPE) between 3 % and 4.4 % based on the frequency, with a standard deviation of approximately 5 %. When employed to estimate the energy over the full validation set, our model achieves a maximum error of 4 %, delivering an equal or superior accuracy with respect to the state of the art. For the GPU, we likewise train the Equation (1) for each of the 14 GPU frequencies with a NNLS linear regression. We use the same 70% and 30% ratio for the training and validation set. Comparing the instantaneous power consumption estimation with the data measured on the real platform, we obtain a MAPE between 6 % and 8 %, depending on the frequency. The standard deviation over all frequencies is approximately 8 %. The maximum energy estimation error over the full validation set is 5.5 % over all frequencies, with an average of 2.2 %."
    },
    {
      "heading": "C. Combined Model Evaluation",
      "text": "After building, training, and validating the CPU and GPU power models individually, we combine them to obtain a system-level power model for every possible combination of f CPU ∈ F * CPU and f GPU ∈ F * GPU , corresponding to the LUT of Equation (1). Figure 2 shows how our decomposable power model can effectively track the instantaneous power consumption of the system over time. The achieved instantaneous power MAPE of the final, combined model has an average of 8.6 % over all CPU and GPU frequency combinations. Regarding energy, the model reaches an average estimation error of 2.5 %. However, our results highlight that the energy estimation error of the combined model is higher when f CP U and f GP U diverge from each other. In particular, when f GP U is very low compared to f CP U , the CPU may stall waiting for the offloaded computation. While our power model does not capture this behavior, a real scenario where this occurs is highly unlikely due to its inefficiency. Therefore, by considering all CPU and GPU frequencies combinations such that f GPU > 600 MHz, we report an instantaneous power MAPE of 7.5 % and energy estimation error of 1.3 %, with a maximum of 3.1 %."
    },
    {
      "heading": "D. CPU Power Monitoring with Runmeter",
      "text": "This section discusses the evaluation of our online monitoring framework, Runmeter. To evaluate Runmeter, as a case study, we integrate it in the kernel of the Linux distribution running on the NVIDIA Jetson AGX Xavier. Then, through Runmeter, we implement the power monitor discussed in Section V with support for the CPU sub-system. First, we discuss the impact of porting models such as the one derived in Section V-B from floating-point to fixed-point arithmetic, which is necessary for their in-kernel implementation. Then, we deploy the CPU sub-system model on the target platform and compare the online power estimation provided by Runmeter with the measures from the onboard analog sensors. Finally, we include an analysis of the overhead introduced to the system by Runmeter, proving that both our power modeling methodology and our monitoring framework are ideal for online policy actuation such as DPM and power-aware task scheduling. 1) Fixed-point Approximation Error: In this section, we evaluate the approximation error introduced by our fixed-point implementation of the power model described in Section V, necessary to integrate it as part of a kernel module. For the fixed-point implementation, we use 64-bit integers, assigning the 29 less significant bits to the fractional part. To analyze the approximation error, we implement the power model in user space as a generic C++ procedure, which can 0 50 100 150 200 250 300 Time [s] 2 4 6 Power [W] Measured total Estimated total 0 50 100 150 200 250 300 Time [s] 2 4 6 Measured CPU Measured GPU Estimated CPU Estimated GPU Power [W] be applied to any numeric type. From user space, we collect the data published by the Runmeter Kernel Module and feed them to the C++ power model, which evaluates it through the floating-point and the fixed-point power models to compute the approximation error. For this evaluation, we use the same validation set discussed in Section VI-B. Figure 3 shows the distribution of the approximation error between the floating-point and the fixed-point implementation. From our extensive evaluation, the maximum absolute approximation error is about 17 mW; the mean error, however, is only of about 0.17 mW. The maximum percentage error is always below 0.8 % of the power consumption estimated using floatingpoint arithmetic, with a mean error of about 0.015 %. Given the negligible magnitude of the error introduced by the fixed-point implementation, we conclude that this approximation does not impact the accuracy of the model in any meaningful way. 2) Online Power Estimation Accuracy: Employing the fixedpoint implementation of the CPU power model analyzed in the previous section, we integrate the power monitor in the Runmeter Framework. We then log the data advertised at runtime by the power model in Runmeter to later perform postmortem analysis. Therefore, differently from the discussion so far, the power estimates analyzed in this section are computed directly at runtime as soon as new PMC samples are available. Figure 4 shows the Absolute Percentage Error (APE) distribution of the energy estimation provided by the in-kernel model when compared against the value collected from the onboard analog sensor for all benchmarks. In general, the maximum APE registered over all our experiments is around 29 %, the error at 90th percentile is around 20.8 %, and the MAPE is around 9 %. The input dataset is the same as the validation set discussed in Section VI-B. It must be noted, however, that most of the estimation error accounted for during such an evaluation can be attributed to very specific time frames when the phase of the workload abruptly changes. Such behavior is visible in the example CPU power profiles depicted in Figure 5. On sharp changes of the system activity, corresponding to rapid switches of the power consumption, the power estimated by the PMCbased power model has faster rising and falling edges than the power measured by the analog sensor. This is especially visible at higher CPU frequencies, where the inertia of the analog current sensors has an increasingly higher impact on the measurements due to the faster CPU activity. On the other 0.2 0.4 0.6 0.8 16 18 0.0 0.1 0.2 0.3 0.4 0.5 Approximation Error [mW] Probability 0.1 0.2 0.3 0.4 0.5 0.02 0.04 0.08 0.0 Approximation Error [%]  hand, PMCs are embedded in the digital domain, and their values instantly reflect the dynamic behavior of the profiled workloads. Nevertheless, our power modeling approach uses the onboard power sensor to build the input dataset for training and validation. While this makes the procedure automatic, easier, and less error-prone, the error on the estimation provided by our model over time is unavoidably high during transients in which the power sensor is too slow to react. As a matter of fact, the occasionally high estimation error is strictly limited to these transient conditions, where the high responsiveness of our PMC-based model can accurately track even brief f"
    },
    {
      "heading": "VII. CONCLUSIONS AND FUTURE WORK",
      "text": "With this work, we propose a systematic, data-driven, and architecture-agnostic approach to DVFS-aware statistical power modeling of heterogeneous computing systems. Our approach individually models each sub-system through its local PMCs, autonomously selecting the best ones to represent its power consumption. The sub-system models are later re-composed in a LUT-based system-level power model, able to grasp the complex behaviors of DVFS-enabled hardware with simple, linear models. This approach achieves an unprecedented combination of general applicability, automated model construction, lightweight model evaluation, and high accuracy. In addition, we propose Runmeter, a novel integrated framework for the evaluation of the generated models online from within the Linux kernel. Runmeter is a substantial improvement over existing mechanisms based on PMC tracking as it focused on minimizing the response time between PMC observation and the evaluation of the model, without introducing any substantial overhead. The validation of our power modeling approach on the stateof-the-art NVIDIA Jetson AGX Xavier embedded platform results in power and energy estimation accuracies aligned or superior with respect to state-of-the-art works. Additionally, the model shows desirable properties of responsiveness and decomposability. By integrating the Runmeter framework in the Linux kernel of the same platform, we also prove the viability of our modeling and monitoring approach for online power tracking, a key prerequisite for implementing power-aware control loops in DPM and power-aware task scheduling. These results pave the way for further work to bring the benefits of this modeling approach to additional components in the Linux kernel, through the Runmeter framework. In particular, we plan to investigate the possible integration of our monitoring framework with the Linux real-time task scheduler (SCHED_DEADLINE), together with the CPU frequency governor, with the aim to improve the effectiveness and correctness of energy-aware real-time task scheduling within Linux. Further directions of work also include going beyond the estimation of the current hardware status through predictive models. As of now, the Linux kernel contains very simple linear models for estimating the power consumed at each frequency, used to make decisions when selecting the appropriate frequency at which the CPU should execute. Models based on online PMC data, like that collected by the Runmeter framework, may prove to be more effective from an energy-saving perspective while maintaining a very low overhead."
    }
  ],
  "figures": [],
  "equations": []
}