{
  "paperid": "QuantumComputing",
  "title": "Quantum computing",
  "authors": [
    "Steane",
    "Abrams",
    "Aharonov",
    "Aspect",
    "Dalibard",
    "Roger",
    "Barenco",
    "Barenco",
    "Bennett",
    "Cleve",
    "Divincenzo",
    "Margolus",
    "Shor",
    "Sleator",
    "Smolin",
    "Weinfurter",
    "Barenco",
    "Brun",
    "Schak",
    "Spiller",
    "Barenco",
    "Deutsch",
    "Ekert",
    "Jozsa",
    "Barenco",
    "Ekert",
    "Barenco",
    "Ekert",
    "Suominen",
    "Torma",
    "Barnum",
    "Fuchs",
    "Jozsa",
    "Schumacher",
    "Beckman",
    "Chari",
    "Devabhaktuni",
    "Preskill",
    "Bell",
    "Benioff",
    "Bennett",
    "Bennett",
    "Bernstein",
    "Brassard",
    "Vazirani",
    "Bennett",
    "Bessette",
    "Brassard",
    "Savail",
    "Smolin",
    "Bennett",
    "Brassard",
    "Bennett",
    "Brassard",
    "Briedbart",
    "Wiesner",
    "Bennett",
    "Brassard",
    "Crépeau",
    "Jozsa",
    "Peres",
    "Wootters",
    "Bennett",
    "Brassard",
    "Popescu",
    "Schumacher",
    "Smolin",
    "Wootters",
    "Bennett",
    "Divincenzo",
    "Smolin",
    "Wootters",
    "Bennett",
    "Landauer",
    "Bennett",
    "Wiesner",
    "Berman",
    "Doolen",
    "Holm",
    "Tsifrinovich",
    "Bernstein",
    "Vazirani",
    "Berthiaume",
    "Brassard",
    "Berthiaume",
    "Deutsch",
    "Jozsa",
    "Boghosian",
    "Bohm",
    "Aharonov",
    "Boyer",
    "Brassard",
    "Hoyer",
    "Tapp",
    "Brassard",
    "Brassard",
    "Crepeau",
    "Braunstein",
    "Mann",
    "Braunstein",
    "Mann",
    "Revzen",
    "Brune",
    "Nussenzveig",
    "Schmidt-Kaler",
    "Bernardot",
    "Maali",
    "Raimond",
    "Haroche",
    "Calderbank",
    "Rains",
    "Shor",
    "Calderbank",
    "Shor",
    "Caves",
    "Caves",
    "Unruh",
    "Zurek",
    "Chuang",
    "Laflamme",
    "Shor",
    "Zurek",
    "Chuang",
    "Yamamoto",
    "Church",
    "Cirac",
    "Pellizari",
    "Zoller",
    "Cirac",
    "Zoller",
    "Cirac",
    "Zoller",
    "Kimble",
    "Mabuchi",
    "Clauser",
    "Holt R A",
    "Horne",
    "Shimony",
    "Clauser",
    "Shimony",
    "Cleve",
    "Divincenzo",
    "Coppersmith",
    "Cory",
    "Fahmy",
    "Havel",
    "Crandall",
    "Deutsch",
    "Deutsch",
    "Barenco",
    "Ekert",
    "Deutsch",
    "Ekert",
    "Jozsa",
    "Macchiavello",
    "Popescu",
    "Sanpera",
    "Deutsch",
    "Jozsa",
    "Diedrich",
    "Bergquist",
    "Itano",
    "Wineland",
    "Dieks",
    "Divincenzo",
    "Divincenzo",
    "Shor",
    "Einstein",
    "Rosen",
    "Podolsky",
    "Ekert",
    "Ekert",
    "Jozsa",
    "Ekert",
    "Macchiavello",
    "Feynman",
    "Fredkin",
    "Toffoli",
    "Gershenfeld",
    "Chuang",
    "Glauber",
    "Golay",
    "Gottesman",
    "Gottesman",
    "Evslin",
    "Kakade",
    "Preskill",
    "Greenberger",
    "Horne",
    "Shimony",
    "Zeilinger",
    "Greenberger",
    "Horne",
    "Zeilinger",
    "Grover",
    "Hamming",
    "Hardy",
    "Wright",
    "Haroche",
    "Raimond",
    "Hellman",
    "Hodges",
    "Hughes",
    "Alde",
    "Dyer",
    "Luther",
    "Morgan",
    "Schauer",
    "Jones",
    "Jozsa",
    "Schumacher",
    "Keyes",
    "Landauer",
    "Kholevo",
    "Kitaev",
    "Yu",
    "Knill",
    "Laflamme",
    "Knill",
    "Laflamme",
    "Zurek",
    "Knuth",
    "Kwiat",
    "Mattle",
    "Weinfurter",
    "Zeilinger",
    "Sergienko",
    "Shih",
    "Laflamme",
    "Miquel",
    "Paz",
    "Zurek",
    "Landauer",
    "Lecerf",
    "Levitin",
    "Lidar",
    "Biham",
    "Lloyd",
    "Lo",
    "Chau",
    "Loss",
    "Divincenzo",
    "Margolus",
    "Mattle",
    "Weinfurter",
    "Kwiat",
    "Zeilinger",
    "Maxwell",
    "Mayers",
    "Menezes",
    "Van Oorschot",
    "Vanstone",
    "Mermin",
    "Meyer",
    "Minsky",
    "Miquel",
    "Paz",
    "Miquel",
    "Paz",
    "Zurek",
    "Monroe",
    "Meekhof",
    "King",
    "Itano",
    "Wineland",
    "Monroe",
    "Meekhof",
    "King",
    "Jefferts",
    "Itano",
    "Wineland",
    "Gould",
    "Myers",
    "Nielsen",
    "Chuang",
    "Palma",
    "Suominen",
    "Ekert",
    "Pellizzari",
    "Gardiner",
    "Cirac",
    "Zoller",
    "Peres",
    "Phoenix",
    "Townsend",
    "Plenio",
    "Knight",
    "Preskill",
    "Privman",
    "Vagner",
    "Kventsel",
    "Rivest",
    "Shamir",
    "Adleman",
    "Schroeder",
    "Schumacher",
    "Schumacher",
    "Nielsen",
    "Shankar",
    "Shannon",
    "Shor",
    "Shor",
    "Laflamme",
    "Simon",
    "Slepian",
    "Spiller",
    "Steane",
    "Szilard",
    "Teich",
    "Obermayer",
    "Mahler",
    "Turchette",
    "Hood",
    "Lange",
    "Mabushi",
    "Kimble",
    "Turing",
    "Unruh",
    "Van Enk",
    "Cirac",
    "Zoller",
    "Vedral",
    "Barenco",
    "Ekert",
    "Weinfurter",
    "Wheeler",
    "Zurek",
    "Wiesner",
    "Wineland",
    "Monroe",
    "Itano",
    "Leibfried",
    "Meekhof",
    "Wooters",
    "Zurek",
    "Zalka",
    "Zbinden",
    "Gautier",
    "Gisin",
    "Huttner",
    "Muller",
    "Tittle",
    "Zurek"
  ],
  "year": 2024,
  "abstract": "The subject of quantum computing brings together ideas from classical information theory, computer science, and quantum physics. This review aims to summarize not just quantum computing, but the whole subject of quantum information theory. Information can be identified as the most general thing which must propagate from a cause to an effect. It therefore has a fundamentally important role in the science of physics. However, the mathematical treatment of information, especially information processing, is quite recent, dating from the mid-20th century. This has meant that the full significance of information as a basic concept in physics is only now being discovered. This is especially true in quantum mechanics. The theory of quantum information and computing puts this significance on a firm footing, and has led to some profound and exciting new insights into the natural world. Among these are the use of quantum states to permit the secure transmission of classical information (quantum cryptography), the use of quantum entanglement to permit reliable transmission of quantum states (teleportation), the possibility of preserving quantum coherence in the presence of irreversible noise processes (quantum error correction), and the use of controlled quantum evolution for efficient computation (quantum computation). The common theme of all these insights is the use of quantum entanglement as a computational resource.It turns out that information theory and quantum mechanics fit together very well. In order to explain their relationship, this review begins with an introduction to classical information theory and computer science, including Shannon's theorem, error correcting codes, Turing machines and computational complexity. The principles of quantum mechanics are then outlined, and the Einstein, Podolsky and Rosen (EPR) experiment described. The EPR-Bell correlations, and quantum entanglement in general, form the essential new ingredient which distinguishes quantum from classical information theory and, arguably, quantum from classical physics.Basic quantum information ideas are next outlined, including qubits and data compression, quantum gates, the 'no cloning' property and teleportation. Quantum cryptography is briefly sketched. The universal quantum computer (QC) is described, based on the Church-Turing principle and a network model of computation. Algorithms for such a computer are discussed, especially those for finding the period of a function, and searching a random list. Such algorithms prove that a QC of sufficiently precise construction is not only",
  "sections": [
    {
      "heading": "",
      "text": "fundamentally different from any computer which can only manipulate classical information, but can compute a small class of functions with greater efficiency. This implies that some important computational tasks are impossible for any device apart from a QC. To build a universal QC is well beyond the abilities of current technology. However, the principles of quantum information physics can be tested on smaller devices. The current experimental situation is reviewed, with emphasis on the linear ion trap, high-Q optical cavities, and nuclear magnetic resonance methods. These allow coherent control in a Hilbert space of eight dimensions (three qubits) and should be extendable up to a thousand or more dimensions (10 qubits). Among other things, these systems will allow the feasibility of quantum computing to be assessed. In fact such experiments are so difficult that it seemed likely until recently that a practically useful QC (requiring, say, 1000 qubits) was actually ruled out by considerations of experimental imprecision and the unavoidable coupling between any system and its environment. However, a further fundamental part of quantum information physics provides a solution to this impasse. This is quantum error correction (QEC). An introduction to QEC is provided. The evolution of the QC is restricted to a carefully chosen subspace of its Hilbert space. Errors are almost certain to cause a departure from this subspace. QEC provides a means to detect and undo such departures without upsetting the quantum computation. This achieves the apparently impossible, since the computation preserves quantum coherence even though during its course all the qubits in the computer will have relaxed spontaneously many times. The review concludes with an outline of the main features of quantum information physics and avenues for future research."
    },
    {
      "heading": "Introduction",
      "text": "The science of physics seeks to ask, and find precise answers to, basic questions about why nature is as it is. Historically, the fundamental principles of physics have been concerned with questions such as 'what are things made of?' and 'why do things move as they do?' In his Principia, Newton gave very wide-ranging answers to some of these questions. By showing that the same mathamatical equations could describe the motions of everyday objects and of planets, he showed that an everyday object such as a teapot is made of essentially the same sort of stuff as a planet: the motions of both can be described in terms of their mass and the forces acting on them. Nowadays we would say that both move in such a way as to conserve energy and momentum. In this way, physics allows us to abstract from nature concepts such as energy or momentum which always obey fixed equations, although the same energy might be expressed in many different ways: for example, an electron in the large electron-positron collider at CERN, Geneva, can have the same kinetic energy as a slug on a lettuce leaf. Another thing which can be expressed in many different ways is information. For example, the two statements 'the quantum computer is very interesting' and 'l'ordinateur quantique est très intéressant' have something in common, although they share no words. The thing they have in common is their information content. Essentially the same information could be expressed in many other ways, for example by substituting numbers for letters in a scheme such as a → 97, b → 98, c → 99 and so on, in which case the English version of the above statement becomes 116 104 101 32 113 117 97 110 116 117 109 . . . . It is very significant that information can be expressed in different ways without losing its essential nature, since this leads to the possibility of the automatic manipulation of information: a machine need only be able to manipulate quite simple things like integers in order to do surprisingly powerful information processing, from document preparation to differential calculus, even to translating between human languages. We are familiar with this now, because of the ubiquitous computer, but even fifty years ago such a widespread significance of automated information processing was not foreseen. However, there is one thing that all ways of expressing information must have in common: they all use real physical things to do the job. Spoken words are conveyed by air-pressure fluctuations, written ones by arrangements of ink molecules on paper, even thoughts depend on neurons (Landauer 1991). The rallying cry of the information physicist is 'no information without physical representation!' Conversely, the fact that information is insensitive to exactly how it is expressed, and can be freely translated from one form to another, makes it an obvious candidate for a fundamentally important role in physics, like energy and momentum and other such abstractions. However, until the second half of this century, the precise mathematical treatment of information, especially information processing, was undiscovered, so the significance of information in physics was only hinted at in concepts such as entropy in thermodynamics. It now appears that information may have a much deeper significance. Historically, much of fundamental physics has been concerned with discovering the fundamental particles of nature and the equations which describe their motions and interactions. It now appears that a different programme may be equally important: to discover the ways that nature allows, and prevents, information to be expressed and manipulated, rather than particles to move. For example, the best way In this illustration the demon sets up a pressure difference by only raising the partition when more gas molecules approach it from the left than from the right. This can be done in a completely reversible manner, as long as the demon's memory stores the random results of its observations of the molecules. The demon's memory thus gets hotter. The irreversible step is not the acquisition of information, but the loss of information if the demon later clears its memory. to state exactly what can and cannot travel faster than light is to identify information as the speed-limited entity. In quantum mechanics, it is highly significant that the state vector must not contain, whether explicitly or implicitly, more information than can meaningfully be associated with a given system. Among other things this produces the wavefunction symmetry requirements which lead to Bose-Einstein and Fermi-Dirac statistics, the periodic structure of atoms, etc. The programme to re-investigate the fundamental principles of physics from the standpoint of information theory is still in its infancy. However, it already appears to be highly fruitful, and it is this ambitious programme that I aim to summarize. Historically, the concept of information in physics does not have a clear-cut origin. An important thread can be traced if we consider the paradox of Maxwell's demon of 1871 (figure 1) (see also Brillouin 1956). Recall that Maxwell's demon is a creature that opens and closes a trap door between two compartments of a chamber containing gas, and pursues the subversive policy of only opening the door when fast molecules approach it from the right, or slow ones from the left. In this way the demon establishes a temperature difference between the two compartments without doing any work, in violation of the second law of thermodynamics, and consequently permitting a host of contradictions. A number of attempts were made to exorcize Maxwell's demon (see Bennett 1987), such as arguments that the demon cannot gather information without doing work, or without disturbing (and thus heating) the gas, both of which are untrue. Some were tempted to propose that the second law of thermodynamics could indeed be violated by the actions of an 'intelligent being'. It was not until 1929 that Leo Szilard made progress by reducing the problem to its essential components, in which the demon need merely identify whether a single molecule is to the right or left of a sliding partition and its action allows a simple heat engine, called Szilard's engine, to be run. Szilard still had not solved the problem, since his analysis was unclear about whether or not the act of measurement, whereby the demon learns whether the molecule is to the left or the right, must involve an increase in entropy. A definitive and clear answer was not forthcoming, surprisingly, until a further 50 years had passed. In the intermediate years digital computers were developed, and the physical implications of information gathering and processing were carefully considered. The thermodynamic costs of elementary information manipulations were analysed by Landauer and others during the 1960s (Landauer 1961, Keyes and Landauer 1970, Keyes 1970) and those of general computations by Bennett, Fredkin, Toffoli and others during the 1970s (Bennett 1973, Toffoli 1980, Fredkin and Toffoli 1982). It was found that almost anything can in principle be done in a reversible manner, i.e. with no entropy cost at all (Bennett and Landauer 1985). Bennett (1982) made explicit the relation between this work and Maxwell's paradox by proposing that the demon can indeed learn where the molecule is in Szilard's engine without doing any work or increasing any entropy in the environment, and so obtain useful work during one stroke of the engine. However, the information about the molecule's location must then be present in the demon's memory (figure 1). As more and more strokes are performed, more and more information gathers in the demon's memory. To complete a thermodynamic cycle, the demon must erase its memory, and it is during this erasure operation that we identify an increase in entropy in the environment, as required by the second law. This completes the essential physics of Maxwell's demon; further subtleties are discussed by Zurek (1989), Caves (1990) and Caves et al (1990). The thread we just followed was instructive, but to provide a complete history of ideas relevent to quantum computing is a formidable task. Our subject brings together what are arguably two of the greatest revolutions in 20th-century science, namely quantum mechanics and information science (including computer science). The relationship between these two giants is illustrated in figure 2. Classical information theory is founded on the definition of information. A warning is in order here. Whereas the theory tries to capture much of the normal meaning of the term 'information', it can no more do justice to the full richness of that term in everyday language than particle physics can encapsulate the everyday meaning of 'charm'. 'Information' for us will be an abstract term, defined in detail in section 2.1. Much of information theory dates back to seminal work of Shannon in the 1940s (Slepian 1974). The observation that i"
    },
    {
      "heading": "Classical information theory",
      "text": "This and the next section will summarize the classical theory of information and computing. This is textbook material (Minsky 1967, Hamming 1986) but is included here since it forms a background to quantum information and computing and the article is aimed at physicists to whom the ideas may be new."
    },
    {
      "heading": "Measures of information",
      "text": "The most basic problem in classical information theory is to obtain a measure of information, that is, of amount of information. Suppose I tell you the value of a number X. How much information have you gained? That will depend on what you already knew about X. For example, if you already knew X was equal to 2, you would learn nothing, no information, from my revelation. On the other hand, if previously your only knowledge was that X was given by the throw of a die, then to learn its value is to gain information. We have met here a basic paradoxical property, which is that information is often a measure of ignorance: the information content (or 'self-information') of X is defined to be the information you would gain if you learned the value of X. If X is a random variable which has value x with probability p(x), then the information content of X is defined to be S({p(x)}) =x p(x) log 2 p(x). (1) Note that the logarithm is taken to base 2, and that S is always positive since probabilities are bounded by p(x) 1. S is a function of the probability distribition of values of X. It is important to remember this, since in what follows we will adopt the standard practice of using the notation S(X) for S({p(x)}). It is understood that S(X) does not mean a function of X, but rather the information content of the variable X. The quantity S(X) is also referred to as an entropy, for obvious reasons. If we already know that X = 2, then p(2) = 1 and there are no other terms in the sum, leading to S = 0, so X has no information content. If, on the other hand, X is given by the throw of a die, then p(x) = 1 6 for x ∈ {1, 2, 3, 4, 5, 6} so S =log 2 1 6 2.58. If X can take N different values, then the information content (or entropy) of X is maximized when the probability distribution p is flat, with every p(x) = 1/N (for example a fair die yields S 2.58, but a loaded die with p(6) = 1 2 , p(1 . . . 5) = 1 10 yields S 2.16). This is consistent with the requirement that the information (what we would gain if we learned X) is maximum when our prior knowledge of X is minimum. Thus the maximum information which could in principle be stored by a variable which can take on N different values is log 2 (N). The logarithms are taken to base 2 rather than some other base by convention. The choice dictates the unit of information: S(X) = 1 when X can take two values with equal probability. A two-valued or binary variable can thus contain one unit of information. This unit is called a bit. The two values of a bit are typically written as the binary digits 0 and 1. In the case of a binary variable, we can define p to be the probability that X = 1, then the probability that X = 0 is 1p and the information can be written as a function of p alone: ( This function is called the entropy function, 0 In what follows, the subscript 2 will be dropped on logarithms, it is assumed that all logarithms are to base 2 unless otherwise indicated. The probability that Y = y given that X = x is written p(y|x). The conditional entropy S(Y |X) is defined by where the second line is deduced using p(x, y) = p(x)p(y|x) (this is the probability that X = x and Y = y). By inspection of the definition, we see that S(Y |X) is a measure of how much information on average would remain in Y if we were to learn X. Note that S(Y |X) S(Y ) always and S(Y |X) = S(X|Y ) usually. The conditional entropy is important mainly as a stepping stone to the next quantity, the mutual information, defined by From the definition, I (X : Y ) is a measure of how much X and Y contain information about each other †. If X and Y are independent then p(x, y) = p(x)p(y) so I (X : Y ) = 0. The relationships between the basic measures of information are indicated in figure 3. The reader may like to prove as an exercise that S(X, Y ), the information content of X and Y (the information we would gain if, initially knowing neither, we learned the value of both X and Y ) satisfies S(X, Y ) = S(X) + S(Y ) -I (X : Y ). Information can disappear, but it cannot spring spontaneously from nowhere. This important fact finds mathematical expression in the data processing inequality: The symbol X → Y → Z means that X, Y and Z form a process (a Markov chain) in which Z depends on Y but not directly on X: p(x, y, z) = p(x)p(y|x)p(z|y). The content of the data processing inequality is that the 'data processor' Y can pass on to Z no more information about X than it received."
    },
    {
      "heading": "Data compression",
      "text": "Having pulled the definition of information content, equation ( 1), out of a hat, our aim is now to prove that this is a good measure of information. It is not obvious at first sight even how to think about such a task. One of the main contributions of classical information theory is to provide useful ways to think about information. We will describe a simple situation in order to illustrate the methods. Let us suppose one person, traditionally called Alice, knows the value of X and she wishes to communicate it to Bob. We restrict ourselves to the simple case that X has only two possible values: either 'yes' or 'no'. We say that Alice is a 'source' with an 'alphabet' of two symbols. Alice communicates by sending binary digits (noughts and ones) to Bob. We will measure the information content of X by counting how many bits Alice must send, on average, to allow Bob to learn X. Obviously, she could just send 0 for 'no' and 1 for 'yes', giving a 'bit rate' of one bit per X value communicated. However, what if X were an essentially random variable, except that it is more likely to be 'no' than 'yes'? (think of the output of decisions from a grant funding body, for example). In this case, Alice can communicate more efficiently by adopting the following procedure. Let p be the probability that X = 1 and 1p be the probability that X = 0. Alice waits until n values of X are available to be sent, where n will be large. The mean number of ones in such a sequence of n values is np, and it is likely that the number of ones in any given sequence is close to this mean. Suppose np is an integer, then the probability of obtaining any given sequence containing np ones is The reader should satisfy him or herself that the two sides of this equation are indeed equal: the right-hand side hints at how the argument can be generalized. Such a sequence is called a typical sequence. To be specific, we define the set of typical sequences to be all sequences such that Now, it can be shown that the probability that Alice's n values actually form a typical sequence is greater than 1-, for sufficiently large n, no matter how small is. This implies that Alice need not communicate n bits to Bob in order for him to learn n decisions. She need only tell Bob which typical sequence she has. They must agree together beforehand how the typical sequences are to be labelled: for example, they may agree to number them in order of increasing binary value. Alice just sends the label, not the sequence itself. To deduce how well this works, it can be shown that the typical sequences all have equal probability and there are 2 nH (p) of them. To communicate one of 2 nH (p) possibilities, clealy Alice must send nH (p) bits. Also, Alice cannot do better than this (i.e. send fewer bits) since the typical sequences are equiprobable: there is nothing to be gained by further manipulating the information. Therefore, the information content of each value of X in the original sequence must be H (p), which proves (1). The mathematical details skipped over in the above argument all stem from the law of large numbers, which states that, given arbitrarily small , δ for sufficiently large n, where m is the number of ones obtained in a sequence of n values. For large enough n, the number of ones m will differ from the mean np by an amount arbitrarily small compared with n. For example, in our case the noughts and ones will be distributed according to the binomial distribution where the Gaussian form is obtained in the limit n, np → ∞, with the standard deviation The above argument has already yielded a significant practical result associated with (1). This is that to communicate n values of X, we need only send nS(X) n bits down a communication channel. This idea is referred to as data compression and is also called Shannon's noiseless coding theorem. The typical sequences idea has given a means to calculate information content, but it is not the best way to compress information in practice, because Alice must wait for a large number of decisions to accumulate before she communicates anything to Bob. A better method is for Alice to accumulate a few decisions, say four, and communicate this as a single 'message' as best she can. Huffman derived an optimal method whereby Alice sends short strings to communicate the most likely messages, and longer ones to communicate the least likely messages, see table 1 for an example. The translation process is referred to as 'encoding' and 'decoding' (figure 4); this terminology does not imply any wish to keep information secret. For the case p = 1 4 Shannon's noiseless coding theorem tells us that the best possible data compression technique would communicate each message of four X values by sending on average 4H 1 4 3.245 bits. The Huffman code in table 1 gives on average 3.273 bits per message. This is quite close to the minimum, showing that practical methods like Huffman's are powerful. Data compression is a concept of great practical importance. It is used in telecommunications, for example to compress the information required to convey television pictures and data storage in computers. From the point of view of an engineer designing a communication channel, data compression can appear miraculous. Suppose we have set up a telephone link to a mountainous area, but the communication rate is not high enough Table 1. Huffman and Hamming codes. The left column shows the sixteen possible 4-bit messages, the other columns show the encoded version of each message. The Huffman code is for data compression: the most likely messages have the shortest encoded forms; the code is given for the case that each message bit is three times more likely to be zero than one. The Hamming code is an error-correcting code: every codeword differs from all the others in at least three places, therefore any single error can be corrected. The Hamming code is also linear: all the words are given by linear combinations of 1010101, 0110011, 0001111, 1111111. They satisfy the parity checks 1010101, 0110011, 0001111."
    },
    {
      "heading": "Message",
      "text": "Huffman Hamming 0000 10 0000000 0001 000 1010101 0010 001 0110011 0011 11000 1100110 0100 010 0001111 0101 11001 1011010 0110 11010 0111100 0111 1111000 1101001 1000 011 1111111 1001 11011 0101010 1010 11100 1001100 1011 111111 0011001 1100 11101 1110000 1101 111110 0100101 1110 111101 1000011 1111 1111001 0010110 to send, say, the pixels of a live video image. The old-style engineering option would be to replace the telephone link with a faster one, but information theory suggests instead the possibility of using the same link, but adding data processing at either end (data compression and decompression). It comes as a great surprise that the usefulness of a cable can thus be improved by tinkering with the information instead of the cable."
    },
    {
      "heading": "The binary symmetric channel",
      "text": "So far we have considered the case of communication down a perfect, i.e. noise-free channel. We have gained two main results of practical value: a measure of the best possible data compression (Shannon's noiseless coding theorem) and a practical method to compress data (Huffman coding). We now turn to the important question of communication in the presence of noise. As in the last section, we will analyse the simplest case in order to illustrate principles which are in fact more general. Suppose we have a binary channel, i.e. one which allows Alice to send noughts and ones to Bob. The noise-free channel conveys 0 → 0 and 1 → 1, but a noisy channel might sometimes cause 0 to become 1 and vice versa. There is an infinite variety of different types of noise. For example, the erroneous 'bit flip' 0 → 1 might be just as likely as 1 → 0 or the channel might have a tendency to 'relax' towards 0, in which case 1 → 0 happens but 0 → 1 does not. Also, such errors might occur independently from bit to bit, or occur in bursts. A very important type of noise is one which affects different bits independently, and causes both 0 → 1 and 1 → 0 errors. This is important because it captures the essential features of many processes encountered in realistic situations. If the two errors 0 → 1 and 1 → 0 are equally likely, then the noisy channel is called a 'binary symmetric channel'. The binary symmetric channel has a single parameter, p, which is the error probability per bit sent. Suppose the message sent into the channel by Alice is X, and the noisy message which Bob receives is Y . Bob is then faced with the task of deducing X as best he can from Y . If X consists of a single bit, then Bob will make use of the conditional probabilities using equations (3) and (2). Therefore, from the definition (6) of mutual information, we have Clearly, the presence of noise in the channel limits the information about Alice's X contained in Bob's received Y . Also, because of the data processing inequality, equation ( 7), Bob cannot increase his information about X by manipulating Y . However, (13) shows that Alice and Bob can communicate better if S(X) is large. The general insight is that the information communicated depends both on the source and the properties of the channel. It would be useful to have a measure of the channel alone, to tell us how well it conveys information. This quantity is called the capacity of the channel and it is defined to be the maximum possible mutual information I (X : Y ) between the input and output of the channel, maximized over all possible sources: Channel capacity is measured in units of 'bits out per symbol in' and for binary channels must lie between zero and one. It is all very well to have a definition, but ( 14) does not allow us to compare channels very easily, since we have to perform the maximization over input strategies, which is nontrivial. To establish the capacity C(p) of the binary symmetric channel is a basic problem in information theory, but fortunately this case is quite simple. From equations ( 13) and ( 14) one may see that the answer is obtained when S(X) = 1 (i.e. P (x = 0) = P (x = 1) = 1 2 )."
    },
    {
      "heading": "Error-correcting codes",
      "text": "So far we have investigated how much information gets through a noisy channel and how much is lost. Alice cannot convey to Bob more information than C(p) per symbol communicated. However, suppose Bob is busy defusing a bomb and Alice is shouting from a distance which wire to cut: she will not say 'the blue wire' just once and hope that Bob heard correctly. She will repeat the message many times and Bob will wait until he is sure to have got it right. Thus error-free communication can be achieved even over a noisy channel. In this example one obtains the benefit of reduced error rate at the sacrifice of reduced information rate. The next stage of our information theoretic programme is to identify more powerful techniques to circumvent noise (Hamming 1986, Hill 1986, Jones 1979, MacWilliams and Sloane 1977). We will need the following concepts. The set {0, 1} is considered as a group (a Galois field GF(2)) where the operations +, -, ×, ÷ are carried out modulo 2 (thus, 1 + 1 = 0). An n-bit binary word is a vector of n components, for example 011 is the vector (0, 1, 1). A set of such vectors forms a vector space under addition, since for example 011 + 101 means (0, 1, 1) + (1, 0, 1) = (0 + 1, 1 + 0, 1 + 1) = (1, 1, 0) = 110 by the standard rules of vector addition. This is equivalent to the exclusive-or operation carried out bitwise between the two binary words. The effect of noise on a word u can be expressed u → u = u + e, where the error vector e indicates which bits in u were flipped by the noise. For example, u = 1001101 → u = 1101110 can be expressed u = u + 0100011. An error correcting code C is a set of words such that where E is the set of errors correctable by C, which includes the case of no error, e = 0. To use such a code, Alice and Bob agree on which codeword u corresponds to which message, and Alice only ever sends codewords down the channel. Since the channel is noisy, Bob receives not u but u + e. However, Bob can deduce u unambiguously from u + e since by condition ( 16), no other codeword v sent by Alice could have caused Bob to receive u + e. An example error-correcting code is shown in the right-hand column of table 1. This is a [7, 4, 3] Hamming code, named after its discoverer. The notation [n, k, d] means that the codewords are n bits long, there are 2 k of them, and they all differ from each other in at least d places. Because of the latter feature, the condition ( 16) is satisfied for any error which affects at most one bit. In other words the set E of correctable errors is {0000000, 1000000, 0100000, 0010000, 0001000, 0000100, 0000010, 0000001}. Note that E can have at most 2 n-k members. The ratio k/n is called the rate of the code, since each block of n transmitted bits conveys k bits of information, thus k/n bits per bit. The parameter d is called the 'minimum distance' of the code, and is important when encoding for noise which affects successive bits independently, as in the binary symmetric channel. A code of minumum distance d can correct all errors affecting less than d/2 bits of the transmitted codeword and for independent noise this is the most likely set of errors. In fact, the probability that an n-bit word receives m errors is given by the binomial distribution (11), so if the code can correct more than the mean number of errors np, the correction is highly likely to succeed. The central result of classical information theory is that powerful error correcting codes exist. Shannon's theorem. If the rate k/n < C(p) and n is sufficiently large, there exists a binary code allowing transmission with an arbitrarily small error probability. The error probability here is the probability that an uncorrectable error occurs, causing Bob to misinterpret the received word. Shannon's theorem is highly surprising, since it implies that it is not necessary to engineer very low-noise communication channels, an expensive and difficult task. Instead, we can compensate noise by error correction coding and decoding, that is, by information processing! The meaning of Shannon's theorem is illustrated by figure 5. The main problem of coding theory is to identify codes with large rate k/n and large distance d. These two conditions are mutually incompatible, so a compromise is needed. The problem is notoriously difficult and has no general solution. To make connection with quantum error correction, we will need to mention one important concept, that of the parity check matrix. An error-correcting code is called linear if it is closed under addition, i.e. u + v ∈ C ∀u, v ∈ C. Such a code is completely specified by its parity-check matrix H , which is a set of (nk) linearly independent n-bit words satisfying H • u = 0 ∀u ∈ C. The important property is encapsulated by the following equation: ) This states that if Bob evaluates H • u for his noisy received word u = u + e, he will obtain the same answer H • e, no matter what word u Alice sent him! If this evaluation were done automatically, Bob could learn H • e, called the error syndrome, without learning u. If Bob can deduce the error e from H • e, which one can show is possible for all correctable errors, then he can correct the message (by subtracting e from it) without ever learning what it was! In quantum error correction, this is the origin of the reason one can correct a quantum state without disturbing it."
    },
    {
      "heading": "Classical theory of computation",
      "text": "We now turn to the theory of computation. This is mostly concerned with the questions 'what is computable?' and 'what resources are necessary?' The fundamental resources required for computing are a means to store and to manipulate symbols. The important questions are such things as how complicated must the symbols be, how many will we need, how complicated must the manipulations be, and how many of them will we need? The general insight is that computation is deemed hard or inefficient if the amount of resources required rises exponentially with a measure of the size of the problem to be addressed. The size of the problem is given by the amount of information required to specify the problem. Applying this idea at the most basic level, we find that a computer must be able to manipulate binary symbols, not just unary symbols †, otherwise the number of memory locations needed would grow exponentially with the amount of information to be manipulated. On the other hand, it is not necessary to work in decimal notation (10 symbols) or any other notation with an 'alphabet' of more than two symbols. This greatly simplifies computer design and analysis. To manipulate n binary symbols, it is not necessary to manipulate them all at once, since it can be shown that any transformation can be brought about by manipulating the binary symbols one at a time or in pairs. A binary 'logic gate' takes two bits x, y as inputs, and calculates a function f (x, y). Since f can be 0 or 1, and there are four possible inputs, there are 16 possible functions f . This set of 16 different logic gates is called a 'universal set', since by combining such gates in series, any transformation of n bits can be carried out. Futhermore, the action of some of the 16 gates can be reproduced by combining others, so we do not need all 16, and in fact only one, the NAND gate, is necessary (NAND is NOT AND, for which the output is 0 if and only if both inputs are 1). By concatenating logic gates, we can manipulate n-bit symbols (see figure 6). This general approach is called the network model of computation, and is useful for our purposes because it suggests the model of quantum computation which is currently most feasible experimentally. In this model, the essential components of a computer are a set of bits, many copies of the universal logic gate, and connecting wires."
    },
    {
      "heading": "Universal computer; Turing machine",
      "text": "The word 'universal' has a further significance in relation to computers. Turing showed that it is possible to construct a universal computer, which can simulate the action of any other, in the following sense. Let us write T (x) for the output of a Turing machine T (figure 7) acting on input tape x. Now, a Turing machine can be completely specified by writing down how it responds to 0 and 1 on the input tape, for every possible internal configuration of the machine (of which there are a finite number). This specification can itself be written as a binary number d[T ]. Turing showed that there exists a machine U , called a universal Turing machine, with the properties and the number of steps taken by U to simulate each step of T is only a polynomial (not exponential) function of the length of d [T ]. In other words, if we provide U with an input tape containing both a description of T and the input x, then U will compute the same function as T would have done, for any machine T , without an exponential slowdown. To complete the argument, it can be shown that other models of computation, such as the network model, are computationally equivalent to the Turing model: they permit the same Figure 7. The Turing machine. This is a conceptual mechanical device which can be shown to be capable of efficiently simulating all classical computational methods. The machine has a finite set of internal states and a fixed design. It reads one binary symbol at a time, supplied on a tape. The machine's action on reading a given symbol s depends only on that symbol and the internal state G. The action consists in overwriting a new symbol s on the current tape location, changing the state to G and moving the tape one place in direction d (left or right). The internal construction of the machine can therefore be specified by a finite fixed list of rules of the form (s, G → s , G , d). One special internal state is the 'halt' state: once in this state the machine ceases further activity. An input 'programme' on the tape is transformed by the machine into an output result printed on the tape. functions to be computed, with the same computational efficiency (see next section). Thus the concept of the univeral machine establishes that a certain finite degree of complexity of construction is sufficient to allow very general information processing. This is the fundamental result of computer science. Indeed, the power of the Turing machine and its cousins is so great that Church (1936) and Turing (1936) framed the 'Church-Turing thesis', to the effect that every function 'which would naturally be regarded as computable' can be computed by the universal Turing machine. This thesis is unproven, but has survived many attempts to find a counterexample, making it a very powerful result. To it we owe the versatility of the modern generalpurpose computer, since 'computable functions' include tasks such as word processing, process control, and so on. The QC, to be described in section 6, will throw new light on this central thesis."
    },
    {
      "heading": "Computational complexity",
      "text": "Once we have established the idea of a universal computer, computational tasks can be classified in terms of their difficulty in the following manner. A given algorithm is deemed to address not just one instance of a problem, such as 'find the square of 237', but one class of problem, such as 'given x, find its square'. The amount of information given to the computer in order to specify the problem is L = log x, i.e. the number of bits needed to store the value of x. The computational complexity of the problem is determined by the number of steps s a Turing machine must make in order to complete any algorithmic method to solve the problem. In the network model, the complexity is determined by the number of logic gates required. If an algorithm exists with s given by any polynomial function of L (e.g. s ∝ L 3 + L) then the problem is deemed tractable and is placed in the complexity class 'P'. If s rises exponentially with l (e.g. s ∝ 2 L = x) then the problem is hard and is in another complexity class. It is often easier to verify a solution, that is, to test whether or not it is correct, than to find one. The class 'NP' is the set of problems for which solutions can be verified in polynomial time. Obviously P ∈ NP, and one would guess that there are problems in NP which are not in P, (i.e. NP = P) though surprisingly the latter has never been proved, since it is very hard to rule out the possible existence of as yet undiscovered algorithms. However, the important point is that the membership of these classes does not depend on the model of computation, i.e. the physical realization of the computer, since the Turing machine can simulate any other computer with only a polynomial, rather than exponential slowdown. An important example of an intractable problem is that of factorization: given a composite (i.e. non-prime) number x, the task is to find one of its factors. If x is even, or a multiple of any small number, then it is easy to find a factor. The interesting case is when the prime factors of x are all themselves large. In this case there is no known simple method. The best known method, the number field sieve (Menezes et al 1997) requires a number of computational steps of order s ∼ exp(2L 1/3 (ln L) 2/3 ) where L = ln x. By devoting a substantial machine network to this task, one can today factor a number of 130 decimal digits (Crandall 1997), i.e. L 300, giving s ∼ 10 18 . This is time-consuming but possible (for example 42 days at 10 12 operations per second). However, if we double L, s increases to ∼ 10 25 , so now the problem is intractable: it would take a million years with current technology, or would require computers running a million times faster than current ones. The lesson is an important one: a computationally 'hard' problem is one which in practice is not merely difficult but impossible to solve. The factorization problem has acquired great practical importance because it is at the heart of widely used cyptographic systems such as that of Rivest et al (1979) (see Hellman 1979). For, given a message M (in the form of a long binary number), it is easy to calculate an encrypted version E = M s mod c where s and c are well chosen large integers which can be made public. To decrypt the message, the receiver calculates E t mod c which is equal to M for a value of t which can be quickly deduced from s and the factors of c (Schroeder 1984). In practice c = pq is chosen to be the product of two large primes p, q known only to the user who published c, so only that user can read the messages-unless someone manages to factorize c. It is a very useful feature that no secret keys need be distributed in such a system: the 'key' c, s allowing encryption is public knowledge."
    },
    {
      "heading": "Uncomputable functions",
      "text": "There is an even stronger way in which a task may be impossible for a computer. In the quest to solve some problem, we could 'live with' a slow algorithm, but what if one does not exist at all? Such problems are termed uncomputable. The most important example is the 'halting problem', a rather beautiful result. A feature of computers familiar to programmers is that they may sometimes be thrown into a never-ending loop. Consider, for example, the instruction 'while x > 2, divide x by 1' for x initially greater than 2. We can see that this algorithm will never halt, without actually running it. More interesting from a mathematical point of view is an algorithm such as 'while x is equal to the sum of two primes, add 2 to x, otherwise print x and halt', beginning at x = 8. The algorithm is certainly feasible since all pairs of primes less than x can be found and added systematically. Will such an algorithm ever halt? If so, then a counterexample to the Goldbach conjecture exists. Using such techniques, a vast section of mathematical and physical theory could be reduced to the question 'would such and such an algorithm halt if we were to run it?' If we could find a general way to establish whether or not algorithms will halt, we would have an extremely powerful mathematical tool. In a certain sense, it would solve all of mathematics! Let us suppose that it is possible to find a general algorithm which will work out whether any Turing machine will halt on any input. Such an algorithm solves the problem 'given x and d[T ], would Turing machine T halt if it were fed x as input?'. Here d[T ] is the description of T . If such an algorithm exists, then it is possible to make a Turing machine T H which halts if and only if T (d[T ]) does not halt, where d[T ] is the description of T . Here T H takes as input d[T ], which is sufficient to tell T H about both the Turing machine T and the input to T . Hence we have So far everything is okay. However, what if we feed T H the description of itself, d[T H ]? Then which is a contradiction. By this argument Turing showed that there is no automatic means to establish whether Turing machines will halt in general: the 'halting problem' is uncomputable. This implies that mathematics, and information processing in general, is a rich body of different ideas which cannot all be summarized in one grand algorithm. This liberating observation is closely related to Gödel's theorem."
    },
    {
      "heading": "Quantum versus classical physics",
      "text": "In order to think about quantum information theory, let us first state the principles of nonrelativisitic quantum mechanics, as follows (Shankar 1980). (1) The state of an isolated system Q is represented by a vector |ψ(t) in a Hilbert space. (2) Variables such as position and momentum are termed observables and are represented by Hermitian operators. The position and momentum operators X, P have the following matrix elements in the eigenbasis of X: (3) The state vector obeys the Schrödinger equation where H is the quantum Hamiltonian operator. (4) Measurement postulate. The fourth postulate, which has not been made explicit, is a subject of some debate, since quite different interpretive approaches lead to the same predictions, and the concept of 'measurement' is fraught with ambiguities in quantum mechanics (Wheeler and Zurek 1983, Bell 1987, Peres 1993). A statement which is valid for most practical purposes is that certain physical interactions are recognizably 'measurements' and their effect on the state vector |ψ is to change it to an eigenstate |k of the variable being measured, the value of k being randomly chosen with probability P ∝ | k|ψ | 2 . The change |ψ → |k can be expressed by the projection operator (|k k|)/ k|ψ . Note that according to the above equations, the evolution of an isolated quantum system is always unitary, in other words |ψ(t) = U(t)|ψ(0) where U(t) = exp(-i H dt/h) is a unitary operator, UU † = I . This is true, but there is a difficulty that there is no such thing as a truly isolated system (i.e. one which experiences no interactions with any other systems), except possibly the whole universe. Therefore there is always some approximation involved in using the Schrödinger equation to describe real systems. One way to handle this approximation is to speak of the system Q and its environment T . The evolution of Q is primarily that given by its Schrödinger equation, but the interaction between Q and T has, in part, the character of a measurement of Q. This produces a non-unitary contribution to the evolution of Q (since projections are not unitary) and this ubiquitous phenomenon is called decoherence. I have underlined these elementary ideas because they are central in what follows. We can now begin to bring together ideas of physics and information processing. It is clear that much of the wonderful behaviour we see around us in nature could be understood as a form of information processing, and conversely our computers are able to simulate, by their processing, many of the patterns of nature. The obvious, if somewhat imprecise, questions are (1) 'can nature usefully be regarded as essentially an information processor?' (2) 'could a computer simulate the whole of nature?' The principles of quantum mechanics suggest that the answer to the first quesion is yes †. For, the state vector |ψ so central to quantum mechanics is a concept very much like those of information science: it is an abstract entity which contains exactly all the information about the system Q. The word 'exactly' here is a reminder that not only is |ψ a complete description of Q, it is also one that does not contain any extraneous information which cannot meaningfully be associated with Q. The importance of this in quantum statistics of Fermi and Bose gases was mentioned in the introduction. The second question can be made more precise by converting the Church-Turing thesis into a principle of physics; every finitely realizable physical system can be simulated arbitrarily closely by a universal model computing machine operating by finite means. This statement is based on that of Deutsch (1985). The idea is to propose that a principle such as this is not derived from quantum mechanics, but rather underpins it, like other principles such as that of conservation of energy. The qualifications introduced by 'finitely realizable' and 'finite means' are important in order to state something useful. The new version of the Church-Turing thesis (now called the 'Church-Turing principle') does not refer to Turing machines. This is important because there are fundamental differences between the very nature of the Turing machine and the principles of quantum mechanics. One is described in terms of operations on classical bits, the other in terms of evolution of quantum states. Hence there is the possibility that the universal Turing machine, and hence all classical computers, might not be able to simulate some of the behaviour to be found in nature. Conversely, it may be physically possible (i.e. not ruled out by the laws of nature) to realize a new type of computation essentially different from that of classical computer science. This is the central aim of quantum computing."
    },
    {
      "heading": "EPR paradox, Bell's inequality",
      "text": "In 1935 EPR drew attention to an important feature of non-relativistic quantum mechanics. Their argument, and Bell's analysis, can now be recognized as one of the seeds from which quantum information theory has grown. The EPR paradox should be familiar to any physics graduate and I shall not repeat the argument in detail. However, the main points will provide a useful way into quantum information concepts. The EPR thought experiment can be reduced in essence to an experiment involving pairs of 2-state quantum systems (Bohm 1951, Bohm andAharonov 1957). Let us consider a pair of spin-half particles A and B, writing the (m z = + 1 2 ) spin 'up' state |↑ and the (m z = -1 2 ) spin 'down' state |↓ . The particles are prepared initially in the singlet state √ 2, and they subsequently fly apart, propagating in opposite directions along the y-axis. Alice and Bob are widely separated and they receive particle A and B respectively. EPR were concerned with whether quantum mechanics provides a complete description of the particles, or whether something was left out, some property of the spin angular momenta s A , s B which quantum theory failed to describe. Such a property has since become known as a 'hidden variable'. They argued that something was left out, because this experiment allows one to predict with certainty the result of measuring any component of s B , without causing any disturbance of B. Therefore all the components of s B have definite values, say EPR, and the quantum theory only provides an incomplete description. To make the certain prediction without disturbing B, one chooses any axis η along which one wishes to know B's angular momentum, and then measures not B but A, using a Stern-Gerlach apparatus aligned along η. Since the singlet state carries no net angular momentum, one can be sure that the corresponding measurement on B would yield the opposite result to the one obtained for A. The EPR paper is important because it is carefully argued and the fallacy is hard to unearth. The fallacy can be exposed in one of two ways: one can say either that Alice's measurement does influence Bob's particle, or (which I prefer) that the quantum state vector |φ is not an intrinsic property of a quantum system, but an expression for the information content of a quantum variable. In a singlet state there is mutual information between A and B, so the information content of B changes when we learn something about A. So far there is no difference from the behaviour of classical information, so nothing surprising has occurred. A more thorough analysis of the EPR experiment yields a big surprise. This was discovered by Bell (1964Bell ( , 1966)). Suppose Alice and Bob measure the spin component of A and B along different axes η A and η B in the x-z plane. Each measurement yields an answer + or -. Quantum theory and experiment agree that the probability for the two measurements to yield the same result is sin 2 ((φ Aφ B )/2), where φ A (φ B ) is the angle between η A (η B ) and the z axis. However, there is no way to assign local properties, that is properties of A and B independently, which lead to this high a correlation, in which the results are certain to be opposite when φ A = φ B , certain to be equal when φ A = φ B + 180 • and also, for example, have a sin 2 (60 • ) = 3 4 chance of being equal when φ Aφ B = 120 • . Feynman (1982) gives a particularly clear analysis. At φ Aφ B = 120 • the highest correlation which local hidden variables could produce is 2 3 . The Bell-EPR argument allows us to identify a task which is physically possible, but which no classical computer could perform: when repeatedly given inputs φ A , φ B at completely separated locations, respond quickly (i.e. too quick to allow light-speed communication between the locations) with yes/no responses which are perfectly correlated when φ A = φ B + 180 • , anticorrelated when φ A = φ B and more than ∼ 70% correlated when φ Aφ B = 120 • . Experimental tests of Bell's argument were carried out in the 1970s and 1980s and the quantum theory was verified (Clauser andShimony 1978, Aspect et al 1982); for more recent work see Aspect (1991), Kwiat et al (1995) and references therein. This was a significant new probe into the logical structure of quantum mechanics. The argument can be made even stronger by considering a more complicated system. In particular, for three spins prepared in a state such as (|↑ |↑ |↑ + |↓ |↓ |↓ )/ √ 2, Greenberger, Horne and Zeilinger (1989) (GHZ) showed that a single measurement along a horizontal axis for two particles, and along a vertical axis for the third, will yield with certainty a result which is the exact opposite of what a local hidden-variable theory would predict. A wider discussion and references are provided by Greenberger et al (1990), Mermin (1990). The Bell-EPR correlations show that quantum mechanics permits at least one simple task which is beyond the capabilities of classical computers and they hint at a new type of mutual information (Schumacher and Nielsen 1996). In order to pursue these ideas, we will need to construct a complete theory of quantum information."
    },
    {
      "heading": "Quantum information",
      "text": "Just as in the discussion of classical information theory, quantum information ideas are best introduced by stating them and then showing afterwards how they link together. Quantum communication is treated in a special issue of J. Mod. Opt., volume 41 (1994); reviews and references for quantum cryptography are given by Bennett et al (1992), Hughes et al (1995), Phoenix and Townsend (1995), Brassard and Crepeau (1996) and Ekert (1997). Spiller (1996) reviews both communication and computing."
    },
    {
      "heading": "Qubits",
      "text": "The elementary unit of quantum information is the qubit (Schumacher 1995). A single qubit can be envisaged as a 2-state system such as a spin-half or a 2-level atom (see figure 12), but when we measure quantum information in qubits we are really doing something more abstract: a quantum system is said to have n qubits if it has a Hilbert space of 2 n dimensions and so has available 2 n mutually orthogonal quantum states (recall that n classical bits can represent up to 2 n different things). This definition of the qubit will be elaborated in section 5.6. We will write two orthogonal states of a single qubit as {|0 , |1 }. More generally, 2 n mutually orthogonal states of n qubits can be written {|i }, where i is an nbit binary number. For example, for three qubits we have {|000 , |001 , |010 , |011 , |100 , |101 , |110 , |111 }."
    },
    {
      "heading": "Quantum gates",
      "text": "Simple unitary operations on qubits are called quantum 'logic gates' (Deutsch 1985(Deutsch , 1989)). For example, if a qubit evolves as |0 → |0 , |1 → exp(iωt)|1 , then after time t we may say that the operation, or 'gate' has been applied to the qubit, where θ = ωt. This can also be written Here are some other elementary quantum gates: These all act on a single qubit, and can be achieved by the action of some Hamiltonian in Schrödinger's equation, since they are all unitary operators †. There are an infinite number of single-qubit quantum gates, in contrast to classical information theory, where only two logic gates are possible for a single bit, namely the identity and the logical NOT operation. The quantum NOT gate carries |0 to |1 and vice versa, and so is analagous to a classical NOT. This gate is also called X since it is the Pauli σ x operator. Note that the set {I, X, Y, Z} is a group under multiplication. Of all the possible unitary operators acting on a pair of qubits, an interesting subset is those which can be written |0 0| ⊗ I + |1 1| ⊗ U, where I is the single-qubit identity operation, and U is some other single-qubit gate. Such a 2-qubit gate is called a 'controlled U ' gate, since the action I or U on the second qubit is controlled by whether the first qubit is in the state |0 or |1 . For example, the effect of controlled-NOT ('CNOT') is Here the second qubit undergoes a NOT if and only if the first qubit is in the state |1 . This list of state changes is the analogue of the truth table for a classical binary logic gate. The effect of controlled-NOT acting on a state |a |b can be written a → a, b → a ⊕ b, where ⊕ signifies the exclusive or (XOR) operation. For this reason, this gate is also called the XOR gate. Other logical operations require further qubits. For example, the AND operation is achieved by use of the 3-qubit 'controlled-controlled-NOT' gate, in which the third qubit experiences NOT if and only if both the others are in the state |1 . This gate is named a Toffoli gate, after Toffoli (1980) who showed that the classical version is universal for classical reversible computation. The effect on a state In other words if the third qubit is prepared in |0 then this gate computes the AND of the first two qubits. The use of three qubits is necessary in order to permit the whole operation to be unitary and thus allowed in quantum-mechanical evolution. It is an amusing excercise to find the combinations of gates which perform elementary arithmetical operations such as binary addition and multiplication. Many basic constructions are given by Barenco et al (1995b), further general design considerations are discussed by Vedral et al (1996) and Beckman et al (1996). The action of a sequence of quantum gates can be written in operator notation, for example X 1 H 2 XOR 1,3 |φ where |φ is some state of three qubits and the subscripts on the operators indicate to which qubits they apply. However, once more than a few quantum gates are involved, this notation is rather obscure and can usefully be replaced by a diagram known as a quantum network-see figure 8. These diagrams will be used hereafter. Each horizontal line represents one qubit evolving in time from left to right. A symbol on one line represents a single-qubit gate. Symbols on two qubits connected by a vertical line represent a 2-qubit gate operating on those two qubits. The network shown carries out the operation X 1 H 2 XOR 1,3 |φ . The ⊕ symbol represents X (NOT), the encircled H is the H gate, the filled circle linked to ⊕ is controlled-NOT."
    },
    {
      "heading": "No cloning",
      "text": "No cloning theorem. An unknown quantum state cannot be cloned. This states that it is impossible to generate copies of a quantum state reliably, unless the state is already known (i.e. unless there exists classical information which specifies it). Proof: to generate a copy of a quantum state |α , we must cause a pair of quantum systems to undergo the evolution U( |α|0 ) = |α |α where U is the unitary evolution operator. If this is to work for any state, then U must not depend on α, and therefore U(|β |0 ) = |β |β for |β = |α . However, if we consider the state |γ = (|α + |β )/ √ 2, we have U(|γ |0 ) = (|α |α + |β |β )/ √ 2 = |γ |γ so the cloning operation fails. This argument applies to any purported cloning method (Wooters andZurek 1982, Dieks 1982). Note that any given 'cloning' operation U can work on some states (|α and |β in the above example) though since U is trace-preserving, two different clonable states must be orthogonal, α| β = 0. Unless we already know that the state to be copied is one of these states, we cannot guarantee that the chosen U will correctly clone it. This is in contrast to classical information, where machines like photocopiers can easily copy whatever classical information is sent to them. The controlled-NOT or XOR operation of equation ( 28) is a copying operation for the states |0 and |1 , but not for states such as |+ ≡ (|0 + |1 )/ √ 2 and |-≡ (|0 -|1 )/ √ 2. The no-cloning theorem and the EPR paradox together reveal a rather subtle way in which non-relativistic quantum mechanics is a consistent theory. For, if cloning were possible, then EPR correlations could be used to communicate faster than light, which leads to a contradiction (an effect preceding a cause) once the principles of special relativity are taken into account. To see this, observe that by generating many clones, and then measuring them in different bases, Bob could deduce unambiguously whether his member of an EPR pair is in a state of the basis {|0 , |1 } or of the basis {|+ , |-}. Alice would communicate instantaneously by forcing the EPR pair into one basis or the other through her choice of measurement axis (Glauber 1986)."
    },
    {
      "heading": "Quantum entanglement is an information resource.",
      "text": "Qubits can be used to store and transmit classical information. To transmit a classical bit string 00101, for example, Alice can send five qubits prepared in the state |00101 . The receiver Bob can extract the information by measuring each qubit in the basis {|0 , |1 } (i.e. these are the eigenstates of the measured observable). The measurement results yield the classical bit string with no ambiguity. No more than one classical bit can be communicated for each qubit sent. Suppose now that Alice and Bob are in possession of an entangled pair of qubits, in the state |00 + |11 (we will usually drop normalization factors such as √ 2 from now on, to keep the notation uncluttered). Alice and Bob need never have communicated: we imagine a mechanical central facility generating entangled pairs and sending one qubit to each of Alice and Bob, who store them (see figure 9(a)). In this situation, Alice can communicate two classical bits by sending Bob only one qubit (namely her half of the entangled pair). This idea due to Wiesner (Bennett and Wiesner 1992) is called 'dense coding', since only one quantum bit travels from Alice to Bob in order to convey two classical bits. Two quantum bits are involved, but Alice only ever sees one of them. The method relies on the following fact: the four mutually orthogonal states |00 + |11 , |00 -|11 , |01 + |10 , |01 -|10 can be generated from each other by operations on a single qubit. This set of states is called the Bell basis, since they exhibit the strongest possible Bell-EPR correlations (Braunstein et al 1992). Starting from |00 + |11 , Alice can generate any of the Bell basis states by operating on her qubit with one of the operators {I, X, Y, Z}. Since there are four possibilities, her choice of operation represents two bits of classical information. She then sends her qubit to Bob, who must deduce which Bell basis state the qubits are in. This he does by operating on the pair with the XOR gate and measuring the target bit, thus distinguishing |00 ±|11 from |01 ±|10 . To find the sign in the superposition, he operates with H on the remaining qubit and measures it. Hence Bob obtains two classical bits with no ambiguity. Dense coding is difficult to implement and so has no practical value merely as a standard communication method. However, it can permit secure communication: the qubit sent by Alice will only yield the two classical information bits to someone in possession of the entangled partner qubit. More generally, dense coding is an example of the statement which began this section. It reveals a relationship between classical information, qubits, and the information content of quantum entanglement (Barenco and Ekert 1995). A laboratory demonstration of the main features is described by Mattle et al (1996), Weinfurter (1994) and Braunstein and Mann (1995) discuss some of the methods employed, based on a source of EPR photon pairs from parametric down-conversion."
    },
    {
      "heading": "Quantum teleportation",
      "text": "It is possible to transmit qubits without sending qubits! Suppose Alice wishes to communicate to Bob a single qubit in the state |φ . If Alice already knows what state she has, for example |φ = |0 , she can communicate it to Bob by sending just classical information, e.g. 'Dear Bob, I have the state |0 . Regards, Alice.' However, if |φ is unknown there is no way for Alice to learn it with certainty: any measurement she may perform may change the state and she cannot clone it and measure the copies. Hence it appears that the only way to transmit |φ to Bob is to send him the physical qubit (i.e. the electron or atom or whatever), or possibly to swap the state into another quantum system and send that. In either case a quantum system is transmitted. Quantum teleportation (Bennett et al 1993, Bennett 1995) permits a way around this limitation. As in dense coding, we will use quantum entanglement as an information resource. Suppose Alice and Bob possess an entangled pair in the state |00 + |11 . Alice wishes to transmit to Bob a qubit in an unknown state |φ . Without loss of generality, we can write |φ = a|0 + b|1 where a and b are unknown coefficients. Then the initial state of all three qubits is a|000 + b|100 + a|011 + b|111 . (29) Alice now measures in the Bell basis the first two qubits, i.e. the unknown one and her member of the entangled pair. The network to do this is shown in figure 9(b). After Alice has applied the XOR and Hadamard gates, and just before she measures her qubits, the state is Alice's measurements collapse the state onto one of four different possibilities, and yield two classical bits. The two bits are sent to Bob, who uses them to learn which of the operators {I, X, Z, Y } he must apply to his qubit in order to place it in the state a|0 + b|1 = |φ . Thus Bob ends up with the qubit (i.e. the quantum information, not the actual quantum system) which Alice wished to transmit. Note that the quantum information can only arrive at Bob if it disappears from Alice (no cloning). Also, quantum information is complete information: |φ is the complete description of Alice's qubit. The use of the word 'teleportation' draws attention to these two facts. Teleportation becomes an especially important idea when we come to consider communication in the presence of noise, section 9."
    },
    {
      "heading": "Quantum data compression",
      "text": "Having introduced the qubit, we now wish to show that it is a useful measure of quantum information content. The proof of this is due to Jozsa and Schumacher (1994) and Schumacher (1995), building on work of Kholevo (1973) and Levitin (1987). To begin the argument, we first need a quantity which expresses how much information you would gain if you were to learn the quantum state of some system Q. A suitable quantity is the Von Neumann entropy where Tr is the trace operation and ρ is the density operator describing an ensemble of states of the quantum system. This is to be compared with the classical Shannon entropy, equation ( 1). Suppose a classical random variable X has a probability distribution p(x). If a quantum system is prepared in a state |x dictated by the value of X, then the density matrix is x p(x)|x x|, where the states |x need not be orthogonal. It can be shown (Kholevo 1973, Levitin 1987) that S(ρ) is an upper limit on the classical mutual information I (X : Y ) between X and the result Y of a measurement on the system. To make a connection with qubits, we consider the resources needed to store or transmit the state of a quantum system q of density matrix ρ. The idea is to collect n 1 such systems, and transfer ('encode') the joint state into some smaller system. The smaller system is transmitted down the channel and at the receiving end the joint state is 'decoded' into n systems q of the same type as q (see figure 9(c)). The final density matrix of each q is ρ and the whole process is deemed successful if ρ is sufficiently close to ρ. The measure of the similarity between two density matrices is the fidelity defined by This can be interpreted as the probability that q passes a test which ascertained if it was in the state ρ. When ρ and ρ are both pure states, |φ φ| and |φ φ |, the fidelity is none other than the familiar overlap: Our aim is to find the smallest transmitted system which permits f = 1for 1. The argument is analogous to the 'typical sequences' idea used in section 2.2. Restricting ourselves for simplicity to 2-state systems, the total state of n systems is represented by a vector in a Hilbert space of 2 n dimensions. However, if the von Neumann entropy S(ρ) < 1 then it is highly likely (i.e. tends to certainty in the limit of large n) that, in any given realization, the state vector actually falls in a typical subspace of Hilbert space. Schumacher and Jozsa showed that the dimension of the typical subspace is 2 nS (ρ) . Hence only nS(ρ) qubits are required to represent the quantum information faithfully and the qubit (i.e. the logarithm of the dimensionality of Hilbert space) is a useful measure of quantum information. Furthermore, the encoding and decoding operation is 'blind': it does not depend on knowledge of the exact states being transmitted. Schumacher and Josza's result is powerful because it is general: no assumptions are made about the exact nature of the quantum states involved. In particular, they need not be orthogonal. If the states to be transmitted were mutually orthogonal, the whole problem would reduce to one of classical information. The 'encoding' and 'decoding' required to achieve such quantum data compression and decompression is technologically very demanding. It cannot at present be done at all using photons. However, it is the ultimate compression allowed by the laws of physics. The details of the required quantum networks have been deduced by Cleve and DiVincenzo (1996). As well as the essential concept of information, other classical ideas such as Huffman coding have their quantum counterparts. Furthermore, Schumacher and Nielson (1996) derive a quantity which they call 'coherent information' which is a measure of mutual information for quantum systems. It includes that part of the mutual information between entangled systems which cannot be accounted for classically. This is a helpful way to understand the Bell-EPR correlations."
    },
    {
      "heading": "Quantum cryptography",
      "text": "No overview of quantum information is complete without a mention of quantum cryptography. This area stems from an unpublished paper of Wiesner written around 1970 (Wiesner 1983). It includes various ideas whereby the properties of quantum systems are used to achieve useful cryptographic tasks, such as secure (i.e. secret) communication. The subject may be divided into quantum key distribution and a collection of other ideas broadly related to bit commitment. Quantum key distribution will be outlined below. Bit commitment refers to the scenario in which Alice must make some decision, such as a vote, in such a way that Bob can be sure that Alice fixed her vote before a given time, but where Bob can only learn Alice's vote at some later time which she chooses. A classical, cumbersome method to achieve bit commitment is for Alice to write down her vote and place it in a safe which she gives to Bob. When she wishes Bob, later, to learn the information, she gives him the key to the safe. A typical quantum protocol is a carefully constructed variation on the idea that Alice provides Bob with a prepared qubit, and only later tells him in what basis it was prepared. The early contributions to the field of quantum cryptography were listed in the introduction, further references may be found in the reviews mentioned at the beginning of this section. Cryptography has the unusual feature that it is not possible to prove by experiment that a cryptographic procedure is secure: who knows whether a spy or cheating person managed to beat the system? Instead, the users' confidence in the methods must rely on mathematical proofs of security and it is here that much important work has been done. There is now strong evidence that proofs can be established for the security of correctly implemented quantum key distribution. However, the bit commitment idea, long thought to be secure through quantum methods, was recently proved to be insecure (Mayers 1997, Lo andChau 1997) because the participants can cheat by making use of quantum entanglement. Quantum key distribution is a method in which quantum states are used to establish a random secret key for cryptography. The essential ideas are as follows: Alice and Bob are, as usual, widely seperated and wish to communicate. Alice sends to Bob 2n qubits, each prepared in one of the states |0 , |1 , |+ , |-, randomly chosen †. Bob measures his received bits, choosing the measurement basis randomly between {|0 , |1 } and {|+ , |-}. Next, Alice and Bob inform each other publicly (i.e. anyone can listen in) of the basis they used to prepare or measure each qubit. They find out on which occasions they by chance used the same basis, which happens on average half the time and retain just those results. In the absence of errors or interference, they now share the same random string of n classical bits (they agree for example to associate |0 and |+ with 0; |1 and |with 1). This classical bit string is often called the raw quantum transmission, RQT. So far nothing has been gained by using qubits. The important feature is, however, that it is impossible for anyone to learn Bob's measurement results by observing the qubits en route, without leaving evidence of their presence. The crudest way for an eavesdropper Eve to attempt to discover the key would be for her to intercept the qubits and measure them, then pass them on to Bob. On average half the time Eve guesses Alice's basis correctly and thus does not disturb the qubit. However, Eve's correct guesses do not coincide with Bob's, so Eve learns the state of half of the n qubits which Alice and Bob later decide to trust and disturbs the other half, for example sending to Bob |+ for Alice's |0 . Half of those disturbed will be projected by Bob's measurement back onto the original state sent by Alice, so overall Eve corrupts n/4 bits of the RQT. Alice and Bob can now detect Eve's presence simply by randomly choosing n/2 bits of the RQT and announcing publicly the values they have. If they agree on all these bits, then they can trust that no eavesdropper was present, since the probability that Eve was present and they happened to choose n/2 uncorrupted bits is ( 3 4 ) n/2 10 -125 for n = 1000. The n/2 undisclosed bits form the secret key. In practice the protocol is more complicated since Eve might adopt other strategies (e.g. not intercept all the qubits) and noise will currupt some of the qubits even in the absence of an eavesdropper. Instead of rejecting the key if many of the disclosed bits differ, Alice and Bob retain it as long as they find the error rate to be well below 25%. They then process the key in two steps. The first is to detect and remove errors, which is done by publicly comparing parity checks on publicly chosen random subsets of the bits, while discarding bits to prevent increasing Eve's information. The second step is to decrease Eve's knowledge of the key, by distilling from it a smaller key, composed of parity values calculated from the original key. In this way a key of around n/4 bits is obtained, of which Eve probably knows less than 10 -6 of one bit (Bennett et al 1992). The protocol just described is not the only one possible. Another approach (Ekert 1991) involves the use of EPR pairs, which Alice and Bob measure along one of three different axes. To rule out eavesdropping they check for Bell-EPR correlations in their results. The great thing about quantum key distribution is that it is feasible with current technology. A pioneering experiment (Bennett and Brassard 1989) demonstrated the principle, and much progress has been made since then. Hughes et al (1995) and Phoenix and Townsend (1995) summarized the state of affairs two years ago and recently Zbinden et al (1997) have reported excellent key distribution through 23 km of standard telecom fibre under lake Geneva. The qubits are stored in the polarization states of laser pulses, i.e. coherent states of light, with on average 0.1 photons per pulse. This low light level is necessary so that pulses containing more than one photon are unlikely. Such pulses would provide duplicate qubits, and hence a means for an evesdropper to go undetected. The system achieves a bit error rate of 1.35%, which is low enough to guarantee privacy in the full protocol. The data transmission rate is rather low: MHz as opposed to the GHz rates common in classical communications, but the system is very reliable. Such spectacular experimental mastery is in contrast to the subject of the next section."
    },
    {
      "heading": "The universal quantum computer",
      "text": "We now have sufficient concepts to understand the jewel at the heart of quantum information theory, namely, the quantum computer (QC). Ekert and Jozsa (1996) and Barenco (1996) give introductory reviews concentrating on the QC and factorization; a review with emphasis on practicalities is provided by Spiller (1996). Introductory material is also provided by DiVincenzo (1995b) and Shor (1996). The QC is first and foremost a machine which is a theoretical construct, like a thought experiment, whose purpose is to allow quantum information processing to be formally analysed. In particular it establishes the Church-Turing principle introduced in section 4. A prescription for a QC follows, based on that of Deutsch (1985Deutsch ( , 1989)). A QC is a set of n qubits in which the following operations are experimentally feasible. (1) Each qubit can be prepared in some known state |0 . (2) Each qubit can be measured in the basis {|0 , |1 }. (3) A universal quantum gate (or set of gates) can be applied at will to any fixed-size subset of the qubits. (4) The qubits do not evolve other than via the above transformations. This prescription is incomplete in certain technical ways to be discussed, but it encompasses the main ideas. The model of computation we have in mind is a network model, in which logic gates are applied sequentially to a set of bits (here, quantum bits). In an electronic classical computer, logic gates are spread out in space on a circuit board, but in the QC we typically imagine the logic gates to be interactions turned on and off in time, with the qubits at fixed positions, as in a quantum network diagram (figures 8 and 12). Other models of quantum computation can be conceived, such as a cellular-automaton model (Margolus 1986(Margolus , 1990))."
    },
    {
      "heading": "Universal gate",
      "text": "The universal quantum gate is the quantum equivalent of the classical universal gate, namely a gate which by its repeated use on different combinations of bits can generate the action of any other gate. What is the set of all possible quantum gates, however? To answer this, we appeal to the principles of quantum mechanics (Schrödinger's equation) and answer that since all quantum evolution is unitary, it is sufficient to be able to generate all unitary transformations of the n qubits in the computer. This might seem a tall order, since we have a continuous and therefore infinite set. However, it turns out that quite simple quantum gates can be universal, as Deutsch showed in 1985. The simplest way to think about universal gates is to consider the pair of gates V (θ, φ) and controlled-not (or XOR), where V (θ, φ) is a general rotation of a single qubit, i.e."
    },
    {
      "heading": "V (θ, φ)",
      "text": "It can be shown that any n × n unitary matrix can be formed by composing 2-qubit XOR gates and single-qubit rotations. Therefore, this pair of operations is universal for quantum computation. A purist may argue that V (θ, φ) is an infinite set of gates since the parameters θ and φ are continuous, but it suffices to choose two particular irrational angles for θ and φ, and the resulting single gate can generate all single-qubit rotations by repeated application; however, a practical system need not use such laborious methods. The XOR and rotation operations can be combined to make a controlled rotation which is a single universal gate. Such universal quantum gates were discussed by Deutsch et al (1995), Lloyd (1995), DiVincenzo (1995a) and Barenco (1995). It is remarkable that 2-qubit gates are sufficient for quantum computation. This is why the quantum gate is a powerful and important concept."
    },
    {
      "heading": "Church-Turing principle",
      "text": "Having presented the QC, it is necessary to argue for its universality, i.e. that it fulfils the Church-Turing principle as claimed. The two-step argument is very simple. First, the state of any finite quantum system is simply a vector in Hilbert space, and therefore can be represented to arbitrary precision by a finite number of qubits. Second, the evolution of any finite quantum system is a unitary transformation of the state and therefore can be simulated on the QC, which can generate any unitary transformation with arbitrary precision. A point of principle is raised by Myers (1997), who points out that there is a difficulty with computational tasks for which the number of steps for completion cannot be predicted. We cannot in general observe the QC to find out if it has halted, in contrast to a classical computer. However, we will only be concerned with tasks where either the number of steps is predictable, or the QC can signal completion by setting a dedicated qubit which is otherwise not involved in the computation (Deutsch 1985). This is a very broad class of problems. Nielsen and Chuang (1997) consider the use of a fixed quantum gate array, showing that there is no array which, operating on qubits representing both data and program, can perform any unitary transformation on the data. However, we consider a machine in which a classical computer controls the quantum gates applied to a quantum register, so any gate array can be 'ordered' by a classical program to the classical computer. The QC is certainly an interesting theoretical tool. However, there hangs over it a large and important question mark: what about imperfection? The prescription given above is written as if measurements and gates can be applied with arbitrary precision, which is unphysical, as is the fourth requirement (no extraneous evolution). The prescription can be made realistic by attaching to each of the four requirements a statement about the degree of allowable imprecision. This is a subject of ongoing research and we will take it up in section 9. Meanwhile, let us investigate more specifically what a sufficiently well made QC might do."
    },
    {
      "heading": "Quantum algorithms",
      "text": "It is well known that classical computers are able to calculate the behaviour of quantum systems, so we have not yet demonstrated that a QC can do anything which a classical computer cannot. Indeed, since our theories of physics always involve equations which we can write down and manipulate, it seems highly unlikely that quantum mechanics, or any future physical theory, would permit computational problems to be addressed which are not in principle solvable on a large enough classical Turing machine. However, as we saw in section 3.2, those words 'large enough' and also 'fast enough', are centrally important in computer science. Problems which are computationally 'hard' can be impossible in practice. In technical language, while quantum computing does not enlarge the set of computational problems which can be addressed (compared with classical computing), it does introduce the possibility of new complexity classes. Put more simply, tasks for which classical computers are too slow may be solvable with QCs."
    },
    {
      "heading": "Simulation of physical systems",
      "text": "The first and most obvious application of a QC is that of simulating some other quantum system. To simulate a state vector in a 2 n -dimensional Hilbert space, a classical computer needs to manipulate vectors containing of order 2 n complex numbers, whereas a QC requires just n qubits, making it much more efficient in storage space. To simulate evolution, in general both the classical and QCs will be inefficient. A classical computer must manipulate matrices containing of order 2 2n elements, which requires a number of operations (multiplication, addition) exponentially large in n, while a QC must build unitary operations in 2 n -dimensional Hilbert space, which usually requires an exponentially large number of elementary quantum logic gates. Therefore the QC is not guaranteed to simulate every physical system efficiently. However, it can be shown that it can simulate a large class Figure 10. Quantum network for Shor's period-finding algorithm. Here each horizontal line is a quantum register rather than a single qubit. The circles at the left represent the preparation of the input state |0 . The encircled ft represents the Fourier transform (see text), and the box linking the two registers represents a network to perform U f . The algorithm finishes with a measurement of the x regisiter. of quantum systems efficiently, including many for which there is no efficient classical algorithm, such as many-body systems with local interactions (Lloyd 1996, Zalka 1996, Wiesner 1996, Meyer 1997, Lidar and Biham 1997, Abrams and Lloyd 1997, Boghosian and Taylor 1997)."
    },
    {
      "heading": "Period finding and Shor's factorization algorithm",
      "text": "So far we have discussed simulation of nature, which is a rather restricted type of computation. We would like to let the QC loose on more general problems, but it has so far proved hard to find ones on which it performs better than classical computers. However, the fact that there exist such problems at all is a profound insight into physics and has stimulated much of the recent interest in the field. Currently one of the most important quantum algorithms is that for finding the period of a function. Suppose a function f (x) is periodic with period r, i.e. f (x) = f (x + r). Suppose further that f (x) can be efficiently computed from x, and all we know initially is that N/2 < r < N for some N . Assuming there is no analytic technique to deduce the period of f (x), the best we can do on a classical computer is to calculate f (x) for of order N/2 values of x, and find out when the function repeats itself (for well-behaved functions only O( √ N) values may be needed on average). This is inefficient since the number of operations is exponential in the input size log N (the information required to specify N ). The task can be solved efficiently on a QC by the elegant method shown in figure 10, due to Shor (1994), building on Simon (1994). The QC requires 2n qubits, plus a further 0(n) for workspace, where n = 2 log N (the notation x means the nearest integer greater than x). These are divided into two 'registers', each of n qubits. They will be referred to as the x and y registers; both are initially prepared in the state |0 (i.e. all n qubits in states |0 ). Next, the operation H is applied to each qubit in the x register, making the total state where w = 2 n . This operation is referred to as a Fourier transform in figure 10, for reasons that will shortly become apparent. The notation |x means a state such as |0011010 , where 0011010 is the integer x in binary notation. In this context the basis {|0 , |1 } is referred to as the 'computational basis'. It is convenient (though not of course necessary) to use this basis when describing the computer. Next, a network of logic gates is applied to both x and y regisiters, to perform the transformation U f |x |0 = |x |f (x) . Note that this transformation can be unitary because the input state |x |0 is in one to one correspondance with the output state |x |f (x) , so the process is reversible. Now, applying U f to the state given in equation ( 34), we obtain This state is illustrated in figure 11(a). At this point something rather wonderful has taken place: the value of f (x) has been calculated for w = 2 n values of x, all in one go! This feature is referred to as quantum parallelism and represents a huge parallelism because of the exponential dependence on n (imagine having 2 100 , i.e. 1000 000 times Avagadro's number, of classical processors!) Although the 2 n evaluations of f (x) are in some sense 'present' in the quantum state in equation ( 35), unfortunately we cannot gain direct access to them since a measurement (in the computational basis) of the y register, which is the next step in the algorithm, will only reveal one value of f (x) †. Suppose the value obtained is f (x) = u. The y register state collapses onto |u , and the total state becomes where d u + jr, for j = 0, 1, 2 . . . M -1, are all the values of x for which f (x) = u. In other words the periodicity of f (x) means that the x register remains in a superposition of M w/r states, at values of x separated by the period r. Note that the offset d u of the set of x values depends on the value u obtained in the measurement of the y register. It now remains to extract the periodicity of the state in the x register. This is done by applying a Fourier transform and then measuring the state. The discrete Fourier transform employed is the following unitary process: Note that equation ( 34) is an example of this, operating on the initial state |0 . The quantum network to apply U FT is based on the fast Fourier transform algorithm (see, e.g. Knuth (1981)). The quantum version was worked out by Coppersmith (1994) and Deutsch (1994, unpublished) independently, a clear presentation may also be found in Ekert and Josza (1996), Barenco (1996) †. Before applying U FT to equation (36) we will make the simplifying assumption that r divides w exactly, so M = w/r. The essential ideas are not affected by this restriction; when it is relaxed some added complications must be taken into account (Shor 1994, 1995a, Ekert and Josza 1996). The y register no longer concerns us, so we will just consider the x state from equation ( 36): where This state is illustrated in figure 11(b). The final state of the x register is now measured and we see that the value obtained must be a multiple of w/r. It remains to deduce r from this. We have x = λw/r where λ is unknown. If λ and r have no common factors, then we cancel x/w down to an irreducible fraction and thus obtain λ and r. If λ and r have a common factor, which is unlikely for large r, then the algorithm fails. In this case, the whole algorithm must be repeated from the start. After a number of repetitions no greater than ∼ log r, and usually much less than this, the probability of success can be shown to be arbitrarily close to 1 (Ekert and Josza 1996). The quantum period-finding algorithm we have described is efficient as long as U f , the evaluation of f (x), is efficient. The total number of elementary logic gates required is a polynomial rather than exponential function of n. As was emphasized in section 3.2, this makes all the difference between tractable and intractable in practice, for sufficiently large n. To add the icing on the cake, it can be remarked that the important factorization problem mentioned in section 3.2 can be reduced to one of finding the period of a simple function. This and all the above ingredients were first brought together by Shor (1994), who thus showed that the factorization problem is tractable on an ideal QC. The function to be evaluated in this case is f (x) = a x mod N where N is the number to be factorized, and a < N is chosen randomly. One can show using elementary number theory (Ekert and Josza 1996) that for most choices of a, the period r is even and a r/2 ± 1 shares a common factor with N. The common factor (which is of course a factor N ) can then be deduced rapidly using a classical algorithm due to Euclid (about 300 BC; see, e.g. Hardy and Wright 1979). To evaluate f (x) efficiently, repeated squaring (modulo N) is used, giving powers ((a 2 ) 2 ) 2 . . .. Such selected powers of a, corresponding to the binary expansion of a, are then multiplied together. Complete networks for the whole of Shor's algorithm were described by Miquel et al (1996), Vedral et al (1996) and Beckman et al (1996). They require of order 300(log N) 3 logic gates. Therefore, to factorize numbers of order 10 130 , i.e. at the limit of current classical methods, would require ∼ 2 × 10 10 gates per run, or 7 h if the 'switching rate' is one megaHertz †. Considering how difficult it is to make a QC, this offers no advantage over classical computation. However, if we double the number of digits to 260 then the problem is intractable classically (see section 3.2), while the ideal QC takes just eight times longer than before. The existence of such a powerful method is an exciting and profound new insight into quantum theory. The period-finding algorithm appears at first sight like a conjuring trick: it is not quite clear how the QC managed to produce the period like a rabbit out of a hat. Examining figure 11 and equations ( 34)-( 38), I would say that the most important features are contained in equation ( 35). They are not only the quantum parallelism already mentioned, but also quantum entanglement and, finally, quantum interference. Each value of f (x) retains a link with the value of x which produced it, through the entanglement of the x and y registers in equation ( 35). The 'magic' happens when a measurement of the y register produces the special state |ψ (equation ( 36)) in the x register, and it is quantum entanglement which permits this (see also Jozsa 1997a). The final Fourier transform can be regarded as an interference between the various superposed states in the x register (compare with the action of a diffraction grating). Interference effects can be used for computational purposes with classical light fields, or water waves for that matter, so interference is not in itself the essentially quantum feature. Rather, the exponentially large number of interfering states, and the entanglement, are features which do not arise in classical systems."
    },
    {
      "heading": "Grover's search algorithm",
      "text": "Despite considerable efforts in the quantum computing community, the number of useful quantum algorithms which have been discovered remains small. They consist mainly of variants on the period-finding algorithm presented above and another quite different task: that of searching an unstructured list. Grover (1997) presented a quantum algorithm for the following problem: given an unstructured list of items {x i }, find a particular item x j = t. Think, for example, of looking for a particular telephone number in the telephone directory (for someone whose name you do not know). It is not hard to prove that classical algorithms can do no better than searching through the list, requiring on average N/2 steps, for a list of N items. Grover's algorithm requires of order √ N steps. The task remains computationally hard: it is not transferred to a new complexity class, but it is remarkable that such a seemingly hopeless task can be speeded up at all. The 'quantum speed-up' ∼ √ N/2 is greater than that achieved by Shor's factorization algorithm (∼ exp(2(ln N) 1/3 )) and would be important for the huge sets (N 10 16 ) which can arise, for example, in code-breaking problems (Brassard 1997). An important further point was proved by Bennett et al (1997), namely that Grover's algorithm is optimal: no quantum algorithm can do better than O( √ N). A brief sketch of Grover's algorithm is as follows. Each item has a label i, and we must be able to test in a unitary way whether any item is the one we are seeking. In other words there must exist a unitary operator S such that S|i = |i if i = j , and S|j = -|j , where j is the label of the special item. For example, the test might establish whether i is the solution of some hard computational problem †. The method begins by placing a single quantum register in a superposition of all computational states, as in the period-finding algorithm (equation ( 34)). Define where j is the label of the element t = x j to be found. The initially prepared state is an equally weighted superposition, | (θ 0 ) where sin θ 0 = 1/ √ N . Now apply S, which reverses the sign of the one special element of the superposition, then Fourier transform, change the sign of all components except |0 , and Fourier transform back again. These operations represent a subtle interference effect which achieves the following transformation: where sin φ = 2 √ N -1/N. The coefficient of the special element is now slightly larger than that of all the other elements. The method proceeds simply by applying U G m times, where m (π/4) √ N. The slow rotation brings θ very close to π/2, so the quantum state becomes almost precisely equal to |j . After the m iterations the state is measured and the value j obtained (with error probability O(1/N )). If U G is applied too many times, the success probability diminishes, so it is important to know m, which was deduced by Boyer et al (1996). Kristen Fuchs compares the technique with cooking a soufflé. The state is placed in the 'quantum oven' and the desired answer rises slowly. You must open the oven at the right time, neither too soon nor too late, to guarantee success. Otherwise the soufflé will fall-the state collapses to the wrong answer. The two algorithms I have presented are the easiest to describe and illustrate many of the methods of quantum computation. However, just what further methods may exist is an open question. Kitaev (1996) has shown how to solve the factorization and related problems using a technique fundamentally different from Shor's. His ideas have some similarities to Grover's. Kitaev's method is helpfully clarified by Jozsa (1997b) who also brings out the common features of several quantum algorithms based on Fourier transforms. The quantum programmer's toolbox is thus slowly growing. It seems safe to predict, however, that the class of problems for which QCs out-perform classical ones is a special and therefore small class. On the other hand, any problem for which finding solutions is hard, but testing a candidate solution is easy, can as a last resort be solved by an exhaustive search and here Grover's algorithm may prove very useful."
    },
    {
      "heading": "Experimental quantum information processors",
      "text": "The most elementary quantum logical operations have been demonstrated in many physics experiments during the past 50 years. For example, the NOT operation (X) is no more than a stimulated transition between two energy levels |0 and |1 . The important XOR operation can also be identified as a driven transition in a four-level system. However, if we wish to contemplate a QC it is necessary to find a system which is sufficiently controllable to allow quantum logic gates to be applied at will, yet is sufficiently complicated to store many qubits of quantum information. Figure 12. Ion-trap quantum information processor. A string of singly charged atoms is stored in a linear ion trap. The ions are separated by ∼ 20 µm by their mutual repulsion. Each ion is addressed by a pair of laser beams which coherently drive both Raman transitions in the ions, and also transitions in the state of motion of the string. The motional degree of freedom serves as a single-qubit 'bus' to transport quantum information among the ions. State preparation is by optical pumping and laser cooling; readout is by electron shelving and resonance fluorescence, which enables the state of each ion to be measured with high signal to noise ratio. It is very hard to find such systems. One might hope to fabricate quantum devices on solid state microchips-this is the logical progression of the microfabrication techniques which have allowed classical computers to become so powerful. However, quantum computation relies on complicated interference effects and the great problem in realizing it is the problem of noise. No quantum system is really isolated and the coupling to the environment produces decoherence which destroys the quantum computation. In solid state devices the environment is the substrate and the coupling to this environment is strong, producing typical decoherence times of the order of picoseconds. It is important to realize that it is not enough to have two different states |0 and |1 which are themselves stable (for example states of different current in a superconductor): we require also that superpositions such as |0 + |1 preserve their phase, and this is typically where the decoherence timescale is so short. At present there are two candidate systems which should permit quantum computation on 10 to 40 qubits. These are the proposal of Cirac and Zoller (1995) using a line of singly charged atoms confined and cooled in vacuum in an ion trap and the proposal of Gershenfeld and Chuang (1997), and simultaneously Cory et al (1996), using the methods of bulk NMR. In both cases the proposals rely on the impressive efforts of a large community of researchers which developed the experimental techniques. Previous proposals for experimental quantum computation (Lloyd 1993, Berman et al 1994, Barenco et al 1995a, DiVincenzo 1995b) touched on some of the important methods but were not experimentally feasible. Further recent proposals (Privman et al 1997, Loss andDiVincenzo 1997) may become feasible in the near future."
    },
    {
      "heading": "Ion trap",
      "text": "The ion-trap method is illustrated in figure 12 and described in detail by Steane (1997b). A string of ions is confined by a combination of oscillating and static electric fields in a linear 'Paul trap' in high vacuum (10 -8 Pa). A single laser beam is split by beam splitters and acousto-optic modulators into many beam pairs, one pair illuminating each ion. Each ion has two long-lived states, for example different levels of the ground-state hyperfine structure (the lifetime of such states against spontaneous decay can exceed thousands of years). Let us refer to these two states as |g and |e ; they are orthogonal and so together represent one qubit. Each laser beam pair can drive coherent Raman transitions between the internal states of the relevant ion. This allows any single-qubit quantum gate to be applied to any ion, but not 2-qubit gates. The latter requires an interaction between ions and this is provided by their Coulomb repulsion. However, exactly how to use this interaction is far from obvious; it required the important insight of Cirac and Zoller. Light carries not only energy but also momentum, so whenever a laser beam pair interacts with an ion, it exchanges momentum with the ion. In fact, the mutual repulsion of the ions means that the whole string of ions moves en masse when the motion is quantized (Mössbauer effect). The motion of the ion string is quantized because the ion string is confined in the potential provided by the Paul trap. The quantum states of motion correspond to the different degrees of excitation ('phonons') of the normal modes of vibration of the string. In particular we focus on the ground state of the motion |n = 0 and the lowest excited state |n = 1 of the fundamental mode. To achieve, for example, controlled-Z between ion x and ion y, we start with the motion in the ground state |n = 0 . which is exactly a controlled-Z between x and y. Each laser pulse must have a precisely controlled frequency and duration. The controlled-Z gate and the single-qubit gates together provide a universal set, so we can perform arbitrary transformations of the joint state of all the ions! To complete the prescription for a QC (section 6), we must be able to prepare the initial state and measure the final state. The first is possible through the methods of optical pumping and laser cooling, the second through the 'quantum jump' or 'electron shelving' measurement technique. All these are powerful techniques developed in the atomic physics community over the past 20 years. However, the combination of all the techniques at once has only been achieved in a single experiment, which demonstrated preparation, quantum gates, and measurement for just a single trapped ion (Monroe et al 1995b). The chief experimental difficulty in the ion-trap method is to cool the string of ions to the ground state of the trap (a submicroKelvin temperature). The chief source of decoherence is the heating of this motion owing to the coupling between the charged ion string and Figure 13. Bulk nuclear spin resonance quantum information processor. A liquid of ∼ 10 20 'designer' molecules is placed in a sensitive magnetometer, which can both generate oscillating magnetic fields and also detect the precession of the mean magnetic moment of the liquid. The situation is somewhat like having 10 20 independent processors, but the initial state is one of thermal equilibrium, and only the average final state can be detected. The quantum information is stored and manipulated in the nuclear spin states. The spin-state energy levels of a given nucleus are influenced by neighbouring nuclei in the molecule, which enables XOR gates to be applied. They are little influenced by anything else, owing to the small size of a nuclear magnetic moment, which means the inevitable dephasing of the processors with respect to each other is relatively slow. This dephasing can be undone by 'spin echo' methods. noise voltages in the electrodes (Steane 1997b, Wineland et al 1997). It is unknown just how much the heating can be reduced. A conservative statement is that in the next few years 100 quantum gates could be applied to a few ions without losing coherence. In the longer term one may hope for an order of magnitude increase in both figures. It seems clear that an ion-trap processor will never achieve sufficient storage capacity and coherence to permit factorization of hundred-digit numbers. However, it would be fascinating to try a quantum algorithm on just a few qubits (4-10) and thus to observe the principles of quantum information processing at work. We will discuss in section 9 methods which should allow the number of coherent gate operations to be greatly increased."
    },
    {
      "heading": "Nuclear magnetic resonance",
      "text": "The proposal using NMR is illustrated in figure 13. The quantum processor in this case is a molecule containing a 'backbone' of about ten atoms, with other atoms such as hydrogen attached so as to use up all the chemical bonds. It is the nuclei which interest us. Each has a magnetic moment associated with the nuclear spin and the spin states provide the qubits. The molecule is placed in a large magnetic field, and the spin states of the nuclei are manipulated by applying oscillating magnetic fields in pulses of controlled duration. So far, so good. The problem is that the spin state of the nuclei of a single molecule can be neither prepared nor measured. To circumvent this problem, we use not a single molecule, but a cup of liquid containing some 10 20 molecules! We then measure the average spin state, which can be achieved since the average oscillating magnetic moment of all the nuclei is large enough to produce a detectable magnetic field. Some subtleties enter at this point. Each of the molecules in the liquid has a very slightly different local magnetic field, influenced by other molecules in the vicinity, so each 'quantum processor' evolves slightly differently. This problem is circumvented by the spin-echo technique, a standard tool in NMR which allows the effects of free evolution of the spins to be reversed, without reversing the effect of the quantum gates. However, this increases the difficulty of applying long sequences of quantum gates. The remaining problem is to prepare the initial state. The cup of liquid is in thermal equilibrium to begin with, so the different spin states have occupation probabilities given by the Boltzman distribution. One makes use of the fact that spin states are close in energy, and so have nearly equal occupations initially. Thus the density matrix ρ of the O(10 20 ) nuclear spins is very close to the identity matrix I . It is the small difference = ρ -I which can be used to store quantum information. Although is not the density matrix of any quantum system, it nevertheless transforms under well-chosen field pulses in the same way as a density matrix would, and hence can be considered to represent an effective QC. The reader is referred to Gershenfeld and Chuang (1997) for a detailed description, including the further subtlety that an effective pure state must be distilled out of by means of a pulse sequence which performs quantum data compression. NMR experiments have for some years routinely achieved spin-state manipulations and measurements equivalent in complexity to those required for quantum information processing on a few qubits, therefore the first few-qubit quantum processors will be NMR systems. The method does not scale very well as the number of qubits is increased, however. For example, with n qubits the measured signal scales as 2 -n . Also the possibility to measure the state is limited, since only the average state of many processors is detectable. This restricts the ability to apply QEC (section 9), and complicates the design of quantum algorithms."
    },
    {
      "heading": "High-Q optical cavities",
      "text": "Both systems we have described permit simple quantum information processing, but not quantum communication. However, in a very high-quality optical cavity, a strong coupling can be achieved between a single atom or ion and a single mode of the electromagnetic field. This coupling can be used to apply quantum gates between the field mode and the ion, thus opening the way to transferring quantum information between separated ion traps, via high-Q optical cavities and optical fibres (Cirac et al 1997). Such experiments are now being contemplated. The required strong coupling between a cavity field and an atom has been demonstrated by Brune et al (1994) and Turchette et al (1995). An electromagnetic field mode can also be used to couple ions within a single trap, providing a faster alternative to the phonon method (Pellizzari et al 1995)."
    },
    {
      "heading": "Quantum error correction",
      "text": "In section 7 we discussed some beautiful quantum algorithms. Their power only rivals classical computers, however, on quite large problems, requiring thousands of qubits and billions of quantum gates (with the possible exception of algorithms for simulation of physical systems). In section 8 we examined some experimental systems, and found that we can only contemplate 'computers' of a few tens of qubits and perhaps some thousands of gates. Such systems are not 'computers' at all because they are not sufficiently versatile: they should at best be called modest quantum information processors. Whence came this huge disparity between the hope and the reality? The problem is that the prescription for the universal QC, section 6, is unphysical in its fourth requirement. There is no such thing as a perfect quantum gate, nor is there such a thing as an isolated system. One may hope that it is possible in principle to achieve any degree of perfection in a real device, but in practice this is an impossible dream. Gates such as XOR rely on a coupling between separated qubits, but if qubits are coupled to each other, they will unavoidably be coupled to something else as well (Plenio and Knight 1996). A rough guide is that it is very hard to find a system in which the loss of coherence is smaller than one part in a million each time a XOR gate is applied. This means the decoherence is roughly 10 7 times too fast to allow factorization of a 130 digit number! It is an open question whether the laws of physics offer any intrinsic lower limit to the decoherence rate, but it is safe to say that it would be simpler to speed up classical computation by a factor of 10 6 than to achieve such low decoherence in a large QC. Such arguments were eloquently put forward by Haroche and Raimond (1996). Their work and that of others such as Landauer (1995Landauer ( , 1996) ) sounds a helpful note of caution. More detailed treatments of decoherence in QCs are given by Unruh (1995), Palma et al (1996) and Chuang et al (1995). Large numerical studies are described by Miquel et al (1996) and Barenco et al (1997). Classical computers are reliable not because they are perfectly engineered, but because they are insensitive to noise. One way to understand this is to examine in detail a device such as a flip-flop, or even a humble mechanical switch. Their stability is based on a combination of amplification and dissipation: a small departure of a mechanical switch from 'on' or 'off' results in a large restoring force from the spring. Amplifiers do the corresponding job in a flip-flop. The restoring force is not sufficient alone, however: with a conservative force, the switch would oscillate between 'on' and 'off'. It is important also to have damping, supplied by an inelastic collision which generates heat in the case of a mechanical switch and by resistors in the electronic flip-flop. However, these methods are ruled out for a QC by the fundamental principles of quantum mechanics. The no-cloning theorem means amplification of unknown quantum states is impossible and dissipation is incompatible with unitary evolution. Such fundamental considerations lead to the widely accepted belief that quantum mechanics rules out the possibility to stabilize a QC against the effects of random noise. A repeated projection of the computer's state by well-chosen measurements is not in itself sufficient (Berthiaume et al 1994, Miquel et. al 1997). However, by careful application of information theory one can find a way around this impasse. The idea is to adapt the error correction methods of classical information theory to the quantum situation. QEC was established as an important and general method by Steane (1996b) and independently Calderbank and Shor (1996). Some of the ideas had been introduced previously by Shor (1995b) and Steane (1996a). They are related to the 'entanglement purification' introduced by Bennett et al (1996a) and independently Deutsch et al (1996). The theory of QEC was further advanced by Knill and Laflamme (1997), Ekert and Macchiavello (1996), Bennett et al (1996b). The latter paper describes the optimal 5qubit code also independently discovered by Laflamme et al (1996). Gottesman (1996) and Calderbank et al (1997) discovered a general group-theoretic framework, introducing the important concept of the stabilizer, which also enabled many more codes to be found (Calderbank et al 1996, Steane 1996c, d). Quantum coding theory reached a further level of maturity with the discovery by Shor and Laflamme (1997) of a quantum analogue to the {I, X, Y, Z}. For example M = I 1 X 2 I 3 Y 4 Z 5 X 6 I 7 for the case n = 7. A general noisy state is s |e s M s |φ E . (44) Now we introduce even more qubits: a further nk, prepared in the state |0 a . This additional set is called an 'ancilla'. For any given encoding E, there exists a syndrome extraction operation A, operating on the joint system of qc and a, whose effect is A(M s |φ E |0 a ) = (M s |φ E )|s a ∀M s ∈ S. The set S is the set of correctable errors, which depends on the encoding. In the notation |s a , s is just a binary number which indicates which error operator M s we are dealing with, so the states |s a are mutually orthogonal. Suppose for simplicity that the general noisy state (44) only contains M s ∈ S, then the joint state of environment, qc and a after syndrome extraction is s |e s (M s |φ E )|s a . (45) We now measure the ancilla state, and something rather wonderful happens: the whole state collapses onto |e s (M s |φ E )|s a , for some particular value of s. Now, instead of general noise, we have just one particular error operator M s to worry about. Furthermore, the measurement tells us the value s (the 'error syndrome') from which we can deduce which M s we have! Armed with this knowledge, we apply M -1 s to qc by means of a few quantum gates (X, Z or Y ), thus producing the final state |e s |φ E |s a . In other words, we have recovered the noise-free state of qc! The final environment state is immaterial, and we can re-prepare the ancilla in |0 a for further use. The only assumption in the above was that the noise in equation ( 44) only contains error operators in the correctable set S. In practice, the noise includes both members and non-members of S, and the important quantity is the probability that the state collapses onto a correctable one when the syndrome is extracted. It is here that the theory of errorcorrecting codes enters in: our task is to find encoding and extraction operations E, A such that the set S of correctable errors includes all the errors most likely to occur. This is a very difficult problem. It is a general truth that to permit efficient stabilization against noise, we have to know something about the noise we wish to suppress. The most obvious quasi-realistic assumption is that of uncorrelated stochastic noise. That is, at a given time or place the noise might have any effect, but the effects on different qubits, or on the same qubit at different times, are uncorrelated. This is the quantum equivalent of the binary symetric channel, section 2.3. By assuming uncorrelated stochastic noise we can place all possible error operators M in a heirarchy of probability: those affecting few qubits (i.e. only a few terms in the tensor product are different from I ) are most likely, while those affecting many qubits at once are unlikely. Our aim will be to find quantum error correcting codes (QECCs) such that all errors affecting up to t qubits will be correctable. Such a QECC is termed a 't-error correcting code'. The simplest code construction (that discovered by Calderbank and Shor and Steane) goes as follows. First we note that a classical error-correcting code, such as the Hamming code shown in table 1, can be used to correct X errors. The proof relies on equation ( 17) which permits the syndrome extraction A to produce an ancilla state |s which depends only on the error M s and not on the computer's state |φ . This suggests that we store k quantum bits by means of the 2 k mutually orthogonal n-qubit states |i , where the binary number i is a member of a classical error-correcting code C, see section 2.4. This will not allow correction of Z errors, however. Observe that since Z = H XH , the correction of Z errors is equivalent to rotating the state of each qubit by H , correcting X errors, and rotating back again. This rotation is called a Hadamard transform; it is just a change in basis. The next ingredient is to note the following special property (Steane 1996a): where H ≡ H 1 H 2 H 3 . . . H n . In words, this says that if we make a quantum state by superposing all the members of a classical error-correcting code C, then the Hadamardtransformed state is just a superposition of all the members of the dual code C ⊥ . From this it follows, after some further steps, that it is possible to correct both X and Z errors (and therefore also Y errors) if we use quantum states of the form g"
    },
    {
      "heading": "Discussion",
      "text": "The idea of 'quantum computing' has fired many imaginations simply because the words themselves suggest something strange but powerful, as if the physicists have come up with a second revolution in information processing to herald the next millenium. This is a false impression. Quantum computing will not replace classical computing for similar reasons that quantum physics does not replace classical physics: no one ever consulted Heisenberg in order to design a house and no one takes their car to be mended by a quantum mechanic. If large QCs are ever made, they will be used to address just those special tasks which benefit from quantum information processing. A more lasting reason to be excited about quantum computing is that it is a new and insightful way to think about the fundamental laws of physics. The quantum computing community remains fairly small at present, yet the pace of progress has been fast and accelerating in the last few years. The ideas of classical information theory seem to fit into quantum mechanics like a hand into a glove, giving us the feeling that we are uncovering something profound about nature. Shannon's noiseless coding theorem leads to Schumacher and Josza's quantum coding theorem and the significance of the qubit as a useful measure of information. This enables us to keep track of quantum information and to be confident that it is independent of the details of the system in which it is stored. This is necessary to underpin other concepts such as error correction and computing. The classical theory of error correction leads to the discovery of QEC. This allows a physical process previously thought to be impossible, namely the almost perfect recovery of a general quantum state, undoing even irreversible processes such as relaxation by spontaneous emission. For example, during a long error-corrected quantum computation, using fault-tolerant methods, every qubit in the computer might decay a million times and yet the coherence of the quantum information be preserved. Hilbert's questions regarding the logical structure of mathematics encourage us to ask a new type of question about the laws of physics. In looking at Schrödinger's equation, we can neglect whether it is describing an electron or a planet and just ask about the state manipulations it permits. The language of information and computer science enables us to frame such questions. Even such a simple idea as the quantum gate, the cousin of the classical binary logic gate, turns out to be very useful, because it enables us to think clearly about quantum-state manipulations which would otherwise seem extremely complicated or impractical. Such ideas open the way to the design of quantum algorithms such as those of Shor, Grover and Kitaev. These show that quantum mechanics allows information processing of a kind ruled out in classical physics. It relies on the propagation of a quantum state through a huge (exponentially large) number of dimensions of Hilbert space. The computation result arises from a controlled interference among many computational paths, which even after we have examined the mathematical description, still seems wonderful and surprising. The intrinsic difficulty of quantum computation lies in the sensitivity of large-scale interference to noise and imprecision. A point often raised against the QC is that it is essentially an analogue rather than a digital device and has many limitations as a result. This is a misconception. It is true that any quantum system has a continuous state space, but so has any classical system, including the circuits of a digital computer. The fault-tolerant methods used to permit error correction in a QC restrict the set of quantum gates to a discrete set, therefore the 'legal' states of the QC are discrete, just as in a classical digital computer. The really important difference between analogue and digital computing is that to increase the precision of a result arrived at by analogue means, one must re-engineer the whole computer, whereas with digital methods one need merely increase the number of bits and operations. The fault-tolerant QC has more in common with a digital than an analogue device. Shor's algorithm for the factorization problem stimulated a lot of interest in part because of the connection with data encryption. However, I feel that the significance of Shor's algorithm is not primarily in its possible use for factoring large integers in the distant future. Rather, it has acted as a stimulus to the field, proving the existence of a powerful new type of computing made possible by controlled quantum evolution, and exhibiting some of the new methods. At present, the most practically significant achievement in the general area of quantum information physics is not in computing at all, but in quantum key distribution. The title 'quantum computer' will remain a misnomer for any experimental device realized in the next twenty years. It is an abuse of language to call even a pocket calculator a 'computer', because the word has come to be reserved for general-purpose machines which more or less realize Turing's concept of the universal machine. The same ought to be true for QCs if we do not want to mislead people. However, small quantum information processors may serve useful roles. For example, concepts learned from quantum information theory may permit the discovery of useful new spectroscopic methods in nuclear magnetic resonance. Quantum key distribution could be made more secure and made possible over larger distances, if small 'relay stations' could be built which applied purification or errorcorrection methods. The relay station could be an ion trap combined with a high-Q cavity, which is realizable with current technology. It will surely not be long before a quantum state is teleported from one laboratory to another, a very exciting prospect. The great intrinsic value of a large QC is offset by the difficulty of making one. However, few would argue that this prize does not at least merit a lot of effort to find out just how unattainable, or hopefully attainable, it is. One of the chief uses of a processor which could manipulate a few quantum bits may be to help us better understand decoherence in quantum mechanics. This will be amenable to experimental investigation during the next few years: rather than waiting in hope, there is useful work to be done now. On the theoretical side, there are two major open questions: the nature of quantum algorithms, and the limits on reliability of quantum computing. It is not yet clear what is the essential nature of quantum computing and what general class of computational problem is amenable to efficient solution by quantum methods. Is there a whole mine of useful quantum algorithms waiting to be delved, or will the supply dry up with the few nuggets we have so far discovered? Can significant computational power be achieved with less than 100 qubits? This is by no means ruled out, since it is hard to simulate even 20 qubits by classical means. Concerning reliability, great progress has been made, so that we can now be cautiously optimistic that quantum computing is not an impossible dream. We can identify requirements sufficient to guarantee reliable computing, involving for example uncorrelated stochastic noise of order 10 -5 per gate and a QC 100 times larger than the logical machine embedded within it. However, can quantum decoherence be relied upon to have the properties assumed in such an estimate, and if not then can error correction methods still be found? Conversely, once we know more about the noise, it may be possible to identify considerably less taxing requirements for reliable computing. To conclude with, I would like to propose a more wide-ranging theoretical task: to arrive at a set of principles like energy and momentum conservation, but which apply to information, and from which much of quantum mechanics could be derived. Two tests of such ideas would be whether the EPR-Bell correlations thus became transparent, and whether they rendered obvious the proper use of terms such as 'measurement' and 'knowledge'. I hope that quantum information physics will be recognized as a valuable part of fundamental physics. The quest to bring together Turing machines, information, number theory and quantum physics is for me, and I hope will be for readers of this review, one of the most fascinating cultural endeavours one could have the good fortune to encounter."
    }
  ]
}