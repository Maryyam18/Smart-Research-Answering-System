{
  "paperid": "2024.emnlp-main.248",
  "title": "PROMETHEUS 2: An Open Source Language Model Specialized in Evaluating Other Language Models",
  "authors": [
    "Seungone Kim",
    "Juyoung Suk",
    "Shayne Longpre",
    "Bill Lin",
    "Jamin Shin",
    "Sean Welleck",
    "Graham Neubig",
    "Moontae Lee",
    "Kyungjae Lee",
    "Minjoon Seo"
  ],
  "year": 2024,
  "abstract": "Proprietary LMs such as GPT-4 are often employed to assess the quality of responses from various LMs. However, concerns including transparency, controllability, and affordability strongly motivate the development of opensource LMs specialized in evaluations. On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment. Additionally, they often do not possess the ability to evaluate based on custom evaluation criteria, focusing instead on general attributes like helpfulness and harmlessness. To address these issues, we introduce Prometheus 2. Prometheus 2 is more powerful than its predecessor, and closely mirrors human and GPT-4 judgements. Moreover, it is capable of processing both direct assessment and pair-wise ranking formats grouped with a user-defined evaluation criteria. On four direct assessment benchmarks and four pairwise ranking benchmarks, PROMETHEUS 2 scores the highest correlation and agreement with humans and proprietary LM judges among all tested open evaluator LMs. Our models, code, and data are all publicly available. 1 * equal contribution. Work was done while Seungone was an intern at LG AI Research and a MS student at KAIST.",
  "sections": [
    {
      "heading": "Introduction",
      "text": "Evaluating the quality of outputs produced by language models (LMs) is progressively becoming difficult, as the outputs cover an extremely diverse distribution of text and complex tasks. To address this issue, language model-based evaluation has emerged as a scalable and cheap paradigm for assessing LM-generated text (Li et al., 2024; Figure 1: Weak evaluators (e.g., Llama-2-Chat-70B, Prometheus, and GPT-3.5-Turbo) achieve low scoring correlation with strong evaluators (e.g., Humans, GPT-4, and Claude-3-Opus). On the other hand, scores provided by strong evaluators highly correlate with each other. Gao et al., 2024). In this paradigm, LMs are either prompted to output a scalar indicator of quality (denoted as direct assessment) (Zheng et al., 2023;Liu et al., 2023b;Ye et al., 2023;Kim et al., 2023) or to determine which of two outputs are preferred (denoted as pairwise ranking) (Wang et al., 2023b;Li et al., 2023b;Lambert et al., 2024). Prior works employing proprietary LMs as evaluators have demonstrated not only high correlations with human evaluations but also increased speed and cost-effectiveness (Zheng et al., 2023;Liu et al., 2023b;Dubois et al., 2023;Ye et al., 2023). However, relying on proprietary LMs for evaluation poses significant challenges. The lack of transparency about their training data compromises both fairness and reproducibility, making it problematic to use them in evaluation pipelines. Additionally, concerns regarding controllability and affordability also persist (Kim et al., 2023). To address these issues, recent works have focused on developing evaluator LMs that are open-access, transparent, and controllable (Kim et al., 2023;Wang et al., 2023a,b;Li et al., 2023a;Zhu et al., 2023;Jiang et al., 2023b,c;Lee et al., 2024). Yet, these models often yield scoring decisions that do not correlate well enough with human judgments or those made by proprietary LMs, failing to effectively simulate them. Moreover, open evaluator LMs are not flexible since they are typically trained only to perform either direct assessment or pairwise ranking and assess based on general public preferences like helpfulness and harmlessness, limiting their ability to handle diverse real-life scenarios. To close the gap with proprietary LMs, we investigate unifying the two model-based evaluation paradigms -direct assessment and pairwise ranking -to train a robust unified evaluator LM. We propose a recipe based on merging the weights of two evaluator LMs trained separately on direct assessment and pairwise ranking formats. Our key empirical observation is that weight merging can yield an evaluator LM that not only works in both formats, but also outperforms evaluator LMs that are jointly trained or only trained on a single format. To demonstrate our approach, we develop the PREFERENCE COLLECTION, a new fine-grained pairwise ranking feedback dataset that builds on the FEEDBACK COLLECTION (Kim et al., 2023), which is a direct assessment feedback dataset. We choose Mistral-7B (Jiang et al., 2023a) and Mixtral-8x7B (Jiang et al., 2024) as our base models, and merge the weights of evaluator LMs separately trained on the FEEDBACK COLLECTION and the PREFERENCE COLLECTION to obtain our resulting models, PROMETHEUS 2 (7B & 8x7B). On four direct assessment benchmarks (Vicuna Bench, MT Bench, FLASK, Feedback Bench), the PROMETHEUS 2 models demonstrate the highest correlation with both human evaluators and proprietary LM-based judges compared to existing open evaluator LMs, with the Pearson correlation surpassing other baselines by 0.2 units across all datasets. Similarly, on four pairwise ranking benchmarks (HHH Alignment, MT Bench Human Judgment, Auto-J Eval, Preference Bench), the PROMETHEUS 2 models show the highest agreement with human evaluators among all the open evaluator LMs we tested, reducing the performance gap with GPT-4 in half. Our contributions are summarized as follows: • We introduce PROMETHEUS 2 (7B & 8x7B), state-of-the-art open evaluator LMs that score high correlations with both human evaluators and proprietary LM-based judges on both direct assessment and pairwise ranking. • We introduce a pairwise ranking feedback dataset called the PREFERENCE COLLEC-TION, which includes 1K custom evaluation criteria beyond helpfulness and harmlessness. • We show that merging the weights of evaluator LMs trained on direct assessment and pairwise ranking feedback datasets results in a unified evaluator LM that excels in both schemes."
    },
    {
      "heading": "Language Model-based Evaluation",
      "text": "To assess the generation capabilities of LMs, prior works such as the GEM benchmark (Gehrmann et al., 2021(Gehrmann et al., , 2022) ) employed ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and BERTScore (Zhang et al., 2019) as their metrics, which measure the lexical or semantic similarity between a reference answer and a response. However, these conventional metrics are prone to false negatives because they are not expressive enough to recognize responses that are of good quality but differ from the reference answer (Schluter, 2017;Freitag et al., 2020;Hanna and Bojar, 2021). Recently, employing language models as a judge has gained attention as a promising paradigm to mimic the depth and granularity that human evaluation offers (Zheng et al., 2023;Liu et al., 2023b;Li et al., 2023b;Chan et al., 2023;Ye et al., 2023). To reduce the over-reliance on proprietary LMs, follow-up works suggest training language models specialized in evaluations (Cui et al., 2023;Kim et al., 2023;Jiang et al., 2023b,c;Li et al., 2023a;Lee et al., 2024). Yet, open evaluator LMs do not possess the flexibility to function in different evaluation schemes and show weak evaluation performance compared to proprietary LMs. We aim to bridge this gap by introducing PROMETHEUS 2."
    },
    {
      "heading": "Weight Merging",
      "text": "Prior works have demonstrated that weight merging can enhance performance across various domains, including language modeling (Li et al., 2022;Matena and Raffel, 2022;Ilharco et al., 2022;Don-Yehiya et al., 2022;Gururangan et al., 2023;Yadav et al., 2024;Sukhbaatar et al., 2024), instruction-tuning (Jang et al., 2023b;Yu et al., 2023), and aligning to user preferences (Jang et al., 2023a;Rame et al., 2024;Wang et al., 2024). In our work, we specifically focus on enhancing the evaluation capabilities of open evaluator LMs. By merging models trained on different assessment formats-specifically, direct assessment and pairwise ranking-we aim to obtain an evaluator LM that not only functions in both formats but also shows as good evaluation performances as proprietary LMs."
    },
    {
      "heading": "Methodology",
      "text": "We propose a new recipe for training a unified evaluator LM based on merging the weights of models trained for direct assessment and pairwise ranking. We begin with background on direct assessment and pairwise ranking for evaluator LMs (Section 3.1, 3.2), followed by the construction process of our training data (Section 3.3). Finally, we present our methods to train state-of-the-art evaluator LMs, Prometheus 2 models (Section 3.4)."
    },
    {
      "heading": "Direct Assessment",
      "text": "Direct assessment is mapping an instruction i and response r into a scalar value score s, such as f direct : (i, r) → s where s ∈ R. For the scoring range, we use an integer between 1 and 5. Prior works have identified several recipes to align the scores provided by evaluator LMs (s LM ) and the scores assigned by humans (s human ). For instance, Liu et al. (2023a) and Zheng et al. (2023) have shown that it is crucial to add a reference answer a as input to the evaluator LM to maximize the correlation between s LM and s human . Also, Zheng et al. (2023) and Ye et al. (2023) showed that prompting the language model to write verbal feedback v r before s also improves the correlation between s LM and s human . Lastly, Ye et al. (2023) and Kim et al. (2023) showed that by explicitly integrating evaluation criteria e, users can define the standards for model assessment, ensuring evaluations are flexible to specific needs rather than generic qualities. Specifically, e is represented as a score rubric including a description for the criterion itself and a set of descriptions for each score between the scoring range. This is expressed as: where s ∈ {1, 2, 3, 4, 5}"
    },
    {
      "heading": "Pairwise Ranking",
      "text": "Pairwise ranking is mapping an instruction i and two pair of responses (r m , r n ) into either i or j, such as f pair : (i, r m , r n ) → s where s ∈ {m, n}. Similar to direct assessment, prior works have identified that integrating a reference answer a and verbal feedback v rm,rn into the evaluation pipeline is crucial (Zheng et al., 2023;Li et al., 2023b,a). In addition, to support granular assessment under Data PREFERENCE FEEDBACK COLLECTION COLLECTION Evaluation Scheme Pairwise Ranking Direct Assessment # Evaluation Criteria 1,000 1,000 # Instructions 20,000 20,000 # Reference Answer 20,000 20,000 # Instances 200,000 100,000 #Verbal Feedback 200,000 100,000 Table 1: Statistics of our training datasets, the FEED-BACK COLLECTION and the PREFERENCE COLLEC-TION. Note that the 1K evaluation criteria, 20K instructions, and 20K reference answers are shared among the two datasets. Both datasets have an equal number of scoring decisions (\"A\" or \"B\"; 100K each & 1-5; 20K each) to prevent unintended biases after training. custom criterion, we add the evaluation criteria e as input to the evaluator LM (Ye et al., 2023;Kim et al., 2023). To the best of our knowledge, we are the first to study such fine-grained evaluation in pairwise ranking settings. This is expressed as: In pairwise ranking, the evaluation criterion e does not include a set of descriptions for each score; instead, only the description of the evaluation criterion itself. Also, it is noteworthy that the verbal feedback v rm,rn compares the commonalities and differences between r m and r n concerning e."
    },
    {
      "heading": "Training Methods & Baselines",
      "text": "Prompting Prompting involves querying an LM to make judgments in a specified evaluation format without training. We employ Llama-2-Chat-7,13,70B (Touvron et al., 2023); Mistral-7B-Instruct-v0.2 (Jiang et al., 2023a); and Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024) as our baselines. It's worth noting that models not explicitly trained on feedback data often fail to generate responses in the required format, making it extremely difficult to parse scoring decisions. Although it is impractical for regular use, we make a fair comparison by infinitely looping until scores can be parsed. Also, we include proprietary LMs such as GPT-3.5-Turbo-0613; GPT-4-1106; and Claude-3-Opus."
    },
    {
      "heading": "Single-Format Training Single-Format training",
      "text": "involves training a base model θ on either on a direct assessment feedback dataset D d or a pairwise ranking feedback dataset D p . For singleformat trained evaluator LMs, we test Prometheus-7,13B (Kim et al., 2023) (direct assessment); UltraRM-13B (Cui et al., 2023) (pairwise ranking); and PairRM-0.4B (Jiang et al., 2023c) (pair-wise ranking). In addition, we also report the performances of single-format training Mistral-7B-Instruct-v0.2 and Mixtral-8x7B-Instruct-v0.1 on either direct assessment or pairwise ranking. Joint Training Joint training involves training a base model θ on both a direct assessment feedback dataset D d and a pairwise ranking feedback dataset D p . This enables the resulting evaluator LM to function across both evaluation formats. For jointly trained evaluator LMs, we test Auto-J (Li et al., 2023a). In addition, we report the performances of jointly training Mistral-7B and Mixtral-8x7B on both direct assessment and pairwise ranking. Weight Merging Weight Merging involves training two models, θ d and θ p , separately on a direct assessment feedback dataset D d and a pairwise ranking feedback dataset D p . Then, the final evaluator LM θ f inal is obtained by merging θ d and θ p . For example, linear merging is as follows: In addition to linear merging, we test 5 additional variants, namely Task Arithmetic merging (Ilharco et al., 2022), TIES merging (Yadav et al., 2024), DARE-TIES and DARE-Linear merging (Yu et al., 2023), and SLERP merging (Goddard et al., 2024). We include an explanation of these merging methods and ablation experiment results of the performance differences in Appendix G. Among them, Evaluation Method Benchmark Metrics Judgment Source Reference Answer # Score Rubrics # Instructions # Judgments Direct Assessment Vicuna Bench Correlation Proprietary LMs Y 80 80 320 MT Bench Correlation Proprietary LMs Y 80 80 320 FLASK Correlation Proprietary LMs & Humans Y 12 200 2,000 Feedback Bench Correlation Proprietary LMs Y 200 200 1,000 Pairwise Ranking HHH Align. Accuracy Humans N 4 221 221 MT Bench Human Judg. Accuracy Humans N 1 80 3,360 Auto-J Eval Accuracy Humans N 1 58 1,392 Preference Bench Accuracy Proprietary LMs Y 200 200 2,000"
    },
    {
      "heading": "Experimental Setup",
      "text": "The statistics of all the benchmarks are in Table 2. The four direct assessment benchmarks are: • Vicuna Bench (Chiang et al., 2023): A singleturn chat benchmark that includes 80 test prompts, 80 hand-crafted score rubrics from Kim et al. (2023), and 320 responses obtained by WizardLM-13B, Vicuna-13B, Llama-2-Chat-13B, GPT-3.5-Turbo-0613. • MT Bench (Zheng et al., 2023): A multiturn chat benchmark that consists of 80 test prompts, 80 hand-crafted score rubrics from Kim et al. (2023), and 320 responses obtained by WizardLM-13B, Vicuna-13B, Llama-2-Chat-13B, GPT-3.5-Turbo-0613. • FLASK (Ye et al., 2023): A fine-grained evaluation benchmark comprised of 200 test prompts, 12 score rubrics, and 2000 responses acquired from Alpaca-7B, Vicuna-13B, Bard, GPT-3.5-Turbo-0613. In addition to scores from proprietary LMs, this benchmark also includes scores marked by human evaluators. • Feedback Bench (Kim et al., 2023): The test set of the FEEDBACK COLLECTION with 1K score rubrics, 200 instructions, and 1K responses that do not overlap with the train data. The four pairwise ranking benchmarks are: • HHH Alignment (Askell et al., 2021): A benchmark consisting of 221 prompts; 4 score rubrics (helpfulness, harmlessness, honesty, and other) and 221 response pairs (graded as 'win' or 'lose') judged by human evaluators. • MT Bench Human Judgment (Zheng et al., 2023): A benchmark that shares the same 80 prompts as MT-Bench. In addition, it provides 3,360 response pairs (graded as 'win', 'tie', or 'lose') judged by human evaluators. • Auto-J Eval (Li et al., 2023a): A benchmark consisted of 58 prompts and 1,392 response pairs (graded as 'win', 'tie', or 'lose') judged by human evaluators. This benchmark is used as the in-domain test set of Auto-J. • Preference Bench: Our in-domain test set for the PROMETHEUS models. Similar to how the PREFERENCE COLLECTION was made with the FEEDBACK COLLECTION, we adjust the FEEDBACK BENCH and pair two out of the five responses, resulting in a test set with 200 prompts, 2,000 response pairs (graded as 'win' or 'lose'), and 200 evaluation criteria. In direct assessment, we conduct referencebased evaluations by appending the reference answer as the input. We use Pearson, Spearman, and Kendall-Tau as performance metrics to measure scoring correlations against reference evaluators. Moreover, we include the results of the referencefree direct assessment evaluation in Appendix F. In pairwise ranking, we conduct reference-free evaluations. Based on judgments assigned by humans, we use accuracy as our metric to measure agreement between evaluator LMs and humans. Also, the MT Bench Human Judgment and Auto-J test set includes a 'tie' option assessed by human evaluators. We evaluate in two ways: by excluding all 'tie' options for pairwise ranking (denoted as 'w/o tie'), or by using direct assessment where responses scored as 'ties' are grouped, and pairwise rankings are applied to the remaining responses with differing scores (denoted as 'w/ tie'). Table 4: Pairwise Ranking Results Accuracy on human preference datasets. The best comparable accuracies are bolded and second best underlined except proprietary LMs. Note that HHH Alignment is an in-domain test set for PairRM, Auto-J Eval is an in-domain test set for Auto-J, and the Preference Bench is an in-domain test set for Prometheus-2 models."
    },
    {
      "heading": "Experimental Results",
      "text": "In this section, we compare the evaluation capabilities of PROMETHEUS-2 models with other baselines using a direct assessment format (Section 5.1) and a pairwise ranking format (Section 5.2). Additionally, we measure the consistency of the scores from evaluator LMs in Appendix E."
    },
    {
      "heading": "Direct Assessment Results",
      "text": "The direct assessment results are shown in Table 3. The scoring decisions of PROMETHEUS 2 models (7B & 8x7B), GPT-4-1106, Claude-3-Opus, and human evaluators all strongly correlate with each other, yielding Pearson correlations higher than 0.5 regardless of the reference evaluator and benchmark. On the other hand, base LMs, single-format trained LMs, and jointly trained LMs show lower correlations, mostly falling below 0.5. Notably, PROMETHEUS 2 models outperform Prometheus and Auto-J by at least 0.2 units across benchmarks in their correlation with proprietary LMs. Moreover, on the FLASK benchmark, while the correlation between humans and GPT-4 is 0.679, the highest correlation previously achieved by Prometheus-13B with humans was 0.449. PROMETHEUS-2-8X7B achieves a correlation of 0.555 with humans, halving the gap."
    },
    {
      "heading": "Pairwise Ranking Results",
      "text": "The pairwise ranking results are shown in Table 4. We exclude the results of Pair RM and Ultra RM on 'w/ Tie' settings since they could not process it. On all of the 4 benchmarks, the PROMETHEUS 2 models achieve the highest scores, showing that they could effectively simulate human judgments. Notably, while HHH Alignment is an in-domain test set for Pair RM, and Auto-J Eval is for Auto-J, PROMETHEUS-2-8X7B achieves higher scores. This shows that training a large LM (i.e., Mixtral-8x7B) with feedback data could be an effective strategy to obtain a robust evaluator LM that could generalize beyond its training data. Moreover, the PROMETHEUS 2 models at least halve the performance gap with proprietary LMs compared to existing evaluator LMs on out-of-domain test sets."
    },
    {
      "heading": "Analyses of Weight Merging",
      "text": "To understand the effectiveness of our proposed weight merging method in the context of evaluations, we address the following research questions: • RQ1: Is weight merging more effective compared to joint training? (Section 6.1) • RQ2: Is the effectiveness of weight merging due to model ensembling? (Section 6.2) • RQ3: To what extent does learning with direct assessment help pairwise ranking performance, and vice versa? (Section 6.3)"
    },
    {
      "heading": "Weight Merging vs Joint Training",
      "text": "Table 5 compares the performance of evaluator LMs trained via weight merging and joint training. Alongside this, we also add and compare the results of prompting and single-format training. Surprisingly, evaluator LMs trained via joint training often show lower performance compared to those trained only in single-format, which indicates negative task transfer. Specifically, evaluator LMs trained only on direct assessment formats obtain higher correlations compared to their jointly trained counterparts across different model scales. Similarly, evaluator LMs trained solely on pairwise ranking formats achieve higher average accuracy compared to those trained on multiple tasks, particularly when using Mixtral-8x7B as the base model. On the other hand, evaluator LMs trained via weight merging show superior performance not only compared to jointly trained evaluator LMs but also single-format trained evaluator LMs, indicating positive task transfer. Also, while both benefit each other, merging the pairwise ranking evaluator LM weights improves direct assessment performance more significantly than the reverse."
    },
    {
      "heading": "Is the Effectiveness of Weight Merging due to Model Ensembling?",
      "text": "While we empirically find that weight merging is effective, the underlying reason remains unclear. A natural assumption is that this effectiveness results from the ensembling effect of combining multiple models. To test this hypothesis, we conduct an ablation experiment where we train multiple evaluator LMs on different random seeds and merge them. Specifically, we merge two evaluator LMs trained on direct assessment formats (denoted as 'Direct Assessment & Direct Assessment') and two evaluator LMs trained on pairwise ranking formats (denoted as 'Pairwise Ranking & Pairwise Ranking'). We use Mistral-7B-Instruct as our base model. The results are presented in Table 6. Across multiple benchmarks, merging evaluator LMs trained on the same evaluation format does not enhance evaluation performance. Specifically, merging two evaluator LMs trained on the same evaluation format-whether direct assessment or pairwise ranking-negatively impacts performance on average for both direct assessment and pairwise ranking benchmarks. In contrast, merging two evaluator LMs, each trained on direct assessment and pairwise ranking formats, results in superior performance compared to the other settings. This indicates that the beneficial task transfer in weight merging arises from integrating different evaluation formats, not ensembling multiple models."
    },
    {
      "heading": "Quantifying Positive Transfer across Evaluation Formats",
      "text": "To explore how training on direct assessment feedback data influences pairwise ranking accuracy and vice versa, we experiment by adjusting the α value during linear merging. We evaluate the average performance using all eight benchmarks in our experiments. To illustrate the average performance (colored in black), we adjust the scale by multiplying the Pearson correlations from direct assessment, which originally range from 0 to 1, by 100 before averaging them with the pairwise ranking accuracy. The results are shown in Figure 3. For direct assessment benchmarks, evaluator LMs obtain the optimal performance when α is set to 0.5. This indirectly indicates that both pairwise ranking and direct assessment feedback data contribute equally. On the other hand, for pairwise ranking benchmarks, the performance is optimal when α is set to 0.3. This also implies that while both benefit each other, training on pairwise ranking improves direct assessment performance more than the reverse. Training Method DIRECT ASSESSMENT BENCHMARKS PAIRWISE RANKING BENCHMARKS Vicuna Ben. MT Ben. FLASK Average HHH Align. MT Ben. H.J. Auto-J Eval Average Mistral-Instruct-7B PROMPTING 0.486 0.284 0.480 0.417 67.42 63.82 60.94 64.06 DIRECT ASSESSMENT ONLY 0.537 0.561 0.519 0.539 73.33 56.76 64.38 64.82 PAIRWISE RANKING ONLY ----78.73 67.06 72.03 72.61 JOINT TRAINING 0.548 0.450 0.457 0.485 80.09 65.49 73.60 73.06 WEIGHT MERGING 0.666 0.548 0.659 0.624 74.66 70.78 75.07 73.50 Mixtral-Instruct-8x7B PROMPTING 0.566 0.551 0.507 0.541 77.38 71.42 73.55 74.56 DIRECT ASSESSMENT ONLY 0.625 0.664 0.587 0.625 74.21 53.14 65.85 64.40 PAIRWISE RANKING ONLY ----84.16 66.27 75.66 75.36 JOINT TRAINING 0.628 0.560 0.596 0.595 82.35 68.73 74.78 75.29 WEIGHT MERGING 0.685 0.665 0.659 0.670 85.52 71.96 79.98 79.15"
    },
    {
      "heading": "Conclusion",
      "text": "We introduce PROMETHEUS 2, an open-source LM specialized in evaluating other responses. Unlike existing open evaluator LMs that cannot effectively process both direct assessment and pairwise rank-ing-the two most prevalent evaluation schemesthe PROMETHEUS 2 models demonstrate superior performance on both schemes, significantly narrowing the gap with proprietary LM-based evaluations. To train the PROMETHEUS 2 models, we develop the PREFERENCE COLLECTION, the first pairwise ranking dataset that includes over 1,000 instancewise evaluation criteria beyond basic qualities such as helpfulness and harmlessness. Notably, we find that merging evaluator LMs trained on either direct assessment or pairwise ranking formats can lead to a unified evaluator LM with strong performance. We hope that our work encourages more research on using open-source LMs as evaluators. the Institute of Information & Communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (RS-2024-00397966, Development of a Cybersecurity Specialized RAG-based sLLM Model for Suppressing Gen-AI Malfunctions and Construction of a Publicly Demonstration Platform, 50%)."
    },
    {
      "heading": "Limitations",
      "text": "Evaluation is fundamentally a very multi-faceted task. In this paper, we used an indirect method to assess the evaluation capability of evaluator LMs by measuring if they perform evaluations similar to human evaluators or proprietary LMs, such as GPT-4-1106 and Claude-3-Opus. However, this may not necessarily be the best approach. Future work could explore meta-evaluation pipelines that reevaluate the results of evaluator LMs or methodologies that allow humans to efficiently review evaluation results. Also note that it is crucial to use modelbased evaluations in conjunction with human evaluation instead of solely relying on it. Additionally, the degree to which evaluator LMs can generalize was based on an analysis by Kim et al. (2023), which checked for overlap between the data used to train the evaluator LMs and the data used to evaluate them. This study extended the evaluation to eight different datasets with human judgments to check the generalization capability of evaluation under various circumstances. However, this may not be sufficient. One of the major challenges in evaluating evaluator LMs is obtaining the \"evaluation results\" (e.g., human judgment). Automating evaluations with LMs could greatly benefit many areas of NLP research, hence the role of future work in creating feedback benchmarks that include human judgment or data for training evaluator LMs is crucial. One downside of the PROMETHEUS 2 is that it operates only on a 1-5 point Likert scale for absolute evaluation or a comparative evaluation style of 'A is better & B is better'. Depending on the use case, people may need a 1-10 point absolute evaluation, a ranking method for five responses at once, or a checklist-based evaluation not covered in the paper. While proprietary LMs can flexibly conduct evaluations in any format if a well-described prompt is devised, open-source LMs cannot produce good evaluation results without training, and conversely, if trained in one or two formats, they lose the flexibility to conduct different evaluations. Future work could examine whether evaluator LMs trained in each format, as done in this paper, can handle evaluations for added formats well when weight merging is employed. Lastly, the paper presents an evaluation model that can handle both absolute and comparative evaluation formats well through weight merging based on empirical experiments. However, fundamentally explaining why weight merging works well remains a challenging task. To address this, Section 6 indirectly analyzes the effectiveness of weight merging by comparing it with joint training, demonstrating that the improvement in evaluation performance is not due to model ensembling, and showing that the impact of comparative evaluation on absolute evaluation is greater than the reverse. Our best current interpretation is that \"absolute and comparative evaluations are not completely different tasks, so weight merging could handle both without degeneration, and conversely, because they are not too similar, weight merging performed better than joint training.\" Future work could theoretically analyze this or further explore whether weight merging can effectively work in fields other than LLM evaluation. Table 10: Hyperparameters used to train PROMETHEUS 2 8x7B."
    },
    {
      "heading": "B Training and Inference Details",
      "text": "The configurations we used for prompting and training evaluator LMs are shown in Table 8, 9, 10. For Auto-J, PairRM and UltraRM, we utilize their prompt template, inference hyperparameter mentioned in the model cards or github repositories in order to ensure the configuration is optimal for a fair performance comparison. For proprietary LMs, PROMETHEUS 1, and PROMETHEUS 2 models, we use the same prompt template and evaluation configurations. For both training and inference, we utilized eight 40GB NVIDIA A100 GPUs. Training required approximately 800 GPU hours, using the implementation from the Alignment Handbook repositoryfoot_0 . For inference, we used the vllm frameworkfoot_1 . The results from Direct Assessment are averaged after three multiple runs, and pairwise grading is conducted in a single run. Instead of using error bars, we report the consistency in assessment formats, Krippendorff's alpha for consistency in direct assessment, and transitivity statistics for consistency in pairwise ranking."
    },
    {
      "heading": "C Direct Assessment Results: Extended",
      "text": "Table 11 and 12 (on the next page) shows the extended results Table 3. Even when changing the metrics to either Kendall-Tau and Spearman, the overall trends are maintained. PROMETHEUS 2 shows superior evaluation performances among the open evaluator LMs, achieving high correlations with humans and proprietary LMs."
    },
    {
      "heading": "D License",
      "text": "Our models are released under the Apache 2.0 license. The Preference Collection dataset is subject to OpenAI's Terms of Use for generated data. The model could be used for commercial purposes while the dataset is intended for research purposes. We used perspective API to ensure that the training data or evaluation datasets do not include PIIincluded instances."
    },
    {
      "heading": "G Merging Method Ablation",
      "text": "In this section, in addition to linear merging, we also test different merging techniques including: • Slerp merging (Goddard et al., 2024) operates by interpolating two weights θ d and θ p while preserving the geometric properties of the spherical space in which θ d and θ p reside. Specifically, this is conducted by normalizing θ d and θ p into unit length and then merging the two weights based on the coefficient α such as: • Task Arithmetic merging (Ilharco et al., 2022) which can be expressed as follows: where θ init is the weight of the base model. However, we empirically find that the resulting evaluator LM θ f inal often does not generate valid scoring decisions (e.g., generating an integer during pairwise ranking). • TIES merging (Yadav et al., 2024), while similar to Task Arithmetic merging, adds (1) a Trim operation to remove redundant weights in the task vector θ dθ init and θ pθ init and (2) Elect and Disjoint operations to resolve disagreement (i.e., opposite directed weights) between θ dθ init and θ pθ init . • DARE merging (Yu et al., 2023), while also similar to Task Arithmetic and TIES merging, performs a Random Drop and Re-scale operations in the task vector θ dθ init and θ pθ init to remove redundant weights. We find that DARE merging work best when we choose Mixtral-8x7B as our base model. DARE-linear merging is what was originally proposed by Yu et al. (2023). In DARE-TIES merging, the Elect operation from Yadav et al. ( 2024) is additionally added after the Re-scale operation. We conduct our experiments based on the implementation from MergeKit (Goddard et al., 2024). 4In Table 17 (on the previous page), we measure the performance of evaluator LMs employing different merging methods. In direct assessment benchmarks, DARE-Linear achieves the best performance, followed by DARE-TIES and Linear merging. In pairwise ranking benchmarks, Task Arithmetics achieves the best performance, with only a minimal difference compared to other methods. On average, DARE-Linear performs best. Based on these results, we have trained Prometheus-2-7B with DARE-Linear merging. We also opted to train Prometheus-2-8x7B using DARE-Linear merging. Although the optimal merging method might differ, we have not conducted additional experiments due to computational limitations. Future work could explore whether these findings hold true."
    }
  ],
  "figures": [],
  "equations": []
}