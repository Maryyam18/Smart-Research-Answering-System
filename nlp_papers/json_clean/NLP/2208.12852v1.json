{
  "paperid": "2208.12852v1",
  "title": "WHAT DO NLP RESEARCHERS BELIEVE? RESULTS OF THE NLP COMMUNITY METASURVEY",
  "authors": [
    "Michael",
    "Holtzman",
    "Parrish",
    "Mueller",
    "Wang",
    "Chen",
    "Madaan",
    "Nangia",
    "Pang",
    "Phang",
    "Bowman",
    "Scale",
    "Agi",
    "Amodei",
    "Olah",
    "Steinhardt",
    "Christiano",
    "Schulman",
    "Mané",
    "Arase",
    "Blunsom",
    "Diab",
    "Dodge",
    "Gurevych",
    "Liang",
    "Raffel",
    "Rücklé",
    "Schwartz",
    "Smith",
    "Strubell",
    "Zhang",
    "Bender",
    "Koller",
    "Bender",
    "Gebru",
    "Mcmillan-Major",
    "Shmitchell",
    "Benotti",
    "Drezde",
    "Fort",
    "Fung",
    "Hovy",
    "Kan",
    "Kim",
    "Nissim",
    "Tsvetkov",
    "Bommasani",
    "Hudson",
    "Adeli",
    "Altman",
    "Arora",
    "Sydney Von Arx",
    "Bernstein",
    "Bohg",
    "Bosselut",
    "Brunskill",
    "Brynjolfsson",
    "Buch",
    "Card",
    "Castellon",
    "Chatterji",
    "Chen",
    "Creel",
    "Davis",
    "Demszky",
    "Donahue",
    "Doumbouya",
    "Durmus",
    "Ermon",
    "Etchemendy",
    "Ethayarajh",
    "Fei-Fei",
    "Finn",
    "Gale",
    "Gillespie",
    "Goel",
    "Goodman",
    "Grossman",
    "Guha",
    "Hashimoto",
    "Henderson",
    "Hewitt",
    "Ho",
    "Hong",
    "Hsu",
    "Huang",
    "Icard",
    "Jain",
    "Jurafsky",
    "Kalluri",
    "Karamcheti",
    "Keeling",
    "Khani",
    "Khattab",
    "Koh",
    "Krass",
    "Krishna",
    "Kuditipudi",
    "Kumar",
    "Ladhak",
    "Lee",
    "Lee",
    "Leskovec",
    "Levent",
    "Xiang",
    "Li",
    "Li",
    "Ma",
    "Malik",
    "Manning",
    "Suvir",
    "Mirchandani",
    "Mitchell",
    "Munyikwa",
    "Nair",
    "Narayan",
    "Narayanan",
    "Newman",
    "Nie",
    "Niebles",
    "Hamed Nilforoshan",
    "Nyarko",
    "Ogut ; Rohan",
    "Taori",
    "Thomas",
    "Tramèr",
    "Wang",
    "Wang",
    "Wu",
    "Wu",
    "Wu",
    "Michael Xie",
    "Yasunaga",
    "You",
    "Matei",
    "Zaharia",
    "Zhang",
    "Zhang",
    "Zhang",
    "Zhang",
    "Zheng",
    "Bostrom",
    "Bourget",
    "Chalmers",
    "Bourget",
    "Chalmers",
    "Bourget",
    "Chalmers",
    "Samuel",
    "Bowman",
    "Dahl",
    "Brown",
    "Mann",
    "Ryder",
    "Subbiah",
    "Kaplan",
    "Dhariwal",
    "Neelakantan",
    "Shyam",
    "Sastry",
    "Askell",
    "Chowdhery",
    "Narang",
    "Devlin",
    "Bosma",
    "Mishra",
    "Roberts",
    "Barham",
    "Chung",
    "Sutton",
    "Gehrmann",
    "Schuh",
    "Shi",
    "Tsvyashchenko",
    "Maynez",
    "Rao",
    "Barnes",
    "Tay",
    "Shazeer",
    "Vinodkumar Prabhakaran",
    "Reif",
    "Du",
    "Hutchinson",
    "Pope",
    "Bradbury",
    "Austin",
    "Isard",
    "Gur-Ari",
    "Yin",
    "Duke",
    "Levskaya",
    "Ghemawat",
    "Dev",
    "Michalewski",
    "Garcia",
    "Misra",
    "Robinson",
    "Fedus",
    "Zhou",
    "Ippolito",
    "Luan",
    "Lim",
    "Zoph",
    "Spiridonov",
    "Sepassi",
    "Dohan",
    "Agrawal",
    "Omernick",
    "Dai",
    "Sankaranarayana Pillai",
    "Pellat",
    "Lewkowycz",
    "Moreira",
    "Child",
    "Polozov",
    "Lee",
    "Zhou",
    "Wang",
    "Saeta",
    "Diaz",
    "Firat",
    "Catasta",
    "Wei",
    "Meier-Hellstern",
    "Eck",
    "Corr",
    "Devlin",
    "Chang",
    "Lee",
    "Toutanova",
    "Diab",
    "Shi Feng",
    "Wallace",
    "Grissom",
    "Iyyer",
    "Rodriguez",
    "Boyd-Graber",
    "Foster",
    "Hearst",
    "Nivre",
    "Zhao",
    "Bruno S Frey",
    "Humbert",
    "Schneider",
    "Victor R Fuchs",
    "Krueger",
    "Poterba",
    "Hubinger",
    "Van Merwijk",
    "Mikulik",
    "Skalse",
    "Garrabrant",
    "Illge",
    "Schwarze",
    "John",
    "Ioannidis",
    "Jain",
    "Wallace",
    "Kurenkov",
    "Merrill",
    "Goldberg",
    "Schwartz",
    "Smith",
    "Saif",
    "Mohammad",
    "Saif",
    "Mohammad",
    "Saif",
    "Mohammad",
    "Saif",
    "Mohammad",
    "Narang",
    "Chung",
    "Tay",
    "Fedus",
    "Fevry",
    "Matena",
    "Malkan",
    "Fiedel",
    "Shazeer",
    "Lan",
    "Zhou",
    "Li",
    "Ding",
    "Marcus",
    "Roberts",
    "Raffel",
    "Patterson",
    "Gonzalez",
    "Hölzle",
    "Le",
    "Liang",
    "Munguia",
    "Rothchild",
    "So",
    "Texier",
    "Dean",
    "Radford",
    "Wu",
    "Child",
    "Luan",
    "Amodei",
    "Sutskever",
    "Inioluwa",
    "Raji",
    "Denton",
    "Bender",
    "Hanna",
    "Paullada",
    "Schwartz",
    "Dodge",
    "Smith",
    "Green",
    "Schütze",
    "Sivasundaram",
    "Hvidtfelt Nielsen",
    "Sutton",
    "Tay",
    "Dehghani",
    "Abnar",
    "Chung",
    "Fedus",
    "Rao",
    "Narang",
    "Vinh Q Tran",
    "Yogatama",
    "Metzler",
    "Wiegreffe",
    "Pinter"
  ],
  "year": 2022,
  "abstract": "We present the results of the NLP Community Metasurvey. Run from May to June 2022, the survey elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split almost exactly in half on questions about the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed metaquestions, asking respondents to predict the distribution of survey responses. This allows us not only to gain insight on the spectrum of beliefs held by NLP researchers, but also to uncover false sociological beliefs where the community's predictions don't match reality. We find such mismatches on a wide range of issues. Among other results, the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its own belief in the importance of linguistic structure, inductive bias, and interdisciplinary science.",
  "sections": [
    {
      "heading": "INTRODUCTION",
      "text": "What do NLP researchers think about NLP? • Are we devoting too many resources to scaling up? • Do language models understand language? Will they ever? • Is the traditional paradigm of model benchmarking still tenable? • What kinds of predictive models are ethical for researchers to build and release? • Will the next most influential advances come from industry or academic labs? These questions and many more are actively debated in the research community, and views on them are a major factor in deciding what work gets done. Understanding the prevalence of different views on these issues is valuable for understanding the trajectory of NLP research and the structure of the field. In addition, communication among researchers often rests on sociological beliefs about these questions: what people think people think. Getting these sociological beliefs wrong can slow down communication and lead to wasted effort, missed opportunities, and needless fights. For example: • An early-career researcher may avoid researching a topic they think is important if they think that it is not valued by the community and it would be hard to get past review. • Extra time and effort may be spent in papers and research agendas defending what is thought to be a provocative position, when it is in fact already well established and widely believed. • Papers or public communications generally appeal to premises they believe are settled or accepted as general consensus, but if the premise is not actually settled, then the argument will not be convincing to a large portion of the community, and the discourse may become more fractured, with increasingly isolated groups of researchers forming echo chambers around different sets of assumptions. The NLP research community gets to know itself in a variety of ways: discussions with colleagues at the same institution, presentations and interactions at conferences, invited talks and panel discussions, and interactions on social media like Twitter. All of these sources of information are biased, for example through self-selection towards similar people and amplification of already-prominent or controversial voices. These effects can make it difficult for any individual to get a sense of the NLP research community's beliefs as a whole. For these reasons, we believe it is worth trying to objectively assess NLP researchers' collective stance on controversial issues. So from May to June 2022, we conducted the NLP Community Metasurvey. On each issue, we present a stance, such as Currently, the field focuses too much on scaling up machine learning models (Q5-1), and ask respondents to state whether they agree or disagree. Then we ask them to also predict the percentage of respondents who will agree. This gives us insight into the community's object-level beliefs as well as its sociological beliefs, and allows us to identify where the two may be misaligned. This work is directly inspired by the PhilPapers Surveys (Bourget & Chalmers, 2014;2020), a research effort created and maintained by philosophers to assess the philosophy community's beliefs about current topics in their field, along with a separate metasurvey to assess philosophers' sociological beliefs about their professional community. As Bourget & Chalmers write: Most of us have had the experience of reading philosophical papers that make sweeping sociological claims about the field that seem quite questionable. It is arguable that the \"received wisdom\" in a field (roughly, what people in a field take as a default view, one that can sometimes be presupposed) is not based on what most people in the field think, but rather is based on what most people think most people think. If the received wisdom is grounded in false sociological beliefs, that is worth knowing. It occurred to us that it would not be all that difficult to actually gather data about these matters, and that doing so might be an interesting and informative exercise. The rest of this document reports the methodology and results of the NLP Community Metasurvey. Some key results include: • A large majority of respondents are against the hypothesis that scaling up current systems and methods could solve \"practically any important problem\" in NLP, and this view is perceived as much more popular than it is ( §4.2, Q2-1). Yet, narrow majorities of respondents also regard recent progress in large-scale modeling as progress towards AGI, and think AGI should concern NLP researchers ( §4.3, Q3-1, Q3-2). • A majority of respondents think that the scientific value of the majority of work in NLP is dubious ( §4.1, Q1-5). • A large majority of respondents believe that NLP researchers should give higher priority to incorporating insights from neighboring fields, and greatly underestimate the number of other NLP researchers that share this belief ( §4.5, Q5-7). • Large majorities of respondents believe NLP research has had a positive impact on the world ( §4.6, Q6-1), will have a positive impact in the future (Q6-2), and could plausibly transform society ( §4. 3, Q3-3). Despite this optimism, a substantial minority also foresee plausible risks of a major global catastrophe caused by ML systems (Q3-4). • A plurality of respondents think the most influential area of advances in the next 10 years will be in problem formulation and task design, as opposed to hardware and data scaling, which respondents believed would be the most popular opinion ( §4.7). It is worth noting that our results are descriptive, not prescriptive, as these issues cannot be resolved by majority vote. By necessity, we are covering a subjectively chosen set of questions and reducing many complex issues into simplified scales, but we hope that these results can create common ground for fruitful discussion among the NLP research community."
    },
    {
      "heading": "METHODOLOGY",
      "text": "Choosing Questions We aimed to ask about issues: • which are frequently discussed in the community, • which are the subject of public disagreement, • about which the NLP community often reflects back on itself, especially where people seem to perceive themselves as in the minority (hot takes) or majority (taking something for granted), and • for which, if we understand the community's opinions and meta-opinions better, it may aid our ability to communicate and help people understand how to most effectively communicate about their research. With these criteria in mind, we (the authors) brainstormed a large initial list of potential questions. After discussing, we voted on which ones to include in the survey, chose roughly the top 30 questions, finalized the agree/disagree question format, and began pilot testing (described in Appendix C), which we used to refine the set of questions, their phrasing, and their presentation format in the survey. The questions used in the survey are shown in Figure 2. Target Demographic: Active Authors in ACL Since we are interested in the public and scientific discourse of the NLP community, we target the survey at active NLP researchers. As an objective criterion, we define the target population of the survey as anyone who is an author on at least two *CL papers published in the last three years. This definition allows us to assess response bias by comparing to ACL members ( §3) and provides an objectively defined reference group for survey respondents to make predictions about in the meta-questions. Question Format We present questions in thematic groupings (e.g., \"State of the Field\"), phrased as statements that people could indicate their (dis)agreement with. They can respond to each statement on a 4-point scale of AGREE, WEAKLY AGREE, WEAKLY DISAGREE, and DISAGREE. We choose not to include an option in the middle of the scale to indicate a neutral position because our intent is to push respondents to consider where they actually stand. We instruct respondents to choose WEAKLY AGREE or WEAKLY DISAGREE if they have even slight preferences for one side or the other (e.g., \"depends, leaning negative\"). However, it is sometimes the case that someone truly cannot make a judgment, and for these cases we include three OTHER answers: QUESTION IS ILL-POSED, INSUFFICIENTLY INFORMED ON THE ISSUE, and PREFER NOT TO SAY. At the end of each section of the survey, we ask respondents to predict the proportion of respondents in our target population who will either AGREE or WEAKLY AGREE with each statement. Respondents can answer with one of five buckets: 0-20%, 20-40%, 40-60%, 60-80%, or 80-100%. Respondents can also skip the meta-questions, but we encourage them to give their best guess even if they are not sure. Finally, each section has a free-response box for the respondent to provide any comments, criticism, or other feedback on the survey. The survey instructions are reproduced in full in Appendix D.1."
    },
    {
      "heading": "Platform and Distribution",
      "text": "To host the survey, we used NYU Qualtrics. Following the guidelines set out by the NYU Institutional Review Board (protocol FY2022-6461), all respondents gave informed consent before beginning the survey and could either refuse to answer (i.e., by responding PREFER NOT TO SAY) or skip each question. As an incentive for participation, we committed to donating $10 for each respondent to one of several non-profit organizations that the respondent chooses at the end of the survey. 1 Region of Residence 0% 10% 20% 30% 40% 50% 60% 70% 80% Percent of Population United States Europe Asia / Pacific Middle East / North Africa Canada Source America / Caribbean Sub-Saharan Africa Prefer not to say 35% 58% 29% 23% 26% 8% 4% 5% 4% 2% 2% 1% 0.5% 0.3% 2% Gender 0% 10% 20% 30% 40% 50% 60% 70% 80% Percent of Population Man Woman Non-binary Other / Prefer not to say 70% 67% 24% 25% 3% 6% 6% Survey Respondents ACL Statistics Figure 1: Basic demographics of survey respondents, compared to available statistics from the ACL. For region, we compare to ACL memberships as of summer 2021, and for gender, we use the diversity statistics currently available from the ACL, based on attendance at ACL 2017 in Vancouver (which lacked a specific \"non-binary\" category). To attempt to reach a broad audience of NLP researchers, we set up a homepage for the survey at https://nlpsurvey.net and advertised in the following ways: (a) ACL Member Portal: we sent a call for participation to the ACL membership mailing list. The email included the details of the survey, its purpose, and the charitable donation incentive. (b) ACL 2022 in Dublin: Four of our team members advertised the survey to conference attendees in-person. They distributed flyers/posters of our survey and free stickers that said \"NLP survey\" or \"I took the NLP survey.\" (c) Twitter: We released multiple tweets as advertisement, with the original being retweeted 100+ times. (d) Slack channels: We posted about the survey in the Slack channels of a few labs, as well as an NLP Slack channel with more than 470 members that was set up during ACL 2020. (e) Emails: We attempted to encourage more participants from senior authors by sending personal invitations to 568 authors that have published at least eight qualifying papers since 2019 (we did not exhaustively email all of them, as it required manually sourcing email addresses based on names in the ACL Anthology). (f) Other social media (including posting on WeChat to encourage participation from researchers in China) and personal interactions with NLP researchers."
    },
    {
      "heading": "DEMOGRAPHICS",
      "text": "480 people completed the survey, of which 327 (68%) are in our target demographic, reporting that they co-authored at least 2 ACL publications between 2019-2022. We compute that 6323 people met this requirement during the survey period according to publication data in the ACL Anthology, meaning we have survey responses from about 5% of the total. For the rest of this paper, we restrict all reported results to this subset. Full question text and results for all demographics are available in Appendix B."
    },
    {
      "heading": "BASIC DEMOGRAPHICS",
      "text": "Figure 1 shows location and gender statistics. Survey respondents are mostly men (67%) and mostly from the United States (58%). To get a sense of biases in our results, we compare to official statistics from the ACL."
    },
    {
      "heading": "Location",
      "text": "The ACL publishes statistics about the countries of origin of its members. While ACL members are not the same population as our target demographic, we can use them as a rough proxy to assess for geographic bias. Comparing to the most recent available statisticsfoot_2 suggests that the United States is heavily overrepresented in our respondent population (58% > 35%), while Asia/Pacific is underrepresented (8% < 26%). Asian countries with large ACL contingents were particularly underrepresented, including China (3% < 9%), India (1% < 5%), and Japan (0.8% < 5%). We suspect this may largely be due to biases in our survey distribution methods (particularly Twitter and our personal networks). Gender For gender, we compare to ACL-published diversity statistics, computed from the attendee population at ACL 2017 in Vancouver, Canada. 3 While our gender distribution is skewed, with 67% men and 25% women, it seems to roughly match the ACL population."
    },
    {
      "heading": "Underrepresented Minorities",
      "text": "We ask respondents if they consider themselves to be part of an underrepresented minority group in NLP, to which 26% report Yes, 63% report No, and 11% Prefer not to say."
    },
    {
      "heading": "CAREER",
      "text": "Job Sector 73% of respondents are from academia, with 22% from industry and 4% in non-profit or government jobs. Asked if their job is \"publication-oriented,\" 89% answer Yes, including 62% of respondents in industry, while 9% answer No. Seniority Faculty and senior managers form a plurality of respondents at 41%, while 23% are junior professionals (including postdocs), 33% are PhD students, and 2% are Masters students or undergraduates. Breaking these down by sector, the largest group was academic PhD students (32%), 4followed by faculty (29%), senior managers (10%), postdocs or other academic researchers (10%), and junior researchers in industry (10%). Since ACL publishes statistics on the number of student memberships and regular memberships, we can also compare these numbers to the ACL population: 35% of respondents are students, compared to 53% of ACL members as of summer 2021. While students are underrepresented in our results compared to ACL members as a whole, it is not clear whether they are underrepresented compared to our target demographic of recently published authors. We also ask respondents to report the year they published their first research work in NLP. 20% of respondents reported a year prior to 2009, with the earliest being 1985. Another 20% were in 2010-2014, 20% in 2015-2017, 20% in 2018-2019, and 10% in 2019-2022, with the rest declining to answer. See Appendix B, Figure 13 for more details."
    },
    {
      "heading": "Subfields",
      "text": "We ask respondents to report all subfields that fit their work from the last 3 years, providing a list derived from the tracks at recent *CL conferences (EMNLP 2021, ACL 2022, and NAACL 2022). The most common responses are machine learning for NLP (39%), interpretability and analysis (31%), generation (28%), resources and evaluation (27%), and machine translation and multilinguality (26%). Coverage of the given subfields is broad, with each being marked by at least 20 respondents (6%). Full results are in Appendix B, Figure 16. Research Activity Respondents are highly active at ACL events, with 96% saying they have attended an ACL event in the last 3 years. Asked about their total number of peer-reviewed publications related to NLP, 13% report 1-4, 47% report 5-20, and 39% report 21 or more."
    },
    {
      "heading": "OTHER INFORMATION",
      "text": "Twitter Use A large majority of respondents use Twitter, with 18% saying I routinely post, 58% saying I follow but don't often post, and 21% saying I don't have a Twitter account, rarely look at it, or don't use it for research or NLP related content. While we don't know what proportion Figure 2: All survey questions with agree/disagree answers. Only the full statements (not the bolded summaries) were shown to respondents. Besides these, the survey also asked for an opinion on likely sources of future advances in NLP ( §4.7) as well as respondent demographics ( §3, Appendix B). of our target demographic uses Twitter, it seems likely that this is one of the largest sources of bias in our respondent population. At the same time, the purpose of this survey is partly to study NLP researchers' perceptions of the NLP community for the purpose of improving the public and scientific discourse. To the extent that these perceptions are formed on Twitter, and the public discourse is carried out on Twitter, our results may be useful even if biased towards Twitter users. Finding the Survey Asked how they heard about the survey, 37% reported hearing about it through Twitter, 21% from the email we sent through the ACL Member Portal, 19% from another mailing list, a Slack channel, or other messaging forum, 15% through word of mouth or personal communication, and 3% through an advertisement at ACL 2022, where we put up flyers and passed out stickers. Again, this suggests that our results are likely skewed towards Twitter users. Meta-Question Confidence We ask respondents to report their confidence in their meta-question predictions. 5% report being Completely confident, 32% Fairly confident, 34% Somewhat confident, 13% Slightly confident, 13% Not at all confident, and 2% Prefer not to say. Between these groups, the Completely confident had the highest mean absolute error on their meta-question predictions (20.9%), though this was only significantly (p < 0.05) different from the Slightly confident group (18.5%), which performed best, despite abstaining from predictions in fewer cases (5%) than the completely confident group (11%). We considered removing meta-question answers from less confident respondents for the purpose of reporting results, but since there are few statistically significant differences and the less confident respondents actually performed better, we include all meta-question responses in the rest of this work for the sake of simplicity."
    },
    {
      "heading": "RESULTS",
      "text": "For reference, all survey questions which took agree/disagree responses are shown in Figure 2. In the rest of this section, we will discuss the results of each section of the survey in detail. For each question (for example, in Figure 3), we display the proportion of AGREE, WEAKLY AGREE, WEAKLY DISAGREE, and DISAGREE answers in a band along the bottom of the visualization. These percentages exclude those who gave one of the other answers (INSUFFICIENTLY INFORMED ON THE ISSUE, QUESTION IS ILL-POSED, or PREFER NOT TO SAY), which were relatively rare (<20% of responses for all questions, <10% for 75% of questions; see Appendix A). The vertical green line shows the total percentage who AGREE or WEAKLY AGREE with the statement, which was the value we asked survey respondents to predict in the meta-questions. The gray bars show the distribution of answers to the meta-questions, each bin aligned with its corresponding range of percentages (0%-20%, 20%-40%, etc.). The green and black dots and bars show the mean and 95% bootstrap confidence intervals of the true and predicted percentage of people who AGREE or WEAKLY AGREE (treating each meta-question answer bucket as its midpoint). Unless otherwise stated, all percentages and percentage differences mentioned in this section will be in absolute terms and exclude the 'other' answers, and when we refer to respondents \"agreeing\" with a statement (without special typesetting) we include both AGREE and WEAKLY AGREE answers (and respectively with DISAGREE and WEAKLY DISAGREE). In this discussion we will sometimes break down results by demographic group (e.g., comparing the agreement rates of men and women); unless otherwise stated, all such comparisons correspond to statistically significant differences between the groups (p < 0.05) by a bootstrap test. More information, visualizations, and confidence intervals for such comparison can be found online at https://nlpsurvey.net/results/."
    },
    {
      "heading": "STATE OF THE FIELD (FIGURE 3)",
      "text": "The first set of questions asks for opinions about the health of the NLP community. Industry's Undue Influence (Q1-1, Q1-2) Private firms are overwhelmingly seen as likely to produce the most-cited research of the next 10 years (Q1-2, 82%), but they are also seen as having too much influence (Q1-1, 74%). This suggests, as some respondents pointed out in the survey feedback, that many believe that number of citations is not a good proxy for value or importance. It also suggests a belief that industry's continued dominance will have a negative effect on the field, 61% 0% 20% 40% 60% 80% 100% 77% 1-1. Private firms have too much influence Private firms have too much influence in guiding the trajectory of the field. 64% 0% 20% 40% 60% 80% 100% 86% 1-2. Industry will produce the most widely-cited research The most widely-cited papers of the next 10 years are more likely to come out of industry than academia. 38% 0% 20% 40% 60% 80% 100% 30% 1-3. NLP winter is coming (10 years) I expect an \"NLP winter\" to come within the next 10 years, in which funding and job opportunities in NLP R&D fall by at least 50% from their peak. 51% 0% 20% 40% 60% 80% 100% 62% 1-4. NLP winter is coming (30 years) I expect an \"NLP winter\" to come within the next 30 years, in which funding and job opportunities in NLP R&D fall by at least 50% from their peak. 48% 0% 20% 40% 60% 80% 100% 67% 1-5. Most of NLP is dubious science A majority of the research being published in NLP is of dubious scientific value. 51% 0% 20% 40% 60% 80% 100% 63% 1-6. Author anonymity is worth it Author anonymity during review is valuable enough to warrant restrictions on the dissemination of research that is under review. Agree Weakly agree Weakly disagree Disagree Here, and in subsequent such figures, the lower number (in green) represents the fraction of respondents who agree with the position out of all those who took a side. The grey bars show the relative proportion of meta-question predictions in each bin (0-20%, 20-40%, etc.), and the upper number (in black) shows the average predicted rate of agreement, computed treating each bin as its midpoint. The green and black horizontal lines show 95% bootstrap confidence intervals. perhaps through their singular control of foundational systems such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022), or from the energy that widely-cited work in pretraining (Devlin et al., 2019;Radford et al., 2019) draws away from other research agendas. Respondents underpredict the popularity of the majority view by more than 15% on both of these questions, suggesting they might believe alternative agendas are already under-prioritized, such as directions focusing on incorporating interdisciplinary insights as opposed to raw scaling, or problem formulation and task design-other under-predicted views, as we will see in §4.2, §4.5, and §4.7). The under-prediction of agreement on Q1-1 and Q1-2 may also be an artifact of our sample population, which is overwhelmingly academic. Opinions are very different between job sectors, where 82% in academia agree that private firms have too much influence (Q1-1) compared to only 58% of respondents in industry. NLP Winter, Eventually (Q1-3, Q1-4) We ask respondents whether they expect there to be an \"NLP winter,\" where funding and job opportunities fall by at least 50% from their peak, in the near future. A substantial minority of 30% expect this to happen within the next 10 years (Q1-3), with only 7% AGREEing. For the next 30 years (Q1-4), confidence is much greater, with 62% expecting an NLP winter. Even a minority predicting such a major shift in the field reflects an overall belief that NLP research will undergo substantial changes in the near future (at least, in who is funding it and how much). Further interpretation of these results is difficult: For example, respondents may believe an NLP winter will arrive because the pace of innovation will stall (perhaps the reason they think industry research is overemphasized), because the ability to advance the state of the art will be monopolized by a small number of well-resourced industry labs (as they expect industry to continue producing widely-cited research), or because the distinction between NLP and other AI disciplines will disappear (as suggested by some respondents). 47% 0% 20% 40% 60% 80% 100% 17% 2-1. Scaling solves practically any important problem Given resources (i.e., compute and data) that could come to exist this century, scaled-up implementations of established existing techniques will be sufficient to practically solve any important real-world problem or application in NLP. 38% 0% 20% 40% 60% 80% 100% 50% 2-2. Linguistic structure is necessary Discrete general-purpose representations of language structure grounded in linguistic theory (involving, e.g., word sense, syntax, or semantic graphs) will be necessary to practically solve some important real-world problems or applications in NLP. 37% 0% 20% 40% 60% 80% 100% 51% 2-3. Expert inductive biases are necessary Expert-designed strong inductive biases (à la universal grammar, symbolic systems, or cognitively-inspired computational primitives) will be necessary to practically solve some important real-world problems or applications in NLP. 42% 0% 20% 40% 60% 80% 100% 61% 2-4. Ling/CogSci will contribute to the most-cited models It is likely that at least one of the five most-cited systems in 2030 will take clear inspiration from specific, non-trivial results from the last 50 years of research into linguistics or cognitive science. Agree Weakly agree Weakly disagree Disagree Dubious Science (Q1-5) A majority agrees that most NLP work is of \"dubious scientific value\" (67%). Respondents expressed uncertainty over what should count as \"dubious,\" as well as concerns about who determines the value of research. On one hand, such research could refer to work which is fundamentally unsound with ill-posed questions and meaningless results, which would be a powerful indictment of NLP research. On the other hand, it could simply mean that many reported findings are of little importance or are not robust, which would arguably not make NLP unique among sciences (Ioannidis, 2005). Either way, this result suggests that many NLP researchers think it is worth reflecting deeply on the value of our work. As respondents see the community being less critical than it actually is (by 19% absolute), it might be that those who are critical of the scientific standards of the field are not as likely to voice their views in public, or that vocal critics who exist are seen as less representative of the population than they actually are. Anonymity: Still Controversial (Q1-6) *CL conferences have much stricter anonymity policies than many other conferences NLP researchers submit to (e.g., NeurIPS, ICLR, and ICML). Responses suggest the community is in favor of these policies on balance (63% agree anonymity is important enough to warrant restrictions on disseminating preprints), though they are perceived as contentious: respondents guessed that around 51% of the target population would be in favor of such restrictive policies. Since the *CL anonymity policies have been subject to intense debate on platforms such as Twitter, this suggests that those critical of the policies may have been disproportionately represented in the minds of NLP researchers. This question was also split by gender, with 77% of women agreeing but only 58% of men-possibly due to concerns or experience with discrimination on the basis of author identity."
    },
    {
      "heading": "SCALE, INDUCTIVE BIAS, AND ADJACENT FIELDS (FIGURE 4)",
      "text": "Questions and meta-questions about the long-term potential of scale, inductive bias, and linguistic structure reveal some of the most striking mismatches between respondent attitudes and beliefs about those attitudes. Broadly speaking, the pro-scale and anti-structure views were much less popular than respondents thought they would be. A common refrain in the era of ever-larger models is the Bitter Lesson (Sutton, 2019): \"General methods that leverage computation are ultimately the most effective, and by a large margin.\" Under this perspective, one may expect benefits from incorporating linguistic structure or expert-designed inductive biases to be superseded by learning mechanisms operating on fewer, more general principles if they have enough training data and model capacity. While the success of deep learning and large language models may be taken as supporting evidence for the Bitter Lesson, we find that the community has bought into the Lesson far less than it thinks it has. 56% 0% 20% 40% 60% 80% 100% 58% 3-1. AGI is an important concern Understanding the potential development of artificial general intelligence (AGI) and the benefits/risks associated with it should be a significant priority for NLP researchers. 57% 0% 20% 40% 60% 80% 100% 57% 3-2. Recent progress is moving us towards AGI Recent developments in large-scale ML modeling (such as in language modeling and reinforcement learning) are significant steps toward the development of AGI. 57% 0% 20% 40% 60% 80% 100% 73% 3-3. AI could soon lead to revolutionary societal change In this century, labor automation caused by advances in AI/ML could plausibly lead to economic restructuring and societal changes on at least the scale of the Industrial Revolution. 37% 0% 20% 40% 60% 80% 100% 36% 3-4. AI decisions could cause nuclear-level catastrophe It is plausible that decisions made by AI or machine learning systems could cause a catastrophe this century that is at least as bad as an all-out nuclear war. Agree Weakly agree Weakly disagree Disagree Support for scaling maximalism is greatly overestimated (Q2-1) We ask respondents for their views on a strong version of the Bitter Lesson: whether scaling up compute and data resources with established existing techniques would be sufficient to practically solve any important problem in NLP (Q2-1). Overall, this is seen as a controversial issue, with respondents predicting a roughly even split of 47% agreement (though variance among predictions was high). However, only a small minority (17%) actually agree with the position, forming the largest discrepancy between predicted and actual opinions in the entire survey. This suggests that the popular discourse around recent developments in scaling up (Chowdhery et al., 2022) may not be reflective of the views of the NLP research community as a whole. Trend reversals are predicted for linguistic theory and inductive bias (Q2-2, Q2-3, Q2-4) The rest of the views articulated in this section were seen as less popular than Q2-1, but in reality they were much more popular (albeit still controversial). On what it will take to practically solve any important problem in NLP, 50% agree that explicit linguistic structure will be necessary (Q2-2), and 51% say the same for expert-designed inductive biases (Q2-3). In addition, 61% of respondents say it's likely that one of the five most-cited systems in 2030 will take inspiration from clear, non-trivial results from the last 50 years of linguistics or cognitive science research (Q2-4). All of these views are under-predicted by 12-19%, though the predictions more closely match the responses given by men, as responses to these questions were split by gender. Most notably, women are significantly more likely to agree with Q2-2 that linguistic structure is necessary (65%) compared to men (42%). Women also agreed with Q2-3 (62%) and Q2-4 (69%) more than men (49% and 57%, respectively), but these differences were not statistically significant. Like many respondents, we find these results surprising. It seems that many believe there will be a reversal of the current trend of end-to-end modeling with low-bias neural network architectures. The results for Q2-4 are particularly surprising to us, as even today's most cited systems seem not to satisfy this requirement, building on little more from cognitive science than a rough construal of neurons, attention, and tokens, which date back much further than 50 years."
    },
    {
      "heading": "AGI AND MAJOR RISKS (FIGURE 5)",
      "text": "The versatility and impressive language output of large pretrained models such as GPT-3 (Brown et al., 2020) and PaLM (Chowdhery et al., 2022) have prompted renewed discussions about artificial general intelligence (AGI), including predictions of when it might arrive, whether we are actually advancing toward it, and what its consequences would be. In this section, we ask about AGI and some of the largest possible impacts of AI technology. One concern is that respondents' answers may depend on their definition of \"artificial general intelligence,\" and whether they think it is well-defined at all. Our approach to this problem is to deliberately not provide a definition (which some respondents would surely find objectionable, no matter which definition we choose). Instead, we instruct respondents to answer according to their preferred definition, i.e., how they think the community should use the term, as we view this as an important issue to assess when figuring out how to talk about the issue as a community. AGI is a known controversy (Q3-1, Q3-2) On the questions explicitly about AGI, respondents were split near the middle, with 58% agreeing that AGI should be an important concern for NLP researchers (Q3-1) and 57% agreeing that recent research has advanced us toward AGI in some significant way (Q3-2). The two views are highly correlated, with 74% of those who think AGI is important also agreeing with Q3-2 that we're progressing towards it, while only 37% of people who don't think AGI is important think we're making that kind of progress. The meta-responses split similarly to the object-level responses, indicating that the community has a good sense that this is a controversial issue. It is worth acknowledging what this means: AGI is a controversial issue, the community in aggregate knows that it's a controversial issue, and now (courtesy of this survey) we can know that we know that it's controversial. While some may believe that AGI is obviously coming soon, and some may believe that it's obviously ill-defined, taking either position for granted in the public discourse or scholarly literature may not be an effective way to communicate to a broad NLP audience; rather, careful and considered discussion of the issue will be more productive for building common ground. Revolutionary and catastrophic outcomes are a concern (Q3-3, Q3-4) 73% of respondents agree that labor automation from AI could plausibly lead to revolutionary societal change in this century, on at least the scale of the Industrial Revolution (Q3-3). This points to a common reason why those who agree with Q3-1 might think AGI is an important concern, especially if we are meaningfully progressing towards it (Q3-2), as it could be fundamentally transformative for society; indeed, all views expressed in this section are positively correlated (see §5). But it's worth noting that a significant fraction of respondents (23%) agree with the prospect of revolutionary change (Q3-3) while disagreeing with the importance of AGI, suggesting that discussions about long-term or large-scale impacts of our work in NLP may not need to be tied up in the AGI debate. About a third (36%) of respondents agree that it is plausible that AI could produce catastrophic outcomes in this century, on the level of all-out nuclear war (Q3-4). While this is a much smaller proportion than those who expect revolutionary societal change (Q3-3), the stakes are extremely high and a substantial minority expressing concern about such outcomes indicates that a deeper discussion of such risks may be warranted in the NLP community. While we do not ask about specific ways in which respondents think this could happen, potential reasons for such concerns are discussed by Bostrom (2014); Amodei et al. (2016) and Hubinger et al. (2019). Certain demographics, particularly women (46%) and underrepresented minority groups (53%), were more likely to agree with Q3-4, reflecting pessimism about our ability to manage dangerous future technology perhaps based in the present-day track record of disproportionate harms to these groups. Q3-4 received a lot of critical feedback. Some respondents object to \"all-out nuclear war\" as far too strong, saying they would agree with less extreme phrasings of the question. This suggests that our result of 36% is an underestimate of respondents who are seriously concerned about negative impacts of AI systems. Some respondents also comment that AI/ML systems should not be discussed as if they have agency to make decisions, as all AI \"decisions\" can be traced back to human decisions regarding training data, architecture, how and on what phenomena models are evaluated (or not), and deployment decisions, among other factors."
    },
    {
      "heading": "LANGUAGE UNDERSTANDING (FIGURE 6)",
      "text": "The question of whether language models understand language has been the subject of some debate in the community (Bender & Koller, 2020;Merrill et al., 2021;Bommasani et al., 2021, §2.6). In this section, we ask some questions relevant to the issue, but one of the challenges is that their answers are highly dependent on how one defines the word \"understand.\" For this reason, as with §4.3, we deliberately choose not to provide a definition, as doing so would risk begging the question or forcing a definition that some would certainly find objectionable. Instead, we instruct respondents to answer according to their preferred definitions, i.e., how they think the community should use the word \"understand,\" as we view this as an important element of the discussion. Many respondents commented that this choice made it harder to respond to the questions in this section, and said 49% 0% 20% 40% 60% 80% 100% 51% 4-1. LMs understand language Some generative model trained only on text, given enough data and computational resources, could understand natural language in some non-trivial sense. 60% 0% 20% 40% 60% 80% 100% 67% 4-2. Multimodal models understand language Some multimodal generative model (e.g., one trained with access to images, sensor and actuator data, etc.), given enough data and computational resources, could understand natural language in some non-trivial sense. 47% 0% 20% 40% 60% 80% 100% 36%"
    },
    {
      "heading": "4-3. Text-only evaluation can measure language understanding",
      "text": "We can, in principle, evaluate the degree to which a model understands natural language by tracking its performance on text-only classification or language generation benchmarks."
    },
    {
      "heading": "Agree",
      "text": "Weakly agree Weakly disagree Disagree they would have preferred a set definition, but only 3-5% responded to any of these questions with QUESTION IS ILL-POSED. LMs understanding language is a known controversy (Q4-1, Q4-2) The question of whether language models can understand language (Q3-1) was split right down the middle, with 51% agreeing. This controversy is reflected in people's predictions as well, which average to an estimate of 49% agreement. Many more people (67%) agree once the model has access to multimodal data (images, etc.). As with the importance of AGI ( §4.3), whether language models understand language is known to be controversial, and the results of this survey can make it known that it is known. So whatever one's views are on the issue, it will likely be less useful to take those views for granted as a premise when communicating to a broad NLP audience in the public discourse or scholarly literature. Again, careful and considered discussion of the issue will likely be more productive for building common ground. Understanding may be learnable, but not measurable, using text (Q4-3) On the question of whether text-only evaluations can measure language understanding (Q4-3), the distribution of predictions was similar to that for language understanding by LMs (Q4-1), averaging 47% predicted agreement. However, unlike Q4-1, only 36% actually agreed with the statement, suggesting that many view it as a separate issue, and that some may believe that there are things which are learnable from text alone, but cannot be measured using text alone. Responses to questions in this section vary considerably with respondents' gender and location. On LMs understanding language (Q4-1), men are more likely to agree (58%) than women (37%), and people in the US are more likely to agree (61%) than those in Europe (31%). There is also a significant gender difference on Q4-3 regarding text-only evaluation of language understanding, where 43% of men agree as opposed to 21% of women."
    },
    {
      "heading": "PROMISING RESEARCH PROGRAMS (FIGURE 7)",
      "text": "In this section, we ask respondents about the kind of research they think the community should be doing, and which research directions they believe are not heading in the right direction. We choose research agendas to ask about based on criticisms, debates, or findings in the literature and public sphere, for example regarding current practice in benchmarking (Bowman & Dahl, 2021;Raji et al., 2021), the relative value of advances in model architectures (Narang et al., 2021;Tay et al., 2022), the use of language models for generation tasks (Bender et al., 2021), and explainability and interpretability of black-box models (Feng et al., 2018;Jain & Wallace, 2019;Wiegreffe & Pinter, 2019). Scaling and benchmarking are seen as over-prioritized (Q5-1, Q5-2) Over 72% of respondents believe that the field focuses too much on scale (Q5-1), a view that was underestimated at 58%. This reflects the same pattern as Q2-1, where the prevalence of pro-scale views is overestimated. An even stronger majority of 88% believe there is too much focus on optimizing performance on benchmarks (Q5-2), a view that is highly correlated with Q5-1 (see §5) and is similarly under-predicted at 65%. 58% 0% 20% 40% 60% 80% 100% 72% 5-1. There's too much focus on scale Currently, the field focuses too much on scaling up machine learning models. 65% 0% 20% 40% 60% 80% 100% 88% 5-2. There's too much focus on benchmarks Currently, the field focuses too much on optimizing performance on benchmarks. 38% 0% 20% 40% 60% 80% 100% 37% 5-3. On the wrong track: model architectures The majority of research on model architectures published in the last 5 years is on the wrong track. 37% 0% 20% 40% 60% 80% 100% 41% 5-4. On the wrong track: language generation The majority of research in open-ended language generation tasks published in the last 5 years is on the wrong track. 36% 0% 20% 40% 60% 80% 100% 50% 5-5. On the wrong track: explainable models The majority of research in building explainable models published in the last 5 years is on the wrong track. 35% 0% 20% 40% 60% 80% 100% 42% 5-6. On the wrong track: black-box interpretability The majority of research in interpreting black-box models published in the last 5 years is on the wrong track. 53% 0% 20% 40% 60% 80% 100% 82% 5-7. We should do more to incorporate interdisciplinary insights Compared to the current state of affairs, NLP researchers should place greater priority on incorporating insights and methods from relevant domain sciences (e.g., sociolinguistics, cognitive science, human-computer interaction). Agree Weakly agree Weakly disagree Disagree On the wrong track? Opinions vary (Q5-{3-6}) We ask whether four specific research directions are \"on the wrong track\": model architectures (Q5-3), open-ended generation tasks (Q5-4), explainable models (Q5-5), and black-box interpretability (Q5-6). Respondents are divided on these questions, with agreement rates between 37% and 50%, reflecting that these are controversial issues. In most cases, respondents' predictions also reflect this divide, with a possible exception in explainability (Q5-5), where 50% true agreement is under-predicted at 36%, reflecting that more community members are critical of research in explainable modeling than expected. While we deliberately used the vague phrase \"on the wrong track\" to get a sense of people's general attitudes, some respondents took issue with the framing of these questions; for example, one asks if it means asking the wrong question or finding the wrong solutions. As such, respondents' precise interpretations of these questions may vary. Interdisciplinary insights are valued more than we think (Q5-7) The largest disparity between predicted and actual results in this section is on Q5-7, stating that NLP researchers should do more to incorporate insights from relevant domain sciences. While respondents' predictions about the community's opinions split this issue down the middle (53%), in reality 82% agree with the view (an outcome only expected by 11% of respondents). This raises a question: If so many people agree that we should place greater priority on interdisciplinary work (Q5-7), why isn't more such work already happening? One possible explanation is that the responses to Q5-7 are a form of wishful thinking: Few believe that scale will be sufficient to solve our problems (Q2-1, Q5-1), and many think benchmarks are overemphasized (Q5-2) and insights from sciences like linguistics and cognitive science will be necessary for long-term progress (Q2-2, Q2-3). However, perhaps few know how to actually get results or useful insights from an interdisciplinary approach, leading this kind of work to be underrepresented in the literature and public discourse despite high demand for it. This suggests that the real issue may not be that NLP researchers do not assume interdisciplinary work has anything to offer so much as that we lack the knowledge and tools to make such work effective. 73% 0% 20% 40% 60% 80% 100% 89% 6-1. NLP's past net impact is good On net, NLP research has had a positive impact on the world. 71% 0% 20% 40% 60% 80% 100% 87% 6-2. NLP's future net impact is good On net, NLP research continuing into the future will have a positive impact on the world. 57% 0% 20% 40% 60% 80% 100% 59% 6-3. It is unethical to build easily-misusable systems It is unethical to build and publicly release a system which can easily be used in harmful ways. 59% 0% 20% 40% 60% 80% 100% 74% 6-4. Ethical and scientific considerations can conflict In the context of NLP research, ethical considerations can sometimes be at odds with the progress of science. 47% 0% 20% 40% 60% 80% 100% 25% 6-5. Ethical concerns mostly reduce to data quality and model accuracy The main ethical challenges posed by current ML systems can, in principle, be solved through improvements in data quality/coverage and model accuracy. 55% 0% 20% 40% 60% 80% 100% 48% 6-6. It is unethical to predict psychological characteristics It is inherently unethical to develop ML systems for predicting people's internal psychological characteristics (e.g., emotions, gender identity, sexual orientation). 58% 0% 20% 40% 60% 80% 100% 60% 6-7. Carbon footprint is a major concern The carbon footprint of training large models should be a major concern for NLP researchers. 35% 0% 20% 40% 60% 80% 100% 41% 6-8. NLP should be regulated The development and deployment of NLP systems should be regulated by governments. Agree Weakly agree Weakly disagree Disagree One caveat with this result is that responses vary significantly by job sector; 85% of those in academia agree with Q5-7 compared to 68% of those in industry, and our survey is mostly academics. Despite this difference, even the industry-only agreement rate is underpredicted, so survey response bias likely does not fully explain the mismatch."
    },
    {
      "heading": "ETHICS (FIGURE 8)",
      "text": "NLP is seen as good, and maybe extremely good (Q6-1, Q6-2) Respondents overwhelmingly regard NLP as having a positive overall impact on the world, both up to the present day (89%, Q6-1) and going into the future (87%, Q6-2). This strong endorsement of NLP's future impact stands in contrast with more substantial worries about catastrophic outcomes (36%, Q3-4). While the views are anticorrelated,foot_5 a substantial minority of 23% of respondents agreed with both Q6-2 and Q3-4, suggesting that they may believe NLP's potential for positive impact is so great that it even outweighs plausible threats to civilization. Whatever this means, it seems clear that many researchers think the stakes will be high in the near future when it comes to the impact of NLP research. Interestingly, agreement with Q6-1 and Q6-2 are both underpredicted by more than 15%, suggesting that pessimistic voices may be overrepresented in the public discourse. Responsibility for misuse: Researchers are somewhat split (Q6-3) In Q6-3, we ask respondents if they think \"it is unethical to build and publicly release a system which can easily be used in harmful ways.\" This is admittedly vague, and its answer depends on many factors (e.g., how \"easily\" the system can be used, how it is released, etc.). Our intent with the question is to get a sense of the degree to which respondents feel that researchers bear ethical responsibility for downstream misuse of the systems that they produce, and assess whether the community's views of itself are accurate in these terms. Responses are somewhat split, with a majority of 59% agreeing, and respondent predictions were reasonably accurate, averaging at 57% predicted agreement. But responses varied by gender, with 74% of women agreeing versus 53% of men. It is worth comparing Q6-3 to Article 1.1 of the ACM Code of Ethics, 6 which is adopted by the ACLfoot_8 and states (among other things): \"Computing professionals should consider whether the results of their efforts will... be used in socially responsible ways.\" Belief in ethical/scientific conflict is underestimated (Q6-4) When asked if ethical considerations can sometimes be at odds with scientific progress, 74% of respondents agreed-considerably more than the average predicted agreement rate of 59%. There are a couple of potential interpretations of disagreement with Q6-4. On one hand, respondents may believe any ethical problems that come up during the course of NLP research can be solved easily or are trumped by the benefits of scientific progress. On the other hand, they might believe that scientific 'progress' which is ethically regressive should not count as 'progress' or is inevitably pseudoscientific. Several views in line with the latter (and none with the former) were expressed in the survey feedback, suggesting that it is likely the dominant interpretation among those who disagree with Q6-4. As disagreement was significantly overpredicted by survey respondents, this view may be overrepresented in the public sphere relative to the proportion of NLP researchers who hold it. Reduction of ethics to data/accuracy is overestimated (Q6-5) In light of public debates about the sources and nature of the harms caused by machine learning systems (Kurenkov, 2020), we ask whether the main ethical challenges posed by current ML systems can be reduced to issues with data quality and model accuracy (Q6-5). It is estimated to be a common view, averaging 47% predicted agreement, but is actually fairly uncommon, with only 25% of respondents agreeing. Predicting psychological characteristics is controversial, with caveats (Q6-6) In light of discussions about surveillance and digital physiognomy (Agüera y Arcas et al., 2017), we ask whether it is inherently unethical to develop ML systems for predicting internal psychological characteristics like emotions, gender identity, and sexual orientation. Responses were split, with 48% agreeing. This question received a lot of critical feedback, and it is unclear how much of this split is due to differences in opinion versus interpretations of the question. Some respondents object to grouping transient states (e.g., emotion) with persistent traits (gender identity, sexual orientation), or say their answer depends on whether the trait is legally protected. Some say it depends on the inputs available to the model, and others say that it may not be inherently unethical but is ethically permissible in only a tiny set of carefully considered use cases. Which of these elements of context respondents assumed may have played a major role in determining their answers, and future surveys on these issues might benefit from splitting Q6-6 into several different questions. Carbon footprint is a concern for many (Q6-7) A majority of 60% agree with the statement that the carbon footprint of training large models should be a major concern for NLP researchers (Q6-8). This concern is based in part on trends in computation for machine learning at large scale, as Schwartz et al. (2020) note a 300,000x increase in computation over 6 years leading up to 2019. Following this, Patterson et al. (2022) argue that advances in model efficiency and energy management can soon lead to a plateau in energy use from training machine learning models. Both argue that accountability and reporting of energy use is important for keeping the future carbon footprint of training ML models under control. The responses to Q6-8 indicate that a majority of the community would likely appreciate explicit reporting of energy use in NLP publications as well as work that increases the compute efficiency of model training. Responses to this question varied greatly by gender, with 78% of women agreeing as opposed to 51% of men. NLP researchers are skeptical of regulation (Q6-8) Finally, we ask if the development and deployment of NLP systems should be regulated by governments (Q6-8). 41% of respondents agree, and while respondents' predictions are accurate on average, a large contingent (31%) of respondents predicted a very low agreement rate of 0-20%. We intend Q6-8 as a weak statement, i.e., that there should be any regulations around the development and deployment of NLP systems. However, respondents ask for more nuance, remarking that the answer depends on development versus deployment, details about use cases, and whether we only mean NLP-specific regulations or also include more general regulations on things like energy use or data privacy. As respondents may have come to this question with different assumptions or interpretations around such issues, it is hard to read into the specific implications of this result, except that respondents express a general skepticism of government regulation."
    },
    {
      "heading": "LIKELY SOURCES OF FUTURE ADVANCES (FIGURE 9)",
      "text": "In addition to the agree/disagree questions that constitute most of the survey, we also ask respondents where they think the most influential advances of the next 10 years will come from, providing four choices: Hardware and data scaling, Algorithmic improvements in ML, Data procurement and labeling practices, and Problem formulation and task design. We also provide an \"other\" option for people to specify their own answer. As the meta-question, we ask respondents to rank the answers from most popular (1) to least popular (5). 8The results reveal one surprise: the popularity of the top answer, Problem formulation and task design, was greatly underestimated. A plurality of 33% of respondents gave this answer, but it was only ranked first by 12% of people and it ranked third on average in respondents' predictions. Besides this, predictions roughly tracked reality (although noisy). This suggests that there is a fairly common but underappreciated belief in the NLP community that researchers should be working on new ways of formulating the problems we're trying to solve, and that such work could have high impact."
    },
    {
      "heading": "CORRELATION ANALYSIS",
      "text": "Having each survey respondent provide opinions on a wide range of issues gives us the opportunity to examine the relationships between these opinions: Which sets of beliefs often come together, and which don't? Also, are there demographic characteristics that correlate with certain beliefs? To get a sense of this, we compute pairwise Spearman (rank-order) correlations between pairs of questions ( §5.1) and demographic characteristics ( §5.2), and perform a clustering analysis on our results using PCA ( §5.3). Question Number 5.1 QUESTION CORRELATIONS Spearman (rank-order) correlations between agree/disagree questions are shown in Figure 10. To compute these, we order the response values as [DISAGREE, WEAKLY DISAGREE, OTHER, WEAKLY AGREE, AGREE], where OTHER includes INSUFFICIENTLY INFORMED ON THE ISSUE, QUESTION IS ILL-POSED, and PREFER NOT TO SAY.foot_10 Unsurprisingly, correlations tend to be stronger within a single section of the survey -for example, perspectives on linguistic structure and inductive bias ( §4.2), AGI ( §4.3), language understanding ( §4.4), and NLP's net impact on society ( §4.6, Q6-1, Q6-2). Beyond these, Table 1 shows the highest-magnitude correlations between questions in different survey sections. Concerns about private influence track concerns about scale The strongest cross-section correlation is between Q5-1 (there's too much focus on scale) and Q1-1 (private firms have too much influence, ρ s = 0.43). Regarding scale, Q5-1 was also moderately correlated with Q6-7 (carbon footprint is a major concern, ρ = 0.38). These correlations suggest that NLP researchers who see the influence of industry as problematic may hold this view in part because of concerns with the large-scale, compute-intensive research paradigm that is spearheaded largely by private firms. Believing that LMs understand language is predictive of belief in AGI and the promise of scale Agreeing that text-only models can meaningfully \"understand\" language (Q4-1) is predictive of several other views. This who agree with Q4-1 are more likely to believe that scaling has moved us towards AGI (Q3-2, ρ s = 0.42) and can solve practically any NLP problem with existing techniques (Q2-1, ρ s = 0.30), and less likely to believe that there is too much focus on scale (Q5-1, ρ s = -0.35), that linguistic structure is necessary to solve important NLP problems (Q2-2, ρ s = -0.38), or that we should do more to incorporate insights from domain sciences (Q5-7, ρ s = -0.30). This suggests the existence of distinct 'LM optimist' and 'LM pessimist' positions, where people either believe that scaling up could solve most NLP problems and potentially lead to AGI, or they think scaling is overprioritized, AGI is less likely, and we should be focusing more on seeking insights Table 1: Top 20 Spearman correlations (ρ s ) between pairs of questions from distinct categories (i.e., with distinct first numbers). Correlations |ρ s | > 0.11 are significant with p < 0.05. 3 5-4 5-5 5-6 5-7 6-1 6-2 6-3 6-4 6-5 6-6 6-7 6-8 Question Number United States United Kingdom Germany Canada Israel France China Non-binary Man Woman No Yes Academia (incl. students) Non-profit / government Industry (for-profit) No Yes Faculty/senior role Junior role/postdoc PhD student Masters student No Yes 1 4 5 20 21 Infrequent/recreational user I routinely post I follow but don't often post Twitter Ad at ACL Mailing list/messaging Word of mouth Email from ACL Other and methods from linguistics, cognitive science, or other domain sciences. It is worth emphasizing, though, that none of our measured correlations here are extremely strong; people hold diverse sets of opinions and individuals cannot be cleanly split into these camps."
    },
    {
      "heading": "DEMOGRAPHIC CORRELATIONS",
      "text": "Spearman correlations between demographic variables and agree/disagree questions are shown in Figure 11, with the top correlations by magnitude in Table 2. We rank agree/disagree answers as in §5.1, and for demographics, we treat each answer choice as a binary variable (1 if chosen by a respondent, 0 otherwise). We exclude demographic values for which we have fewer than 5 responses. Table 3: The top four components from running PCA on survey responses and demographic data, with human-written cluster labels, percent variance explained in parentheses, and the questions/data with the highest magnitude associations per component. We negate the statements with negative loadings so the statements for each component correspond to the set of beliefs that vary together. Membership in demographic groups does not strongly correlate with answers to any questions in the survey. The strongest correlation is ρ s = -0.25 (Table 2), smaller than the top 20 correlation coefficients between pairs of questions (Table 1). This suggests that there is a diversity of viewpoints in each demographic category, with more variation within demographics than between demographics. Nonetheless, there were a few demographics that were particularly predictive of responses. For example, Men and women answer many questions differently, especially regarding ethics and belief in the value of interdisciplinary research. In addition, under-represented minorities are more likely to agree that AI could have catastrophic consequences this century. Perhaps this reflects a worry that more powerful AI systems would have especially negative impacts on underrepresented minorities, or that minorities would be negatively affected before society as a whole is significantly affected."
    },
    {
      "heading": "CLUSTERING",
      "text": "To analyze the results beyond pairwise correlations, we identify clusters of opinions with principal component analysis (PCA). To do this, we linearize the agree/disagree questions along [-1, 1]: {DISAGREE → -1, WEAKLY DISAGREE → -0. 5, OTHER → 0, WEAKLY AGREE → 0.5, AGREE → 1}, where OTHER includes QUESTION IS ILL-POSED, INSUFFICIENTLY INFORMED ON THE ISSUE, and PREFER NOT TO SAY. For demographics, we treat every answer choice as a 0/1 binary variable as in §5.2. This process gives us a total of 101 features for each respondent. We run PCA using scikit-learn with 34 components, enough to explain 80% of the variance in the data. The variance in the data is fairly long-tailed, with the first 8 components covering 41.1% of the total variance and the remaining 26 explaining 38.9% (with less than 3% of variance explained by each component in the tail). This indicates that perspectives among NLP researchers may be difficult to reduce to a small number of opposing camps (e.g., pro-scale and anti-scale) without missing a great deal of internal disagreement within those groups. The top four principal components are shown in Table 3. The most prominent cluster of views in the data corresponds to the belief that we should (or shouldn't) be prioritizing large-scale modeling (\"Scaling Maximalism\"), explaining 11.7% of the variance in the data, and aligning with the \"LM optimist\" perspective proposed in §5.1. Another prominent theme is concern with the pace of progress, characterized by a belief that we are making steps towards AGI and that it is an important concern for NLP researchers. While other themes seem to appear in the components after that, no individual cluster of beliefs explains a large amount of the variance in the data, so these clusters are probably not very useful for directly reasoning about the beliefs of individuals, who are combinations of all principal components. As found in §5.2, demographic features are not as explanatory of the data as the agree/disagree questions are. Accordingly, they are not among the questions most strongly associated with the top clusters. The exception is the fourth component (\"Jaded Empiricism\"), for which one of the strongest associations is having a large number of publications, though this component only explains 4.0% of the total variance."
    },
    {
      "heading": "RELATED WORK",
      "text": "This work is directly inspired by the 2009 PhilPapers Surveys (Bourget & Chalmers, 2014), a survey and metasurvey of professional philosophers designed to discover and compare both their philosophical beliefs and their sociological beliefs about the philosophy community. More generally, introspective surveys to get a sense of the demographics and opinions of a field are common in many disciplines, such as economics (Fuchs et al., 1998;Illge & Schwarze, 2009;Frey et al., 2010, inter alia) and physics (Sivasundaram & Nielsen, 2016). We design our survey to fill a similar role for NLP, with a special focus on controversies where the field's perception of its views may not match reality, due for example to the influence of social media or a small number of dominant voices. The NLP Community Metasurvey overlaps slightly with other surveys of the NLP community, which are typically organized by ACL committees and focused on specific, timely issues related to the organization policy or logistics. Issues covered by such surveys include preprint posting practices (Foster et al., 2017), experiences with rolling review infrastructure (Schütze, 2022), ethical stances (Fort & Couillault, 2016;Benotti et al., 2022), attitudes towards energy use and environmental impacts (Arase et al., 2021), and language diversity in NLP research (Diab, 2022). Instead of focusing on a specific issue, our aim with the NLP Community Survey is to provide a broad sense of the distribution of views (and meta-views) held by NLP researchers. To do this, we cover a range of topics with existing debate in the literature (see the subsections of §4 for references). This is related to the the NLP Scholar project, which analyzes broad demographic (Mohammad, 2020c;2019), publication (Mohammad, 2020b), and citation (Mohammad, 2020a) trends in NLP and their interplay."
    },
    {
      "heading": "DISCUSSION",
      "text": "With the NLP Community Metasurvey, we have made concrete numbers of many contentious issues in NLP: the necessity of expert-designed inductive bias, the importance of AGI, whether language models understand language, and more. Perhaps more interestingly, we have also made concrete numbers of the community's impressions of these controversies, in some cases confirming what we already believe and in others producing surprises. For example, the idea that mere scale will solve most of NLP is much less controversial (and much less believed) than it is thought to be, and NLP researchers unexpectedly agree that we should do more to incorporate insights and methods from domain sciences, and that we should prioritize problem formulation and task design. Interestingly, very few of the issues we ask about (only the necessity of linguistic structure and expert-designed inductive bias, Q2-2 and Q2-3) are noticeably more controversial than respondents expected them to be. This could be due to biases from the amplification of controversy (e.g., in social media), or it could just reflect mundane biases in respondent predictions, e.g., regression towards the middle of the range (∼50%) under uncertainty. There are other biases to keep in mind when interpreting our results: people in the United States are overrepresented in our respondent population relative to ACL members as a whole, and senior researchers and academics are probably overrepresented as well (Appendix B), not to mention unmeasured population biases based in the personal networks of the authors. Many of the questions have multiple possible interpretations, many rely on vague terms like \"plausible\" or \"major concern,\" and some rely on comparisons to reference points such as the Industrial Revolution (Q3-3) or \"specific, non-trivial results from... linguistics or cognitive science\" (Q2-4) which may carry different implications for different readers. Given these issues, it is probably reasonable to view the answers to the questions on this survey as reflecting something between objective beliefs and signaling behavior. Agreement or disagreement with a particular statement may indicate where a respondent believes they stand relative to \"received wisdom,\" which would determine what statements would be worth asserting in the context of the status quo; or, a response could be driven by identification with (or rejection of) an already-known ideological camp that the statement is taken to refer to. While these issues affect the way we should interpret the absolute numbers in our results, they should apply equally to the meta-questions, so we believe it is meaningful to compare the survey's actual and predicted results as a way of discovering false sociological beliefs. We hope the results of the NLP Community Metasurvey can help us update our sociological beliefs to closer match reality, creating common ground for fruitful and productive discourse among NLP researchers as we confront these issues in the course of our work. Year of first NLP publication 0% 20% 40% 60% 80% 100% Percent of respondents (Cumulative) 0% 2% 4% 6% 8% 10% Percent of respondents Prefer not to say In what year was your first research work in NLP published? Include NLP work in non-ACL venues. Figure 13: Respondents' year of first NLP publication. Figures 14-17 show full results for the demographics questions. Results are restricted to those in the target demographic, i.e., with at least 2 *CL publications in the last three years. Numeric labels for percentages below 5% are omitted from the charts for space. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents United States United Kingdom Germany Israel China Europe (other) Asia / Pacific (other) Sub-Saharan Africa Middle East / North Africa (other) Canada South America / Caribbean Prefer not to say 189 14 16 36 6 20 7 20 3 4 1 11 58% 11% 6% 6% Where do you live? To preserve privacy, responses given by fewer than 10 people will be aggregated into larger geographical regions in reported results. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents Man Woman Non-binary Prefer not to say 218 81 19 9 67% 25% 6% What gender should you be grouped with in our analysis? 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents Yes No Prefer not to say 207 85 35 63% 26% 11% Do you consider yourself to be a member of an underrepresented minority group in NLP? Figure 14: Basic demographics."
    },
    {
      "heading": "C PILOT TESTING",
      "text": "The first author conducted 6 pilot studies with about 26 different participants from Computer Science and Linguistics departments, mostly based in the United States, during the months of February and March of 2022. After pilot participants took the survey, they were asked for feedback in a group Zoom call. Participants were asked about any questions they perceived as leading, reasons they might have refused to answer questions, reasons they might have wanted to stop taking the survey in the middle, and whether the purpose of the survey was clear, etc. The survey instructions and question wording were updated in accordance with their feedback."
    },
    {
      "heading": "D DATA CLEANING AND POSTPROCESSING",
      "text": "Likely Sources of Future Advances As mentioned in §4.7, 20% of respondents who answered the meta-question on likely sources of future advances ranked \"Other\" as the most common answer to the question. We assume these were mistakes, since it is unlikely that people would think a plurality of respondents would reject all of the (fairly broad) provided options. We take this, then, to mean the rankings provided by these respondents were probably reversed, with 5 being the most common and 1 the least common. So we reverse the rankings provided by these respondents for the purposes 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents Computational Social Science and C… Dialogue and Interactive Systems Discourse and Pragmatics Ethics and NLP Generation Information Extraction Information Retrieval and Text Mining Interpretability and Analysis of Model… Language Grounding to Vision, Rob… Linguistic Theories, Cognitive Modeli… Machine Learning for NLP Machine Translation and Multilinguality NLP Applications Phonology, Morphology, and Word S… Question Answering Resources and Evaluation Semantics: Lexical Semantics: Sentence-level Semantic… Sentiment Analysis, Stylistic Analysis… Speech and Multimodality Summarization Syntax: Tagging, Chunking and Pars… None of the above Prefer not to say 14% 19% 13% 13% 28% 18% 13% 31% 13% 17% 39% 26% 29% 12% 20% 27% 12% 21% 7% 6% 10% 12% 45 61 42 43 93 60 44 101 43 54 129 86 96 38 66 89 40 68 22 20 33 38 0 10 What subfields fit your work from the last 3 years? (Choose all that apply.) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents Yes No 314 13 96% Have you attended an ACL event in the last 3 years? Includes workshops, conferences, etc. held in 2019 or later. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents ≥2 327 100% How many publications have you co-authored in core ACL venues in the last 3 years? Include anything with a publication date 2019 or later in ACL, EMNLP, NAACL, EACL, AACL, TACL, or CL (including Findings). 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents 1-4 5-20 21+ Prefer not to say 126 154 44 3 39% 47% 13% How many total peer-reviewed publications do you have related to NLP? Figure 15: Respondents' research activities. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents Academia (including students) Industry (for-profit) Non-profit / government Prefer not to say 240 72 14 1 73% 22% What sector do you primarily work in? 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents Yes No Prefer not to say 290 31 6 89% 9% Is your job publication-oriented? 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents Faculty / senior managerial role Junior professional / postdoc PhD student Masters student Undergraduate Prefer not to say 134 107 74 4 6 2 41% 33% 23% What is your career position? Figure 16: Career demographics. 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents I routinely post I follow but don't often post I don't have a Twitter account, rarely… Prefer not to say 58 190 69 10 18% 58% 21% Do you use Twitter to engage with the NLP research community? 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents Twitter Email from the ACL Member Portal Broadcast on a mailing list, Slack ch… Word of mouth / personal communic… Advertisement at the ACL conference Other public social media post Prefer not to say 49 120 63 5 10 70 10 15% 37% 19% 21% How did you last hear about this survey before deciding to take it? 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% of analysis. Besides changing the \"Other\" statistics, this does not seem to have a noticeable effect on overall trends (Figure 18)."
    },
    {
      "heading": "Percent of respondents",
      "text": "1 2 3 4 5 Predicted Rank 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Percent of respondents Hardware and data scaling Algorithmic improvements in ML Data procurement and labeling … Problem formulation and task d… Other What area will the most influential advances of the next 10 years be in? Figure 18: We postprocessed the predicted rankings for likely sources of future advances by reversing the rankings given by all respondents who placed \"Other\" first (20% of respondents). The rankings for the unadjusted data are shown in the faded lines; besides the change to the \"Other\" line, overall trends are the same."
    },
    {
      "heading": "The NLP Community Metasurvey",
      "text": "This should take ~20 minutes to complete. For the first 1,000 respondents, we will donate $10 on your behalf to one of several non-profits that you choose at the end of the survey. This is a survey of opinions on issues being publicly discussed in NLP. We (researchers at UW and NYU) invite anyone doing NLP research to take it, though our primary target demographic is people who have authored or coauthored at least 2 publications in core ACL venues in the past 3 years. Please share this survey widely -we hope to cover as much of the target demographic as possible. For each statement, mark whether you agree or disagree. Then, you will report what percentage of community members you think agree with the statement. This will give a sense of whether our community's impression of itself aligns with its members' actual beliefs, and help us improve this alignment, communicate better, and motivate our work more effectively. More details about our motivation can be found at nlpsurvey.net. This was inspired by the PhilPapers surveys (philpapers.org/surveys)."
    },
    {
      "heading": "Agree",
      "text": "Weakly agree Weakly disagree Disagree Choose the answer that best reflects your views. In our analysis, we will interpret \"weakly agree\" and \"weakly disagree\" to include marginal views just Of those on the agree/disagree spectrum, what percentage of community members do you think will mark \"agree\" or \"weakly agree\" to each statement?"
    },
    {
      "heading": "State of the Field",
      "text": "For each of the following statements, mark the answer that best reflects your position. At the end of this section, you will predict what percentage of community members will have marked \"agree\" or \"weakly agree\" for each. Agree Weakly agree Weakly disagree Disagree Insufficiently informed on the issue Question is ill-posed Prefer not to say Agree/Disagree Other Powered by Qualtrics A (Optional) Any comments or feedback on this section? peak. I expect an \"NLP winter\" to come within the next 30 years, in which funding and job opportunities in NLP R&D fall by at least 50% from their peak. A majority of the research being published in NLP is of dubious scientific value. Author anonymity during review is valuable enough to warrant restrictions on the dissemination of research that is under review."
    }
  ]
}