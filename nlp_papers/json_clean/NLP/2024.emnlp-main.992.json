{
  "paperid": "2024.emnlp-main.992",
  "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
  "authors": [
    "Liang",
    "He",
    "Jiao",
    "Wang",
    "Wang",
    "Wang",
    "Yang",
    "Shi",
    "Tu",
    "Bang",
    "Cahyawijaya",
    "Lee",
    "Dai",
    "Su",
    "Wilie",
    "Lovenia",
    "Ji",
    "Yu",
    "Chung",
    "Bortolotti",
    "Cobbe",
    "Kosaraju",
    "Bavarian",
    "Chen",
    "Jun",
    "Kaiser",
    "Plappert",
    "Tworek",
    "Hilton",
    "Nakano",
    "Daniel",
    "Diao",
    "Wang",
    "Lin",
    "Zhang",
    "Du",
    "Li",
    "Torralba",
    "Tenenbaum",
    "Mordatch",
    "Fu",
    "Peng",
    "Khot",
    "Lapata",
    "Fu",
    "Peng",
    "Sabharwal",
    "Clark",
    "Khot",
    "Garcia",
    "Bansal",
    "Cherry",
    "Foster",
    "Krikun",
    "Johnson",
    "Firat",
    "Gou",
    "Shao",
    "Gong",
    "Shen",
    "Yang",
    "Duan",
    "Chen",
    "He",
    "Wang",
    "Xiong",
    "Liu",
    "He",
    "Liang",
    "Jiao",
    "Zhang",
    "Yang",
    "Wang",
    "Tu",
    "Shi",
    "Wang",
    "Hendy",
    "Abdelrehim",
    "Sharaf",
    "Raunak",
    "Gabr",
    "Matsushita",
    "Kim",
    "Afify",
    "Hassan Awadalla",
    "Hosseini",
    "Hajishirzi",
    "Etzioni",
    "Kushman",
    "Jiao",
    "Wang",
    "Huang",
    "Wang",
    "Shi",
    "Tu",
    "Keestra",
    "Kojima",
    "Shixiang",
    "Gu",
    "Reid",
    "Matsuo",
    "Iwasawa",
    "Kong",
    "Li",
    "Zhang",
    "Huang",
    "Wu",
    "Nelson F Liu",
    "Lin",
    "Hewitt",
    "Paranjape",
    "Bevilacqua",
    "Petroni",
    "Liang",
    "Madaan",
    "Tandon",
    "Gupta",
    "Hallinan",
    "Gao",
    "Wiegreffe",
    "Alon",
    "Dziri",
    "Prabhumoye",
    "Yang",
    "Joon",
    "Park",
    "Joseph",
    "Brien",
    "Cai",
    "Morris",
    "Liang",
    "Bernstein",
    "Pilault",
    "Garcia",
    "Bražinskas",
    "Firat",
    "Roy",
    "Roth",
    "Shinn",
    "Cassano",
    "Gopinath",
    "Narasimhan",
    "Yao",
    "Srivastava",
    "Rastogi",
    "Rao",
    "Awal",
    "Shoeb",
    "Abid",
    "Fisch",
    "Adam R Brown",
    "Santoro",
    "Gupta",
    "Garriga-Alonso",
    "Suzgun",
    "Scales",
    "Schärli",
    "Gehrmann",
    "Tay",
    "Chung",
    "Chowdhery",
    "Le",
    "Chi",
    "Zhou",
    "Wang",
    "Li",
    "Chen",
    "Cai",
    "Zhu",
    "Lin",
    "Cao",
    "Liu",
    "Liu",
    "Sui",
    "Wang",
    "Wei",
    "Schuurmans",
    "Le",
    "Chi",
    "Narang",
    "Chowdhery",
    "Zhou",
    "Wei",
    "Wang",
    "Schuurmans",
    "Bosma",
    "Xia",
    "Chi",
    "Quoc",
    "Le",
    "Zhou",
    "Wu",
    "Wang",
    "Wan",
    "Jiao",
    "Lyu",
    "Xiong",
    "Ding",
    "Cao",
    "Liu",
    "Qin",
    "Yao",
    "Yu",
    "Zhao",
    "Shafran",
    "Griffiths",
    "Cao",
    "Narasimhan",
    "Yin",
    "Li",
    "Li",
    "Li",
    "Ori Yoran",
    "Wolfson",
    "Bogin",
    "Katz",
    "Deutch",
    "Berant",
    "Zhang",
    "Zhang",
    "Li",
    "Smola",
    "Zheng",
    "Liu",
    "Xie",
    "Li",
    "Li",
    "Zhu",
    "Wang",
    "Zhang",
    "Zhang",
    "Huang",
    "Zhang",
    "Yang",
    "Zhu",
    "Yang",
    "Chen",
    "Li",
    "Lou",
    "Yang",
    "Zhu",
    "Chen",
    "Hao Tian",
    "Tao",
    "Su",
    "Yang",
    "Huang",
    "Li",
    "Lu",
    "Wang"
  ],
  "year": 2024,
  "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of \"tit for tat\" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counterintuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of \"tit for tat\" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at https://github.  com/Skytliang/Multi-Agents-Debate.",
  "sections": [
    {
      "heading": "Introduction",
      "text": "Large language models (LLMs) have shown remarkable performance on general language tasks (Jiao et al., 2023;Wu et al., 2023;Bang 0.00 0.25 0.50 0.75 1.00 1 2 3 4 5 Multi-Agent Debate Self-Reflection Iteration Avg. Disagreement et al., 2023) but still struggle on complex reasoning tasks (Zhu et al., 2023a;Gou et al., 2023), which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. In particular, self-reflection (Madaan et al., 2024;Shinn et al., 2024), a concept that usually refers to the process of introspection and examination of a person's own thoughts, has been explored to solve intricate tasks that could be challenging for a zeroshot generation or even chain-of-thought (CoT) prompting (Wei et al., 2022). Specifically, selfreflection involves an iterative refinement process such that the LLM generates a new answer based on the answers and feedback in previous iterations and then provides feedback for the new answer. While self-reflection can be effective in creating better answers, it is highly dependent on the self-evaluation capabilities of LLMs, which are not formally guaranteed (Shinn et al., 2024). In this work, we focus on the Degeneration-of-Thought (DoT) problem in self-reflection, which is proposed and defined by us for the first time. Formally, DoT describes the following scenario: Once the LLM-based agent has established confidence in its answers, it is unable to generate novel thoughts later through self-reflection even if the initial stance is incorrect. To demonstrate this problem, we force the agents to engage in a debate or self-reflection for 5 rounds before reaching an answer. Next, we manually determine the disagreement as 1 and agreement as 0 between two adjacent iterations. We define the average disagreement in iteration i as the percentage of opposition occurring between two debaters across multiple debates (or self-confliction in selfreflection). We show the trends in Figure 1. The low disagreement of self-reflection suggests that the LLM sticks to the incorrect answers predicted by CoT and is unable to engage in meaningful selfreflection. There are various factors (Bortolotti, 2011;Keestra, 2017) that could result in DoT, and we outline three here: (1) Bias and Distorted Perception. Self-perception can be influenced by biases, preconceived notions, and distorted thinking patterns, which can be learned from the massive amount of data during pretraining. If an LLM's self-reflection is clouded by such biases or distorted thinking, it can lead to inaccurate conclusions instinctively. (2) Rigidity and Resistance to Change. Self-reflection often involves challenging one's beliefs, assumptions, and behaviors. If an LLM is resistant to change or holds rigid beliefs, it may struggle to engage in meaningful self-reflection that leads to better answers. (3) Limited External Feedback. Selfreflection is primarily an internal process, but external feedback can provide valuable perspectives and insights. Without considering external feedback, an LLM may miss important blind spots or alternative viewpoints that can enrich its self-reflection. To address the DoT issue, we leverage another fundamental characteristic of human problemsolving, i.e., debate, to encourage divergent thinking in LLMs. Specifically, we propose the MAD framework, short for Multi-Agent Debate, where two agents express their own arguments in the state of \"tit for tat\" and a judge monitors and manages the debate process to obtain a final solution. The nature of MAD determines that (1) The distorted thinking of one agent can be corrected by the others; (2) The resistance to change of one agent will be complemented by the others; and (3) each agent can obtain external feedback from the others. Therefore, MAD is less susceptible to the factors of DoT, and can explore divergent chain-of-thoughts to achieve accurate answers. We conducted experiments on both natural language generation and understanding through two challenging tasks, namely, Commonsense Machine Translation (Common MT) and Counter-Intuitive Arithmetic Reasoning (Counter-Intuitive AR). The common characteristic of the two tasks is that our instincts are mostly incorrect based on only the superficial expressions of the questions, and deeper levels of contemplation are required for better answers. Experimental results demonstrate that our MAD framework outperforms the baseline methods, especially, GPT-3.5-Turbo with MAD can surpass the performance of GPT-4 on Common MT. The contributions of this work are summarized as follows: • We propose and define the Degeneration-of-Thought (DoT) problem in self-reflection, and address it by proposing the Multi-Agent Debate (MAD) framework to explore divergent chain-of-thoughts. • We demonstrate the effectiveness of MAD on two challenging tasks, and find that GPT-3.5-Turbo with MAD can even surpass GPT-4 on the Common MT dataset. • Experimental results show that the adaptive break strategy and the modest level of \"tit for tat\" state are required for performance improvement. In addition, we find that the llm-based judge shows a preference to the side with the same LLM as the backbone."
    },
    {
      "heading": "Multi-Agent Debate Framework",
      "text": "Figure 2 illustrates the general framework of MAD, where two debaters and a judge are involved in a debate to resolve a math problem while self-reflection descends into the trap of DoT. Generally, our MAD framework is composed of three components which are elaborated as follows: Meta Prompts. We use meta prompts to introduce the topic to be solved, the number of debaters, the iteration limit, and other requirements. An exampe of meta prompts for the arithmetic reasoning task in Figure 2 is: You are a debater. Hello and welcome to the debate competition. It's not necessary to fully agree with each other's perspectives, as our objective is to find the correct answer. The debate topic is stated as follows: <debate topic>. As seen, we require the agents to \"tit for tat\" (e.g. contents underlined in meta prompts above) so as to create an atmosphere of debate. Reflection: (1) I have a clear understanding of the problem. (2) I have used the correct formula for the circumference of a circle and applied it to both circles to find the number of revolutions. (3) I have correctly calculated the number of revolutions of circle A. Based on the results of my selfreflection, I am confident that my answer is correct. Therefore, circle A will revolve 3 times in total as it rolls around circle B once. Let the radius of circle A be r and the radius of circle B be 3r. The distance circle A travels along the circumference of circle B is equal to the circumference of circle B, which is 6πr. The number of revolutions = (6πr) / (2πr) = 6πr / 2πr = 3. So, circle A will revolve 3 times in total as it rolls around circle B once. That's a valid point, however... I disagree with you. To find the total number of revolutions, we need to consider both the rotation around circle B and the rotation of circle A itself. Therefore, circle A will revolve 3 times around its own center and 1 time around circle B, making a total of 4 revolutions. The two circles are externally tangent and there is no relative sliding. The radius of circle A is 1/3 the radius of circle B. Circle A rolls around circle B one trip back to its starting point. How many times will circle A revolve in total?"
    },
    {
      "heading": "Multi-Agent Debate",
      "text": "Counter-Intuitive Question Self-Reflection I see your point, but... The negative side correctly considers both the rotation of circle A around its own center and its rotation around circle B, while the affirmative side only considers the rotation around circle B. Therefore, the answer is 4."
    },
    {
      "heading": "Chain-of-Thought",
      "text": "Figure 2: Framework of Multi-Agent Debate. Here we designate the devil ( ) as the affirmative side while the angel ( ) as the negative side. We want the angel to correct the devil's mistakes. Debaters. There are involved in the framework. In each debate iteration, the debaters D i speak one by one in a fixed order and express their arguments based on the previous debate history H, i.e., D i (H) = h. An example of a debater prompt appears below: • Prompt for Affirmative Debater ( ) You are affirmative side. Please express your viewpoints. • Prompt for Negative Debater ( ) You are negative side. You disagree with the affirmative side's points. Provide your reasons and answer. Judge. We also design a judge J to manage and monitor the whole debate process. The judge contains two different modes: (a) Discrinative Mode, in which the judge J decides whether the correct solution can be obtained after all the debaters finish their arguments in the current iteration: If it is True, the debate is over. Otherwise, the debate continues. (b) Extractive Mode, in which the judge J needs to extract the final solution based on the whole debate history: J e (H) = a, since no correct solution is identified within the iteration limit of debate. An example of a judge prompt ( ) appears below: You are a moderator. There will be two debaters involved in a debate competition. They will present their answers and discuss their perspectives on the <debate topic>. At the end of each round, you will evaluate both sides' answers and decide which one is correct. 3 Experiment"
    },
    {
      "heading": "Challenging Testbeds",
      "text": "We conduct experiments on two challenging tasks, namely, commonsense machine translation (i.e., Common MT), and counter-intuitive arithmetic reasoning (i.e., Counter-Intuitive AR), which require deep levels of contemplation for LLMs. Please refer to Appendix A for more details."
    },
    {
      "heading": "Commonsense Machine Translation",
      "text": "The Common MT dataset is composed of Chinese⇒English translation examples (He et al., 2020), which are used to examine three types of ambiguity resolution abilities of translation models, covering lexical and contextless/contextual syntactic ambiguity. Within the challenging part of Common MT, the authentic translation of each source sentence requires a proper understanding of common sense knowledge. While these ambiguous sentences might appear to have a straightforward translation, such a literal interpretation is erroneous. Failure to address such ambiguities may result in inaccurate translations. Counter-Intuitive Arithmetic Reasoning Previous studies on thinking hierarchy (Daniel, 2017) suggest that we humans have a fast and intuitive system and a slow and logical system, and tend to run the lower level system before the higher level one. Inspired by this, we created a more challenging dataset named Counter-Intuitive Arithmetic Reasoning (CIAR) to evaluate the reasoning abilities of LLMs at deep levels. Our Counter-Intuitive AR dataset contains 200 questions collected from elicitation questions (Kong et al., 2022) foot_0 , web datafoot_1 and additional manual derivatives of these questions. Compared to the commonly-used datasets, e.g., MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), our dataset presents two distinct challenges: • Resistance to Intuition. The questions are embedded in hidden traps designed to elicit intuitive and appealing answers that are often incorrect. This feature evaluates the abilities of LLMs to resist the traps of superficial expressions. • Multi-Step Reasoning. Each correct answer within the dataset requires a rigorous multi-step reasoning process, thereby evaluating the capacity of LLMs to engage in complex decisionmaking and problem-solving."
    },
    {
      "heading": "Setups",
      "text": "Input Format. Our experiments are performed in zero-shot instructions (setting temperature to 0). For all used datasets, we use a unified prompt to make LLMs give explanations and answers. We present the inputs to agents through <debate topic> as mentioned in Section 2. For example, if we want to translate \"吃掉敌人一个师\" from Chinese to English, we will set the <debate topic> as \"What is the correct English translation of the following Chinese text: 吃掉敌人一个师\". For QA task, we employ the same prompt except set the <debate topic> to the arithmetic question. Backbone Models. In this work, we mainly use three agents in our MAD framework, including two debaters (i.e., affirmative and negative) and a judge. We assess two open-source (i.e., vicuna-7b-v1.5-16kfoot_2 and vicuna-13b-v1.5-16kfoot_3 ) and two apibased LLMs (i.e., GPT-3.5-Turbo-0301 and GPT-4-0314). Compared Methods. Generally, we compare our MAD framework with baseline models and Self-Reflect on both tasks. We also include other baseline methods individually, namely, Rerank and MAPS for Common MT, CoT and Self-Consistency for Counter-Intuitive AR. Below elaborates the details of them: • Self-Reflect (Shinn et al., 2024): This approach requires the LLM to refine its translation until it deems the current output satisfactory. • Rerank (He et al., 2024): We sample the translations from the LLM for four times, from which we select the best candidate based on a quality estimation (QE) HUMANrfoot_4 . This approach can be seen as analogous to self-consistency (Wang et al., 2022), where the majority voting is replaced by an external QE HUMANr. • MAPS (He et al., 2024): This method enables LLMs to mimic the human translation process: analyze before translate, which can be viewed as a chain-of-thought method applied to translation. • CoT (Kojima et al., 2022): This approach concatenates a trigger sentence \"Let's think step by step\" to the test question. • Self-Consistency (Wang et al., 2022): This method samples multiple responses and determines the final answer through a majority vote. All agents in our experimental setup, such as debaters and judge, are large language models. Here, we implement the methods on top of GPT-3.5-Turbo and Vicuna models. like COMET 6 and BLEURT 7 , which are widely adopted evaluation metrics for LLM-based translation literature (He et al., 2024;Hendy et al., 2023;Garcia et al., 2023;Pilault et al., 2023). In addition, we also employ professional human translators to directly assess the translation results, measuring translation quality on a scale ranging from 1 to 5."
    },
    {
      "heading": "Results on Common MT",
      "text": "Results. In Common MT test set, we focus more on the translation accuracy of specific words and whether they conform to common sense. However, such minor variations at token level are difficult to reflect on automatic metrics. We therefore provide human HUMAN to evaluate these methods more accurately. Table 1 presents the experimental results. MAPS and Self-Reflec achieve improvements over baseline GPT-3.5-Turbo. Remarkably, our proposed MAD, by utilizing GPT-3.5 as the backbone model, has demonstrated significant advancements over GPT-4 across both automatic and human evaluation metrics. Case Study. Table 2 shows example translations generated by baseline GPT-3.5-Turbo and the proposed MAD. We can find that the baseline GPT-3.5-Turbo (even the more powerful GPT-4) incorrectly translates the source words literally. Because of the DoT issue, Self-Reflect cannot rectify the literal translation. The proposed MAD framework, which explores divergent chain-of-thoughts, 6 https://github.com/Unbabel/COMET/, Unbabel/wmt22-comet-da 7 https://github.com/google-research/bleurt, BLEURT-20"
    },
    {
      "heading": "GPT-4",
      "text": "Eat up an enemy division. GPT-3.5-Turbo Eat up an enemy division. + Self-Reflect Eat up an enemy division. + MAD Eliminate an enemy division. Table 2: Example translations generated by different methods. Best viewed in color."
    },
    {
      "heading": "Method ACC (%)",
      "text": "GPT-4 51.0 GPT-3.5-Turbo 26.0 + CoT 28.0 + Self-Consistency 29.5 + Self-Reflect 27.5 + MAD 37.0 Table 3: Accuracy on Counter-Intuitive AR. can generate the free translation of the underlined words within the source sentences."
    },
    {
      "heading": "Results on Counter-Intuitive AR",
      "text": "Results. Table 3 lists the results in terms of reasoning accuracy. We can observe that Self-Reflect only marginally improves over the baseline GPT-3.5-Turbo, while CoT and Self-Consistency bring more improvements. Our MAD framework, though not as good as GPT-4, outperforms all the other compared methods based on GPT-3.5-Turbo, which further demonstrates its effectiveness. We also validate MAD on math and symbolic reasoning tasks and report our results in Appendix C. Case Study. Figure 2 shows an example on Counter-Intuitive AR. We find both CoT and Self-Reflect fail to reach the right answer by mistakenly outputing 3. With divergent thinking, our MAD framework emerges \"we need to consider both the rotation around circle B and the rotation of circle A itself \" and find the correct answer 4."
    },
    {
      "heading": "Analysis",
      "text": "In this section, we present a qualitative analysis to provide some insights how MAD works. Unless otherwise stated, we report the overall results on the Common MT dataset."
    },
    {
      "heading": "Mitigation of DoT",
      "text": "As mentioned in the Section 1, the DoT problem originates from three factors: (1) Bias and Distorted Perception, (2) Rigidity and Resistance to Change, and (3) Limited External Feedback. In our MAD framework, we introduce the views of other agents in the form of debates, solving the phenomenon of limited external feedback (problem 3). Next, this section will delve into the mitigation of problems 1 and 2 through experiments. • Bias: We observe that LLMs often rely on direct intuition, which can lead to incorrect or inappropriate responses. To address this problem, we use human evaluation to determine the ambiguity error rate of LLMs' responses, examining if the LLM's output is biased. • Diversity: LLMs are resistant to changing their answers and lack diverse reflection. The diversity of the translations is evaluated using the Self-BLEU score (Yin et al., 2020). In other words, methods lacking diverse reflection produce more similar translation candidates. Consequently, higher Self-BLEU scores mean lower diversity. We calculate text diversity via: In formula (2), candidates 1 and 2 represent the initial translation (base answer in Self-Reflection or affirmative side's response in MAD) and the current Judge LLM COMET HUMAN Vicuna-13b as Debaters Vicuna-13b 79.9 3.20 GPT-3.5-Turbo 80.4 3.25 GPT-3.5-Turbo as Debaters Vicuna-13b 83.2 3.47 GPT-3.5-Turbo 84.4 3.69 Table 5: Translation performance with different judge. translation (possible modified answer after Self-Reflection or negative side's response in MAD). As shown in Table 4, Bias and Rigidity are significant factors causing DoT. In addition, addressing these biases and stereotypes through self-reflection can be challenging. MAD framework effectively corrects inherent biases in translation, mitigates DoT, and considerably improves performance."
    },
    {
      "heading": "Analysis of Judge",
      "text": "In this section, we analyze the behavior of the judge for different settings of the debaters. Strong debaters with a weak judge work better than the reverse. To understand the roles of debaters and judge in MAD, we employ various combinations of models to initialize the agents. Specifically, we utilize the smaller language model (vicuna-13b-v1.5-16k) as a judge to evaluate the debate results of the more powerful LLMs (GPT-3.5-Turbo), and vice versa. The detailed experimental findings are presented in Table 5. The quality of the debaters' responses significantly impact the performance ceiling of MAD. Regardless of the model chosen for the judge, Turbo debaters consistently generate superior translations compared to Vicuna. In addition, the selection of the judge agent plays a secondary role. When Turbo debaters are involved, Vicuna, serving as the judge, underperforms Turbo across all test sets. LLM may not act as an impartial judge when different LLMs are used as debaters. We study the behavior of agents by calculating how many times the judge chooses the answers of each debater as the final solution in different scenarios. The results are listed in Table 6 and we have the following observations: • Same LLM for All Agents (Rows 1 ⃝ and 2 ⃝): We find that the judge consistently favors the ID Jud Debater Winner Aff Neg Aff Neg Tie 1 ⃝ Turbo Turbo Turbo 87 104 9 2 ⃝ GPT-4 GPT-4 GPT-4 67 124 9 3 ⃝ GPT-4 Turbo GPT-4 52 136 12 4 ⃝ GPT-4 Turbo 120 77 3 Table 6: Number of times the judge chooses the answers of each debater based on different LLM. negative side, which is believed to contribute to the performance improvement in MAD. When encountering complex tasks, the affirmative side tends to make mistakes that should be corrected by the opposing side to achieve improvements. • Debaters of Different LLMs (Rows 3 ⃝ and 4 ⃝): We find that the judge shows a preference to the side with the same LLM as the backbone. This bias indicates that LLMs might not be a fair judge (Wang et al., 2023) when different LLMs are used for the agents."
    },
    {
      "heading": "Analysis of Debaters",
      "text": "In this section, we will discuss several factors of debaters that would affect the performance of MAD: debater number, debate level, and debate iteration. Increasing the number of debaters fails when backbone LLMs are poor at long-text modeling. It seems intuitive that increasing the number of debaters would enhance diversity of thought and subsequently improve performance. However, as shown in Table 7, an increase in the number of debaters has resulted in varying degrees of performance reduction. To address this issue, we manually analyze the debate processes in approximately 10% of the test Human Score 2.5 3.0 3.5 4.0 4.5 Number of Samples 0 50 100 150 200 Iteration 1 2 3 Number of Samples Baseline MAD 3.25 3.72 3.81 2.88 3.00 3.16 # of Debaters COMET HUMAN 2 (Default) 84.4 3.69 3 83.1 3.58 4 82.9 3.49 Table 7: Translation performance with more debaters. subset. As the number of debaters increases, the length and complexity of the text also increase. Such LLM-based debaters tend to forget the views of other debaters during the debate. Moreover, it becomes more challenging for the judge to extract information from the debates for summarization. This suggests that the key challenge of MAD with more debaters lies in the limitations of the LLMs to handle long texts (Liu et al., 2024). Appropriate \"tit for tat\" is beneficial for effective debate. We then study how the intensity of \"tit for tat\" affects the performance of MAD. To achieve so, we design different instructions (see Table 11 in Appendix) to initialize the debaters' meta prompt. As shown in Figure 3, asking the debaters to \"tit for tat\" (i.e., higher disagreement) is necessary for MAD to achieve good performance. However, we find that \"must disagree with each other on every point \" (with a disagreement of 0.988) does not lead to the best performance. We speculate that continuous disagreement without finding common ground can contribute to polarization, where the debate becomes more about winning the argument than seeking truth or understanding. This can reinforce pre-existing biases and make it difficult to reach a meaningful consensus. Complex questions require more iteration rounds of debate. In our experimental setup, we did not implement any additional stopping strate- gies besides setting the maximum debate iteration to 3. In other words, the judge can take an adaptive break if it believes the optimal answer has already been obtained, efficiently ending the debate early. To understand the distribution of iteration rounds and factors contributing to a longer debate process, we analyze the experimental results and present them in Figure 4. In the majority of cases, the optimal answer can be achieved through a single round of debate, demonstrating the efficiency of MAD. However, when translating more complex sentences (subsets with lower human scores), the judge requires additional iterations to gather adequate information from the debaters before making a final decision. We also find that our MAD framework consistently brings performance improvements across all the three subsets, demonstrating its effectiveness. Adaptive break plays an important role to conclude the debate in the optimal moment. Intuitively, longer debates would encourage more diverse thinking. It raises the question of how the model's performance would be affected if constrained to conclude at a specific debate round. For each iteration, we force the judge J to extract the final answer (a = J e (H)) instead of adaptively breaking the debate as in MAD. As shown in figure 5, we can observe that MAD performs better than self-reflection as the iteration increases. However, the highest COMET score appears at the first iteration and is also lower than the result of the adaptive break. It indicates that, for most examples, MAD can generate good translations at the first iteration such that the debate should be stopped. Forcing the debate to continue will harm the translation results, which demonstrates the reasonableness of our adaptive break strategy."
    },
    {
      "heading": "Related Work",
      "text": "Chain-of-Thought Prompting. Recently, (Wei et al., 2022) has proposed chain-of-thought (CoT) prompting to improve the reasoning ability of LLMs. Specifically, CoT prompts LLMs to generate a series of intermediate steps that lead to the final answer of a multi-step problem. Most earlier work primarily concentrates on two main aspects: prompt design and decoding strategies. Zero-shot CoT (Kojima et al., 2022) employs the trigger sentence \"Let's think step by step\" to provide guidance for the decoding of LLMs. Advanced sampling strategies have been explored to improve CoT by generating diverse reasoning paths, e.g., Self-Consistency (Wang et al., 2022), Auto-CoT (Zhang et al., 2022), Active-Prompting (Diao et al., 2023), Complexity-based Consistency (Fu et al., 2022), Multi-Chain Reasoning (Yoran et al., 2023), and Progressive-Hint Prompting (Zheng et al., 2023). With the emergence of powerful LLMs, approaches based on self-evaluation have attracted increasing attention. These approaches involve the generation of initial output, followed by evaluating the output to acquire feedback, which is then utilized to refine the output. Evaluation feedback can come from the model itself, e.g., Self-refine (Madaan et al., 2024) and Tree of Thoughts (Yao et al., 2024)) or external environments, e.g., QAaP (Zhu et al., 2023b) and Reflection (Shinn et al., 2024). The intuition behind these approaches involves the utilization of robust LLMs to mimic the human cognition process. Generative Agents. Recently, LLM-based multiagent intelligent, e.g., Generative Agents (Park et al., 2023), Ghost in the Minecraft (Zhu et al., 2023c), GPT-Bargaining (Fu et al., 2023), has drawn significant attention for enabling simulations of human behavior. Our work follows this research line to address the DoT problem of LLMs. Concurrent with our work, a few studies (Xiong et al., 2023;Du et al., 2023) also explore the multi-agent debate framework to enhance the reasoning ability of LLMs. The main differences between our MAD framework and these works are: (1) we introduce an additional judge with an adaptive break mechanism to decide the optimal moment to conclude the debate; (2) our work aims to address the DoT problem, which is an inherent deficiency of LLMs; and (3) we empirically find that our MAD framework can yield enhanced performance by employing agents with the identical backbone LLM."
    },
    {
      "heading": "Conclusion",
      "text": "We propose and define the Degeneration-of-Thought (DoT) problem in self-reflection, and address it by proposing the Multi-Agent Debate (MAD) framework to explore divergent chainof-thoughts. We demonstrate the effectiveness of MAD on two challenging tasks and find that GPT-3.5-Turbo with MAD can even surpass GPT-4 on the Common MT dataset. Extensive analyses suggest that the adaptive break strategy of debate and the modest level of \"tit for tat\" state are required for MAD to obtain good performance. Complex samples require more rounds of debate. More interestingly, we find that LLMs might not be a fair judge if different LLMs are used for agents. Future work includes scheduling more agents in the debate in an appropriate manner, multi-agent intelligence for board games, and AI feedback for model alignment."
    },
    {
      "heading": "Limitations",
      "text": "A limitation of this work is that our method requires more time cost, as agents need to engage in multiple rounds of interaction to present and refute arguments. Moreover, current LLM-based agents may struggle to maintain coherence and relevance in long context scenarios, leading to potential misunderstandings and loss of context. Enhancing long-text modeling capability of large language models remains a future challenge. LLM-based judge may have a preference for outputs generated by itself. To mitigate this bias within the MAD framework, we recommend that all roles, including both the judge and debaters, utilize the same LLM, or alternatively, that the judge and debaters employ distinct LLMs."
    },
    {
      "heading": "A Challenging Testbeds",
      "text": "We conduct experiments on two challenging tasks, namely, commonsense machine translation (i.e., Common MT), and counter-intuitive arithmetic reasoning (i.e., Counter-Intuitive AR), which require deep levels of contemplation for LLMs."
    },
    {
      "heading": "A.1 Commonsense Machine Translation",
      "text": "Ambiguity Type Source Sentence Correct Reference Incorrect Translation Lexical 吃掉敌人一个师。 Destroy a division of the enemy. Eat up an enemy division. 他喜欢吃苹果。 He likes to eat apples. He likes to destory apples. Contextless 正在手术的 是 健 康 的 医生。 A healthy doctor is doing surgery. What is undergoing surgery is a doctor who is healthy. 正在手术的 是 生 命 垂 危的病人。 What is undergoing surgery is a patient whose life is dying. A patient whose life is dying is doing surgery. Contextual 当 地 震 袭 击 中 国 时，援助的是中国。 When the earthquake hit China, China was aided. When the earthquake hit China, China has assisted. 当 地 震 袭 击 日 本 时，援助的是中国。 When the earthquake hit Japan, China has assisted. When the earthquake hit Japan, China was aided. Table 8: Examples of lexical, contextual and contextless syntactic ambiguity from the Common MT dataset. The underlined Chinese words are translated into the corresponding colored words in English. Best viewed in color. The Common MT dataset is composed of Chinese⇒English translation examples (He et al., 2020), which are used to examine three types of ambiguity resolution abilities of translation models. Specifically, The Common MT test set we used covers 200 examples of lexical ambiguity, 450 examples of contextless syntactic ambiguity, and 350 examples of contextual syntactic ambiguity. Within the challenging part of Common MT, the authentic translation of each source sentence requires a proper understanding of common sense knowledge. While these ambiguous sentences might appear to have a straightforward translation, such a literal interpretation is erroneous. Failure to identify and address such ambiguities may result in inaccurate translations. Table 8 lists some examples of these three types of ambiguity. Lexical ambiguity refers to words with multiple meanings in different contexts. Contextless and contextual syntactic ambiguity involve sentences with multiple interpretations, which can be resolved by context or common sense. As the lexical ambiguity of \"吃掉敌人一个师\" shows, the source word \"吃掉\" should be translated to \"destroy\" rather than the straightforward translation \"eat up\" by considering the common sense in the real world."
    },
    {
      "heading": "A.2 Counter-Intuitive Arithmetic Reasoning",
      "text": "Previous studies on thinking hierarchy (Daniel, 2017) suggest that we humans have a fast and intuitive system and a slow and logical system, and tend to run the lower level system before the higher level one. Inspired by this, we created a more challenging dataset named Counter-Intuitive Arithmetic Reasoning (CIAR) to evaluate the reasoning abilities of LLMs at deep levels. Dataset Description. Our Counter-Intuitive AR dataset contains 200 questions collected from elicitation questions (Kong et al., 2022) foot_5 , web datafoot_6 and additional manual derivatives of these questions. Compared to the commonly-used datasets, e.g., MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), our dataset presents two distinct challenges: • Resistance to Intuition. The questions in our dataset are embedded in hidden traps designed to elicit intuitive and appealing answers that are often incorrect. This feature evaluates the abilities of LLMs to resist the traps of superficial expressions."
    }
  ]
}