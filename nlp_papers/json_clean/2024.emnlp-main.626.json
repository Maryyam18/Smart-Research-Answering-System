{
  "paperid": "2024.emnlp-main.626",
  "title": "ORPO: Monolithic Preference Optimization without Reference Model",
  "authors": [
    "Jiwoo Hong",
    "Noah Lee",
    "James Thorne",
    "Ebtesam Almazrouei",
    "Hamza Alobeidli",
    "Abdulaziz Al- Shamsi",
    "Alessandro Cappelli",
    "Ruxandra Cojocaru",
    "Mérouane Debbah",
    "Étienne Goffinet"
  ],
  "year": 2024,
  "abstract": "While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we revisit SFT in the context of preference alignment, emphasizing that a minor penalty for the disfavored style is sufficient for preference alignment. Building on this foundation, we introduce a straightforward reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the need for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across diverse sizes from 125M to 7B. Specifically, finetuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models including Llama-2 Chat and Zephyr with more than 7B and 13B parameters: achieving up to 12.20% on AlpacaEval 2.0 (Figure 1), and 7.32 in MT-Bench (Table 2). We release code 1 and model checkpoints 2 for Mistral-ORPO-α and Mistral-ORPO-β.",
  "sections": [
    {
      "heading": "Introduction",
      "text": "Pre-trained language models (PLMs) with vast training corpora such as web texts (Gokaslan and Cohen, 2019;Penedo et al., 2023) or textbooks (Li et al., 2023c) have shown remarkable abilities in diverse natural language processing (NLP) tasks (Brown et al., 2020;Zhang et al., 2022;Touvron et al., 2023;Jiang et al., 2023;Almazrouei et al., 2023). However, the models must undergo further tuning to be usable in downstream applications, typically through processes such as instruction tuning and preference alignment. Instruction-tuning (Wei et al., 2022;Taori et al., 2023;Wang et al., 2023;Zhou et al., 2023a) trains 1 GitHub: https://github.com/xfactlab/orpo 2 Models: ORPO collection models to follow task descriptions given in natural language, which enables models to generalize well to previously unseen tasks. However, despite the ability to follow instructions, models may generate harmful or unethical outputs (Carlini et al., 2021;Gehman et al., 2020;Pryzant et al., 2023). To further align these models with human values, additional training is required with pairwise preference data using techniques such as reinforcement learning with human feedback (Ziegler et al., 2020;Stiennon et al., 2022, RLHF) and direct preference optimization (Rafailov et al., 2023, DPO). Existing preference alignment methods typically consist of a multi-stage process, as shown in Figure 2, typically requiring a second reference model and a separate warm-up phase with supervised fine-tuning (SFT) (Ziegler et al., 2020;Rafailov et al., 2023;Wu et al., 2023), which adds additional resource overheads. We propose a novel alignment method, Odds Ratio Preference Optimization (ORPO), which adds a penalization term that steers the model away from learning undesired generation styles during SFT. In evaluation, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO results in higher win rates in AlpacaEval 2.0 , when compared to DPO with Phi-2 (2.7B) (+5.57%), and official checkpoints for Llama-2 (7B) Chat (+4.48%) and Llama-2 (13B) Chat (+1.74%), and Zephyrα (7B) (+2.98%), respectively. In our analyses, we demonstrate empirical superiority by comparing ORPO against established methods for model alignment, RLHF, and DPO for different datasets and model sizes. Further analysis from theoretical, empirical, and efficiency perspectives offers the following enhancements over previous methods: (1) requiring neither a reference model nor SFT stage and (2) less than half of the computational load in actual training. We release the training code and the checkpoints for Mistral-ORPO-α (7B) and Mistral-ORPO-β (7B), which achieve 7.24 and 7.32 in MT-Bench and 11.33% and 12.20% on AlpacaEval 2.0 , as shown in Figure 1 and Table 2."
    },
    {
      "heading": "Related Works",
      "text": "Alignment with reinforcement learning Reinforcement learning with human feedback (RLHF) commonly applies the Bradley-Terry model (Bradley and Terry, 1952) to estimate the probability of a pairwise competition between two independently evaluated instances. An additional reward model is trained to score instances. Reinforcement learning algorithms such as proximal policy optimization (PPO) (Schulman et al., 2017) are employed to train the model to maximize the score of the reward model for the chosen response, resulting in language models that are trained with human preferences (Ziegler et al., 2020;Stiennon et al., 2022;Ouyang et al., 2022). However, RLHF faces challenges of extensive hyperparameter searching due to the instability of PPO (Rafailov et al., 2023;Wu et al., 2023) and the sensitivity of the reward models (Gao et al., 2022;Wang et al., 2024). Therefore, there is a demanding need for stabler preference alignment algorithms. Alignment without reward model Recently proposed techniques for preference alignment mitigate the need for reinforcement learning (Rafailov et al., 2023;Song et al., 2023;Azar et al., 2023;Ethayarajh et al., 2024;Xu et al., 2024;Rosset et al., 2024). Rafailov et al. (2023) introduce direct preference optimization (DPO), which removes the reward modeling stage. Azar et al. (2023) prevented potential overfitting problems in DPO through identity preference optimization (IPO). Ethayarajh et al. (2024) and Cai et al. (2023) proposed Kahneman-Tversky Optimization (KTO) and Unified Language Model Alignment (ULMA) that does not require the pair-wise preference dataset, unlike RLHF and DPO. Song et al. (2023) and Xu et al. (2024) further suggest incorporation of the softmax value of the reference response set in the negative log-likelihood loss to merge the supervised finetuning and preference alignment. Alignment with supervised fine-tuning There have been approaches to build human-aligned language models by conducting supervised finetuning (SFT) only with filtered datasets (Zhou et al., 2023a;Li et al., 2023a;Haggerty and Chandra, 2024;Zhou et al., 2023b;Dong et al., 2023;Yuan et al., 2023;Gulcehre et al., 2023). Zhou et al. (2023a) demonstrated that SFT with a small amount of data with fine-grained curation could be sufficient for building helpful language model assistants. Furthermore, Li et al. (2023a) and Haggerty and Chandra (2024) proposed an iterative process of fine-tuning the supervised fine-tuned language models with their own generations after fine-grained selection of aligned generations and Zhou et al. (2023b) suggested that a curated subset of preference dataset is sufficient for alignment."
    },
    {
      "heading": "Preliminaries",
      "text": "Given a language model π θ and an input sequence x, the average log-likelihood of π θ generating the output sequence y is computed as: And the odds of generating the output sequence y given an input sequence x is defined as: where we use exponentiated average log-likelihood P θ (y|x) to represent the likelihood of response y given prompt x in the form of probability. We adopt the odds to model the preference given the likelihood of binary outcomes, preferred and dispreferred responses (Rafailov et al., 2023). While this is due to model a binary aspect of preferences, setting in which the odds ratio is applied, the use of average log-likelihood in alignment methods is widely studied for length regularization (Yuan et al., 2023;Park et al., 2024;Grinsztajn et al., 2024;Meng et al., 2024). Intuitively, odds θ (y|x) = k implies that it is k times more likely for the model π θ to generate the output sequence y than not generating it. Thus, the odds ratio of the chosen response y w over the rejected response y l , OR θ (x, y w , y l ), indicates how much more likely it is for the model θ to generate y w than y l given input x. -2.5 -2.4 -2.3 -2.2 0 5000 10000 15000 Training Step Log Probability Response Type Chosen Rejected Figure 3: Log probabilities for chosen and rejected responses during OPT-350M model fine-tuning on HH-RLHF dataset. Despite only chosen responses being used for supervision, rejected responses show a comparable likelihood of generation."
    },
    {
      "heading": "Revisiting Supervised Fine-tuning",
      "text": "Methods in RLHF often leverage SFT to ensure the stable update of the active policy (Schulman et al., 2017), using the SFT model as a reference policy. Even in non-RL alignment methods, empirical findings indicate that the SFT is crucial for achieving convergence to desired results (Rafailov et al., 2023;Tunstall et al., 2023). In detail, SFT uses cross-entropy loss to penalize the model if the predicted logits for the reference answers are low: where y i is a boolean value that indicates if ith token in the vocabulary set V is a label token, p i refers to the probability of ith token, and m is the length of sequence. Cross-entropy alone gives no direct penalty or compensation for the logits of nonanswer tokens (Lin et al., 2017) as y i will be set to 0. While cross-entropy is generally effective for domain adaptation (Mao et al., 2023), there are no mechanisms to penalize rejected responses when compensating for the chosen responses. Therefore, the log probabilities of the tokens in the rejected responses increase along with the chosen responses, which is not desired from the viewpoint of preference alignment."
    },
    {
      "heading": "Generalization over both response styles",
      "text": "We conduct a pilot study to empirically demonstrate the miscalibration of chosen and rejected responses with supervised fine-tuning alone. We fine-tune OPT-350M (Zhang et al., 2022) on the chosen responses only from the HH-RLHF dataset (Bai et al., 2022b). Throughout the training, we monitor the log probability of rejected responses for each batch and report this in Figure 3. Both the log probability of chosen and rejected responses exhibited a simultaneous increase. This can be interpreted from two different perspectives. First, the cross-entropy loss effectively guides the model toward the intended domain (e.g., dialogue). However, the absence of a penalty for unwanted generations results in rejected responses sometimes having even higher log probabilities than the chosen ones. Penalizing undesired generations Appending an unlikelihood penalty to the loss has successfully reduced unwanted degenerative traits in models (Welleck et al., 2019;Li et al., 2020). For example, to prevent repetitions, an unwanted token set of previous contexts, k ∈ C recent , is disfavored by adding the following term to (1p i ) to the loss which penalizes the model for assigning high probabilities to recent tokens. Motivated by SFT ascribing high probabilities to rejected tokens (Figure 3) and the effectiveness of appending penalizing unwanted traits, we design a monolithic preference alignment method that dynamically penalizes the disfavored response for each query without the need for crafting sets of rejected tokens."
    },
    {
      "heading": "Objective Function of ORPO",
      "text": "The objective function of ORPO in Equation 4 consists of two components: 1) supervised fine-tuning (SFT) loss (L SF T ); 2) odd ratio loss (L OR ) for the tuple d = (x, y l , y w ) ∼ D: L SF T follows the conventional NLL loss formulation in Equation (3) to maximize the likelihood of generating the reference tokens as previously discussed in Section 3.2. L OR maximizes the odds ratio between the likelihood of generating the favored response y w and the disfavored response y l : We wrap the log odds ratio with the log sigmoid function so that L OR could be minimized by increasing the log odds ratio between y w and y l . Weighting the L OR term with λ tailors the pretrained language model to adapt to the specific subset of the desired domain and disfavor generations in the rejected response sets. We further discuss the role of the odds ratio in learning the preference in Appendix D, showing that a simple probability ratio could lead to an excessive likelihood margin between the chosen and rejected responses."
    },
    {
      "heading": "Gradient Analysis",
      "text": "We provide the derivation of ∇ θ L OR in Appendix A. Analysis of the gradient of L OR justifies using the odds ratio loss. Where the two terms, have complementary roles: δ(d) penalizes the wrong predictions of the model and h(d) contrasts between chosen and rejected responses: When the odds of the favored responses are relatively higher than the disfavored responses, δ(d) in Equation 7will converge to 0. This indicates that the δ(d) will play the role of a penalty term, accelerating the parameter updates if the model is more likely to generate the rejected responses. In equation 8, h(d) implies a weighted contrast between the gradients from the chosen and rejected responses. Specifically, the term 1 -P (y|x) in the denominators amplifies the gradients when the corresponding side of the likelihood, P (y|x), is high. For chosen responses, this will accelerate the model's adaptation toward the distribution of chosen responses as the likelihood increases."
    },
    {
      "heading": "Training Configurations",
      "text": "Models and datasets We finetune state-of-theart pre-trained language models with ORPO by different scales, Phi-2 (2.7B) (Javaheripi and Bubeck, 2023), Llama-2 (7B) (Touvron et al., 2023) and Mistral (7B) (Jiang et al., 2023) on Binarized Ultra-Feedback (Tunstall et al., 2023). Furthermore, to assess the controlled scalability of ORPO, we finetune a series of OPT models (Zhang et al.,   (2023). We notate this by prepending \"+\" to each algorithm (e.g., +DPO). Further training details are in Appendix C. We filtered out instances where y w = y l or where y w = ∅, or y l = ∅. Reward models We train OPT-350M and OPT-1.3B on each dataset for a single epoch for reward modeling with the objective function in Equation 9 (Ziegler et al., 2020). The OPT-350M reward model was used for PPO, and OPT-1.3B reward model was used to assess the generations of finetuned models. We refer to these reward models as RM-350M and RM-1.3B in Section 5. -E (x,y l ,yw) [log σ (r(x, y w )r(x, y l ))] (9)"
    },
    {
      "heading": "Leaderboard Evaluation",
      "text": "In Section 5.1, we evaluate the models using the AlpacaEval 1.0 and AlpacaEval 2.0 (Li et al., 2023b) benchmarks, comparing ORPO to other instructiontuned models reported in the official leaderboard, 3 including Llama-2 Chat (7B) and (13B) (Touvron et al., 2023), and Zephyr α and β (Tunstall et al., 2023). Similarly, in Section 5.2, we evaluate the models with MT-Bench (Zheng et al., 2023) and 3 https://tatsu-lab.github.io/alpaca_eval/ report the results and the scores of the same models reported in the official leaderboard. 4 Using GPT-4 (Achiam et al., 2023) as an evaluator in AlpacaEval 1.0 , we assess if the trained model can be preferred over the responses generated from text-davinci-003. For AlpacaEval 2.0 , we used GPT-4-turbofoot_1 as an evaluator following the default setting. We assess if the generated responses are favored over those generated from GPT-4. Finally, using GPT-4 as an evaluator in MT-Bench, we check if the models can follow the instructions with hard answers in a multi-turn conversation."
    },
    {
      "heading": "Experimental Results",
      "text": "First, we assess the general instruction-following abilities of the models by comparing the preference alignment algorithms in single-turn (Section 5.1) and multi-turn (Section 5.2) instruction following benchmarks. Then, we compare ORPO against other alignment methods in the controlled setting, using OPT with various model sizes (Section 5.3)."
    },
    {
      "heading": "Single-turn Instruction Following",
      "text": "Phi-2 (2.7B) ORPO improved pre-trained Phi-2 to exceed the performance of the Llama-2 Chat instruction-following language model by only using UltraFeedback as the instruction-tuning dataset, as shown in Table 1. λ of 0.25 was applied for Phi-2, resulting in 71.80% and 6.35% in AlpacaEval. Llama-2 (7B) Notably, UltraFeedback and ORPO with λ of 0.2 on Llama-2 (7B) resulted in higher AlpacaEval scores than the chat versions of both 7B and 13B scale trained with RLHF, eventually showing 81.26% and 9.44% in both AlpacaEvals. In contrast, in our controlled experimental setting of conducting one epoch of SFT and three epochs of DPO following Tunstall et al. (2023) and Rafailov et al. (2023), Llama-2 + SFT and Llama-2 + SFT + DPO yielded models with outputs that could not be evaluated. This supports the efficacy of ORPO, in which the model can rapidly learn the desired domain and the preference with limited data. This aligns with the h(d) examination in the gradient of our method studied in Section 3.4. Mistral-ORPO-α (7B) Furthermore, fine-tuning Mistral (7B) with single-turn conversation dataset, UltraFeedback, and ORPO with λ of 0.1 outperforms Zephyr series, which are the Mistral (7B) models fine-tuned with SFT on 200K UltraChat (Ding et al., 2023) and DPO on the full Ultra-Feedback. As shown in Table 1, Mistral-ORPO-α (7B) achieves 87.92% and 11.33%, which exceeds Zephyr α by 1.98% and Zephyr β by 0.34% in AlpacaEval 2.0 . The sample responses and corresponding references from GPT-4 can be found in Appendix I. Mistral-ORPO-β (7B) Using the same configuration of Mistral-ORPO-α (7B), we additionally compare fine-tuning Mistral on the cleaned version of the UltraFeedback (Bartolome et al., 2023) to demonstrate the effect of the data quality. While the sizes of datasets are similar, ORPO gains further advantages from the dataset quality by scoring over 91% and 12% on AlpacaEval, as shown in Table 1. Further evaluations on instruction-following with IFEval (Zhou et al., 2023c) and four different benchmarks are in the Appendices E and F."
    },
    {
      "heading": "Multi-turn Instruction Following",
      "text": "With our best model, Mistral-ORPO-α (7B) and Mistral-ORPO-β (7B), we also assess the multi-turn instruction-following skills with deterministic answers (e.g., math) through MT-Bench. As shown in Table 2, Mistral-ORPO series exceeds larger instruction-following chat models, especially Llama-2-Chat (70B). Eventually, Mistral-ORPO-α (7B) and Mistral-ORPO-β (7B) scored 7.23 and 7.32 in MT-Bench without being exposed to the multi-turn conversation dataset during training, while Zephyr-β was trained on 200k multi-turn conversations during SFT. MT-Bench 1 st Turn 2 nd Turn Average Llama-2-7B Chat 6.41 6.13 6.27 Llama-2-13B Chat 7.06 6.24 6.65 Llama-2-70B Chat 6.99 6.73 6.86 Mistral-ORPO-α 7.49 6.96 7.23 Zephyr-β 7.68 6.98 7.33 Mistral-ORPO-β 7.64 7.00 7.32 Table 2: MT-Bench results of instruction-following language models. Mistral-ORPO-β surpasses or is on par with the models trained on more data."
    },
    {
      "heading": "Case Study with Smaller Models",
      "text": "We assess the win rate of ORPO over other preference alignment methods using different scales of OPT models, including supervised fine-tuning (SFT), PPO, and DPO, using RM-1.3B to understand the effectiveness of ORPO in Tables 3 and 4. Additionally, we visually verify that ORPO can effectively enhance the expected reward compared to SFT in Figure 4. OPT-1.3B 78.0 (0.16) 70.9 (0.52) 65.9 (0.33) Table 3: Average win rate (%) and its standard deviation of ORPO and standard deviation over other methods on HH-RLHF dataset for three rounds. Sampling decoding with a temperature of 1.0 was used on the test set. ORPO vs SFT +DPO +PPO OPT-125M 73.2 (0.12) 48.8 (0.29) 71.4 (0.28) OPT-350M 80.5 (0.54) 50.5 (0.17) 85.8 (0.62) OPT-1.3B 69.4 (0.57) 57.8 (0.73) 65.7 (1.07) Table 4: Average win rate (%) and its standard deviation of ORPO and standard deviation over other methods on UltraFeedback dataset for three rounds. Sampling decoding with a temperature of 1.0 was used. computational costs of DPO and ORPO in Section 6.1. Then, we analyze ORPO from the reward maximization objective in Section 6.2. Furthermore, we measure the lexical diversity of the models trained with ORPO and DPO in Section 6.3."
    },
    {
      "heading": "DPO",
      "text": "ORPO Training Time (hours) (↓) 12.6 5.5 Max Batch (↑) 1 4 Table 5: Computational costs of DPO and ORPO for 1 epoch on UltraFeedback using 2 NVIDIA A100 GPUs with AdamW and DeepSpeed ZeRO 3. We exclude SFT training time for DPO. As depicted in Figure 2, ORPO's efficiency de-rives from two aspects: 1) no reference model and 2) no separate SFT stage. In that sense, ORPO is computationally more efficient than RLHF and DPO in both time and memory. We demonstrate this through controlled training on DPO and ORPO with 2 NVIDIA A100 GPUs using DeepSpeed ZeRO 3 (Rajbhandari et al., 2021) and AdamW (Loshchilov and Hutter, 2019) on Mistral (7B). As shown in Table 5, ORPO reduces 56.3% of training time compared to that of DPO and was able to fit a four times larger batch size per device. Furthermore, as DPO typically requires a pre-supervised fine-tuning (SFT) stage, the actual computational efficiency difference becomes more drastic as the SFT datasets are quite largefoot_3 . The reference model (π SF T ) in the context of RLHF and DPO denotes the model trained with SFT, which will be the base model for updating the parameters with PPO or DPO (Ziegler et al., 2020;Rafailov et al., 2023). Thus, two π SF T s, a frozen reference model and the model undergoing tuning, are required during training. Furthermore, in theory, two forward passes should be calculated for each model to acquire the logits for the chosen and rejected responses. In other words, four forward passes happen for a single batch. On the other hand, a reference model is not required in ORPO as π SF T is directly updated. This leads to half the number of forward passes required for each batch."
    },
    {
      "heading": "Overall Reward Distribution",
      "text": "In addition to the reward model win rate discussed in Section 5.3, we compare the reward distribution of the responses generated with respect to the test set of the UltraFeedback and HH-RLHF in Figure 4. Regarding the SFT reward distribution as a default, PPO, DPO, and ORPO shift it in both datasets. However, the magnitude of reward shifts for each algorithm differs. In Figure 4, RLHF (i.e., SFT + PPO) has some abnormal properties of the distribution with a low expected reward. We attribute this to empirical evidence of the instability and reward mismatch problem (Rafailov et al., 2023;Gao et al., 2022;Shen et al., 2023) as the RLHF models were trained with RM-350M and assessed with RM-1.3B. Meanwhile, it is notable that the ORPO distribution (red) is mainly located on the very right side of each subplot, indicating higher expected rewards. Recalling the intent of preference alignment, the distributions in Figure 4 indicate that ORPO tends to fulfill the aim of preference alignment for all model sizes."
    },
    {
      "heading": "Lexical Diversity",
      "text": "The lexical diversity of the preference-aligned language models was studied in previous works (Kirk et al., 2024). We expand the concept of per-input and across-input diversity introduced in Kirk et al. (2024) by using Gemini-Pro (Gemini Team et al., 2023) as an embedding model, which is suitable for assessing the diversity of instruction-following language models by encoding a maximum of 2048 tokens. The diversity metric with the given set of sampled responses is defined as:"
    },
    {
      "heading": "D(O",
      "text": "where cos(h i , h j ) refers to the cosine similarity between the embedding h i and h j . 5 different responses are sampled with a temperature of 1.0 to 160 queries in AlpacaEval (i.e., K = 5, N = 160) using Phi-2 and Llama-2 trained with ORPO and DPO. We report the results in Table 6. Per Input Diversity (PID) We average the inputwise average cosine similarity between the generated samples with Equation 12to assess the perinput diversity. In Table 6, ORPO models have the highest average cosine similarity in the first column for both models, which implies the lowest diversity per input. This indicates that ORPO generally assigns high probabilities to the desired tokens, while DPO has a relatively smoother logit distribution. Across Input Diversity (AID) Using 8 samples generated per input, we sample the first item for each input and examine their inter cosine similarity with Equation 13for across-input diversity. Unlike per-input diversity, it is noteworthy that Phi-2 (ORPO) has lower average cosine similarity in the second row of Table 6. We can infer that ORPO triggers the model to generate more instructionspecific responses than DPO."
    },
    {
      "heading": "Minimizing L OR",
      "text": "We demonstrate that models trained with ORPO learned to reflect the preference throughout the training process. We monitored the log probabilities of the chosen and rejected responses and the log odds ratio with λ = 1.0. With the same dataset and model as Figure 3, Figure 5 shows that the log probability of rejected responses is diminishing while that of chosen responses is on par with Figure 3 as the log odds ratio increases. This indicates that ORPO is successfully preserving the domain adaptation of SFT while the penalty term L OR induces the model to lower the likelihood of unwanted generations. We discuss the effect of λ in Equation 4in Appendix G, studying the proclivity of the log probability margin between the favored and disfavored responses with respect to λ."
    },
    {
      "heading": "Conclusion",
      "text": "In this paper, we introduced a reference-free monolithic preference alignment method, odds ratio preference optimization (ORPO), by revisiting and understanding the value of the supervised fine-tuning (SFT) phase in the context of preference alignment. ORPO was consistently preferred by the fine-tuned reward model against SFT and RLHF across the scale, and the win rate against DPO increased as the model size increased. Furthermore, we validate the scalability of ORPO with 2.7B and 7B pre-trained language models by exceeding the larger state-of-the-art instruction-following language models in AlpacaEval. Specifically, Mistral-ORPO-α and Mistral-ORPO-β achieved 11.33% and 12.20% in AlpacaEval 2.0 , 7.23 and 7.32 in MT-Bench, thereby underscoring the efficiency and effectiveness of ORPO."
    },
    {
      "heading": "Limitations",
      "text": "While conducting a comprehensive analysis of the diverse preference alignment methods, including DPO and RLHF, we did not incorporate a more comprehensive range of preference alignment algorithms. We leave the broader range of comparison against other methods as future work, along with scaling our method to over 7B models. In addition, we will expand the fine-tuning datasets into diverse domains and qualities, thereby verifying the generalizability of our method in various NLP downstream tasks. Finally, we would like to study the internal impact of our method on the pre-trained language model, expanding the understanding of preference alignment procedure to not only the supervised fine-tuning stage but also consecutive preference alignment algorithms. A Derivation of ∇ θ L OR with Odds Ratio Suppose that g(x, y l , y w ) = odds θ P (yw|x) = σ ′ (log g(x, y l , y w )) σ (log g(x, y l , y w )) = σ (-log g(x, y l , y w )) g(x, y l , y w ) • ∇ θ g(x, y l , y w ) = σ (-log g(x, y l , y w )) • ∇ θ log g(x, y l , y w ) = 1 + odds θ P (y w |x) odds θ P (y l |x) • ∇ θ log odds θ P (y w |x) In Equation 18, the remaining derivative can be further simplified by replacing 1 -P θ (y|x) terms where P (y|x) = N N t P θ (y t |x, y <t ) in odds θ (y|x) as follows. = -P θ (y|x) 1 -P θ (y|x) • ∇ θ log P θ (y|x) As an AI developed by OpenAI, I don't have personal opinions. However, I can share that ChatGPT is an advanced language model that has received considerable attention for its ability to generate human-like text. It's designed to assist users in a wide range of tasks, from answering questions to creating content and engaging in conversation. ChatGPT has been praised for its fluency and versatility in handling various topics and for its potential to streamline and enhance communication in both personal and professional settings. However, like any technology, it also faces challenges and criticisms, such as ensuring the accuracy of information, preventing misuse, and managing ethical considerations surrounding AI-generated content. Overall, ChatGPT represents a significant achievement in natural language processing and has sparked discussions about the future of AI in our daily lives."
    },
    {
      "heading": "Mistral-ORPO-β:",
      "text": "As an AI language model, I am an example of the technology used in ChatGPT. I can provide a personal opinion, but please keep in mind that I am designed to be socially unbiased and positive in nature. ChatGPT is an impressive advancement in artificial intelligence and natural language processing. It has the ability to understand and generate human-like text, making it possible for users to have engaging and informative conversations with the AI. This technology has the potential to revolutionize various industries, including customer support, education, content creation, and more. However, it's important to recognize that ChatGPT, like any AI technology, is not perfect. It may sometimes provide inaccurate or biased information due to the limitations of its training data or the complexity of human language. It's crucial to verify the facts and use critical thinking when interacting with AI systems like ChatGPT. Overall, ChatGPT represents a significant step forward in the field of AI, and its potential applications are vast. As a responsible user, it's important to approach this technology with a critical and respectful mindset, recognizing its strengths and limitations."
    }
  ],
  "figures": [],
  "equations": []
}