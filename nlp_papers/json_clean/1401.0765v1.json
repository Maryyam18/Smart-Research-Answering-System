{
  "paperid": "1401.0765v1",
  "title": "A Survey of Techniques For Improving Energy Efficiency in Embedded Computing Systems",
  "authors": [
    "Sparsh Mittal",
    "H Ghasemzadeh",
    "R Jafari",
    "W Dally",
    "J Balfour",
    "D Black-Shaffer",
    "J Chen",
    "R Harting",
    "V Parikh",
    "J Park"
  ],
  "year": 2014,
  "abstract": "Recent technological advances have greatly improved the performance and features of embedded systems. With the number of just mobile devices now reaching nearly equal to the population of earth, embedded systems have truly become ubiquitous. These trends, however, have also made the task of managing their power consumption extremely challenging. In recent years, several techniques have been proposed to address this issue. In this paper, we survey the techniques for managing power consumption of embedded systems. We discuss the need of power management and provide a classification of the techniques on several important parameters to highlight their similarities and differences. This paper is intended to help the researchers and application-developers in gaining insights into the working of power management techniques and designing even more efficient high-performance embedded systems of tomorrow.",
  "sections": [
    {
      "heading": "Introduction",
      "text": "Recent years have witnessed a phenomenal growth in features and applications of embedded systems. Embedded systems such as mobile computing systems now offer integration of video camera, net browser, wireless data modem and phone. Further, it has been estimated that the number of mobile devices has now become almost equal to the population of the world [1]. These trends, however, have also presented significant challenges for managing power consumption of embedded systems. Many portable systems have stringent power budgets, such as 1 or 2W, while ultra low-power embedded systems (e.g. wearable systems) have a power budget of a few milli watts [2]. This is in sharp contrast with general-purpose and graphics processors which have the power budget of up to a few hundred watts [3]. The low power budgets of embedded systems present severe demands for improving their energy efficiency. As an example, a 3G mobile phone receiver requires nearly 40 GOPS (giga operations per second) to handle a 14.4 Mbps channel and doing this in a power budget of 1W would require energy efficiency of 25pJ per operation [4]. Thus, to continue to scale their performance and ensure reliability, longevity and adoption in wide range of applications, power management has become extremely important for embedded systems. In this paper, we highlight the need of power management in embedded systems and survey several research works which are aimed at improving energy efficiency of embedded systems. To provide insights into the working of these techniques, we classify them on the basis of their key research idea. We believe that this survey will help the researchers and designers in understanding the state-of-the-art in power management of embedded systems and also motivate them to further improve the energy efficiency of embedded systems. In a paper of this length, it is not possible to do justice to the broad range of developments in the field of embedded systems and hence, we take the following approach to limit the scope of the paper. We include only those research works that propose methods for improving energy efficiency and also evaluate it. Those works which only evaluate performance improvement are not included although they may also lead to better energy efficiency. We review application and architectural level techniques and not circuit-level techniques. Since different techniques have been evaluated using different platforms and methodologies, we only focus on their fundamental research idea and do not present the qualitative results. The remainder of this paper is organized as follows. Section 2 provides a background on embedded systems and also highlights the need of power management. Section 3 provides an overview and classification of power management techniques in embedded systems. Section 4 discusses some of these techniques in detail. Finally, Section 5 provides concluding remarks and also discusses the future challenges."
    },
    {
      "heading": "Background",
      "text": "An embedded system is a computing system which is designed for specific control functions and is embedded as part of the complete device which may include hardware and mechanical parts. Thus, in contrast with general-purpose computers (e.g. desktop), an embedded system performs a few pre-defined tasks, with very specific requirements. Typical examples of embedded systems include MP3 players, smart cameras and cellular phones."
    },
    {
      "heading": "Sources Of Power Consumption",
      "text": "We briefly review the sources of power consumption in embedded systems and refer the reader to previous work [5,6] for more details. The power consumption of embedded systems can be broadly divided in two categories, namely dynamic power and static power. The dynamic power (P dyn ) consumption arises from charging and discharging of the load capacitance, and the short circuit currents. The leakage power (P leak ) arises due to leakage currents that flow even when the device is inactive. Thus, we have Here α shows the switching activity, F shows the operating frequency and V shows the operating voltage. I leak shows the leakage current. With CMOS scaling the leakage power is increasing dramatically [7]. DVFS based techniques work by reducing dynamic energy, while the techniques which transition the system to low-power aim to reduce leakage energy."
    },
    {
      "heading": "Importance of power management",
      "text": "Power management in embedded systems is important for the following reasons."
    },
    {
      "heading": "Limited Size and Battery",
      "text": "For battery-operated mobile embedded systems, energy supply is a crucial limitation. Power consumption leads to heating, which is unacceptable in several domains such as wearable embedded systems. Further, the small size of these systems also limits the amount of heat-dissipation that can be managed. Smaller power consumption enables use of smaller power supplies and reduced heatdissipation overhead, which also reduces the cost, weight and area of embedded systems. Thus power management can lead to easier system design."
    },
    {
      "heading": "Ensuring Longevity",
      "text": "A 15 degree Celsius rise in temperature increases the device failure rates by up to a factor of two [8]. Thus, power dissipation has deleterious effect on reliability of embedded systems and this phenomenon may be crucial for medical devices and mission-critical systems."
    },
    {
      "heading": "Addressing Inefficiency Arising due to Overprovisioning of Resources",
      "text": "In embedded systems, idle intervals arise for several reasons, such as pessimistic estimate of worst-case execution time and inherent slack due to relaxed deadline etc. Despite this, the designers need to provision resources to meet the worst-case performance requirement which leads to energy wastage. Thus, dynamic energy saving techniques can use runtime adaption to trade performance for saving energy. Also, since the embedded systems are typically used for welldefined applications, static techniques can be easily used for per-application tuning of resources."
    },
    {
      "heading": "Meeting Performance Requirements",
      "text": "In recent years, embedded processors are used to execute resource-intensive applications (e.g. multimedia processing [9][10][11]) that were originally designed for general-purpose processors. To meet these performance demands, modern embedded processors use many complex features such as multi-cores, multi-level caches etc. [12][13][14][15][16]. These trends have influenced the design of embedded systems to be optimized for higher performance, instead of lower power consumption."
    },
    {
      "heading": "Power Challenges Posed by CMOS Scaling",
      "text": "The advancements in CMOS technology have greatly increased the on-chip transistor densities and speeds. These trends have led to a technology-imposed utilization wall which limits the fraction of the chip that can be simultaneously used at full speed within the power budget. Thus, today the processor performance is primarily constrained by energy efficiency and it has been estimated that, if left addressed, power challenges may end future performance scaling [17,18]. Conversely, techniques for improving energy efficiency can enable the designers to scale performance by executing parallel computations without violating the power budget."
    },
    {
      "heading": "Trends in Usage Pattern",
      "text": "In recent years, mobile computing devices have become the key platform for the mobile convergence applications, e.g. web browsing, imaging, and video streaming. Due to these features, embedded systems have become ubiquitous. Thus, while an individual portable system consumes much less power than a server in the data center, the large user-base of embedded systems makes their total power consumption very high."
    },
    {
      "heading": "Enabling Green Computing",
      "text": "It has been estimated that the ICT (Information and communications technology) contributes nearly 3% in the overall carbon footprint [19]. Thus, power management in embedded systems is also important for achieving the goals of green computing."
    },
    {
      "heading": "Overview",
      "text": "Based on their main energy saving approach, we classify the techniques into following categories. 1. DVFS (dynamic voltage and frequency scaling) and power-aware scheduling based techniques . 2. Using low power modes, called power mode management (PMM) [25,27,32,37,56,73,[75][76][77][78][79][80][81]. Note that this is also sometimes referred to as 'dynamic power management'. We however use a more specific term 'power mode management' to avoid any confusion. 3. Microarchitectural techniques for saving energy in specific components e.g. main memory ( [82][83][84]), cache [6,16,35,[85][86][87][88][89][90][91][92][93][94][95][96][97][98][99][100][101][102][103], scratchpad memory [104], TLB [105] or making other changes to memory hierarchy e.g. adding extra components [106,107]. 4. Using unconventional-cores such as DSP or GPUs of FPGAs [108][109][110][111][112][113][114][115][116][117]."
    },
    {
      "heading": "Power Management Techniques",
      "text": "We now discuss some power management techniques in detail. As evident from the classification in previous section, several techniques can be classified in more than one category (e.g. DVFS and PMM). For sake of brevity, we discuss them in one category only."
    },
    {
      "heading": "DVFS and Power-Aware Scheduling based Techniques",
      "text": "For sake of convenience, we discuss power-aware scheduling techniques along with DVFS since these techniques often make scheduling decisions with a view to use DVFS for saving energy. DVFS is a technique for altering the voltage and/or frequency of a computing system based on performance and power requirements. For CMOS circuits, dynamic power is related with voltage and frequency as P ∝ F V 2 and hence, by reducing the frequency, the voltage at which the circuit needs to be operated for stable operation can also be lowered, which leads to energy saving. Several commercial microprocessors support DVFS technology for saving power, e.g. AMD PowerNow and and Intel's SpeedStep. The limitation of DVFS is that it harms the performance and hence, it may increase execution time or lead to missed deadlines. Also, DVFS requires programmable clock generator and DC-DC converter which incur energy overhead. Further, voltage transitions may require time on the order of tens of microseconds ( [30]). Finally, due to increase in leakage energy and trend of using multi-core processor instead of increasing clock frequency, the returns from DVFS are diminishing. Hua et al. [44] study the opportunity available for saving energy in embedded systems using DVFS. Since supporting a large number of voltage levels causes overhead (e.g. area and power overhead of voltage regulators, overhead of transitions), their work explores the optimal number of voltage levels and their values to implement on the multiple-voltage system for achieving energy efficiency. They have shown that systems which provides 3 or 4 voltages are nearly as energy efficient as the ideal system that can vary the voltage arbitrarily. Kianzad et al. [54] use genetic algorithm to integrate task scheduling and voltage scaling under a single iterative optimization loop. Their technique searches the solution space to find an assignment and ordering of tasks on each processing element and generates a schedule such that deadline constraints are met and the power consumption is minimized. Further, their technique distributes the slack proportionately to different tasks and uses DVFS to save energy. They propose techniques for saving energy in both homogeneous and heterogeneous multiprocessor embedded systems. In several multimedia applications, missing some task deadlines can be acceptable since it remains unnoticed to human visual and auditory system. Hua et al. [45] utilize this fact, along with the information on statistical task execution time to propose techniques to save energy in embedded systems by dynamic voltage scaling. They have proposed two algorithms. The first algorithm ensures achieving highest completion ratio with lowest possible energy consumption. The second algorithm deliberately drops some tasks to create slack for saving additional energy, such that applicationspecific quality-of-service constraint is fulfilled. Thus, their algorithms provide opportunity to exercise tradeoff between achieving high energy saving and achieving high task completion ratio (i.e. low deadline miss ratio). Choi et al. [46] propose a DVFS technique which enables achieving a precise energy-performance tradeoff. Their technique makes use of runtime information about the external memory access statistics and chooses the optimal CPU clock frequency and the corresponding minimum voltage level based on the ratio of the onchip computation time to the off-chip access time. Their technique lowers the CPU frequency in the memorybound region of a program to keep the performance degradation to a low value. Gheorghita et al. [47] propose a technique for saving energy in embedded systems by utilizing the knowledge of operation modes. As an example, a portable MP3 player may have different application scenarios e.g. actual use for listening, connection with computer etc. Further, the player provides two operation modes viz. mono or stereo. Since mono mode requires less computation power, battery power can be saved by using smaller voltage at the time of low resource usage. This observation can also be used to guide design-time choices to provide specific operation modes for specific usecase scenarios. They also show the application of their technique for both hard and soft real-time systems. Saewong et al. [58] propose four DVFS techniques for saving energy in embedded systems. Their first technique selects a single frequency which is used for the entire execution, such that the task-deadline is met and energy is minimized. This technique is suitable for systems where the overhead of DVFS is very high. The third technique finds suitable frequency like the first technique with the difference that it takes decision in the order of priority of tasks. Hence, if meeting the deadline of a higher priority task requires running other tasks at larger frequency than what is minimally required for meeting their deadlines, extra slack is created which can be used to save extra energy in low-priority t"
    },
    {
      "heading": "Using Power Modes",
      "text": "In embedded systems, the hardware typically provides a range of operating modes which can be used to save energy. Different modes consume different amount of power and take different time to return back to the normal mode. In general, the modes with lower energy consumption also take the largest time to return to the normal mode and vice versa. For saving energy while keeping the performance loss bounded, these modes should be judiciously used. Also, while a low-power mode can be used when the system is idle, the system must return to the normal mode for actually servicing a request or performing the task. Li et al. [78] propose a method for selecting the power modes for the optimal power management of embedded systems under timing and power constraints. Their method determines the schedule of mode transitions such that the whole system can meet all power and timing constraints. Hoeller et al. [79] propose an interface for power management of hardware and software components. They method allows applications to express when certain components are not being used and based on this information, individual components, subsystems or the whole system can be transitioned to low-power modes. This frees the programmer from the task of individually managing the power consumption of each component. Huang et al. [76] propose an energy saving technique which works by adaptively controlling the power mode of the embedded system according to historical arrivals of tasks. Their technique takes decision regarding when to transition the system to low-power from normal-power mode or vice versa, based on the relative time overhead and energy advantage from mode transition and the consideration of meeting the deadlines of the tasks. Bhatti et al. [26] present an online framework to integrate DVFS with power-mode management (PMM) scheme to save energy in embedded systems. Their scheme utilizes conventional DVFS and PMM schemes and uses machine-learning approach to adapt at runtime to the best-performing policy for any given workload. To save energy, DVFS policy makes use of dynamic slack while PMM makes use of idle time intervals. They have shown that their technique achieves energy savings comparable to the best-performing policy at any time. Also, their framework allows use of existing as well as new DVFS and PMM schemes. Niu et al. [6] propose a technique to save both leakage and dynamic energy in embedded systems by integrating DVFS and PPM. In the case when processor is active, their technique chooses a processor speed such that the dynamic and leakage power consumption are balanced. Further, in the case when the processor is idle, the coming tasks are delayed as much as possible, such that their deadlines are not missed and scattered, short intertask idle intervals are merged in a few large idle intervals. Large idle intervals lead to reduced mode transition overhead, since the processor can stay in either idle or active state continuously for longer time. Kim et al. [55] study the trade-off between voltage scaling and dynamic power-mode management. Under the assumption that voltage scaling does not reduce energy consumption in peripheral devices, voltage scaling increases the execution time and thus the leakage energy consumption of peripheral devices is increased and opportunity to transition them to low-power mode is reduced. Towards this, they propose a technique which exploits task-slack by partitioning the task execution into several intervals and shuts down the unneeded peripheral device on a per-interval basis. Shin et al. [32] propose a technique for saving energy by integrating DVFS and PMM. They note that in realtime embedded systems, idle intervals can arise due to either inherent slacks, need to maintain priorities or early completion of tasks than their worst case estimates. To exploit inherent slack for saving energy, they use an offline DVFS approach. Further, they use an online approach which evaluates both DVFS and PMM to find the best way to exploit the remaining two kinds of idle intervals for saving energy. Cheng et al. [80] propose an online technique for performing energy-aware I/O scheduling for hard realtime systems. Their technique utilizes device slack to perform power mode transitions to save energy, while maintaining temporal correctness. Their technique performs inter-task scheduling and not intra-task scheduling, since it may lead to missed deadlines which may have severe consequences in hard real-time systems. The decision to transition are taken based on break-even time calculation, which shows the minimum time the device needs to be idle for the mode transition to provide positive energy savings. Awan et al. [81] propose an approach for saving energy in embedded systems using multiple low-power modes. Their technique computes the break-even time for each mode using offline analysis. Further, since early completion of high-priority task creates slack, their technique accumulates this task an"
    },
    {
      "heading": "Saving Energy in Specific Components",
      "text": "Several researchers propose microarchitectural techniques for saving energy in specific components of embedded systems. These techniques leverage application properties or variation in workload to dynamically reconfigure the component of the system to save energy. Yang et al. [82] discuss a technique for saving main memory energy in embedded systems. Their technique uses software-based RAM compression to increase the effective size of the memory. The memory compression is used only for those applications which may gain benefit in performance or energy from the compression. For such applications, compression of memory data and swapped-out pages is performed in an online manner, thus dynamically adjusting the size of the compressed RAM area. Trajkovic et al. [83] propose a buffering based technique for saving energy in low-power embedded systems. Their technique is based on the observation that since the DRAMs allow the row to be left 'on' after a memory access; if in a synchronous DRAM (SDRAM), two memory access (i.e. read/write) operation are done in a same activate-precharge cycle, then the overhead of activation and precharging can be avoided. Using this observation, their technique prefetches additional cache blocks on read accesses and combines multiple blocks (which are to be written to the same DRAM row) in write accesses. Their technique uses small storage structures to store the extra prefetched lines and to buffer the writes to the same DRAM row. By adapting the above mentioned write-combining and prefetching schemes for each application, their technique reduces the memory power consumption. Reddy et al. [89] present an approach for saving cache energy in multitasking embedded systems. Their algorithm selects best cache partitioning for different running applications in offline manner and uses this information to allocate cache at runtime. Their algorithm also reduces inter-task interference in a preemptive multitasking environment. Tsai et al. [106] propose a technique for saving energy in embedded processors by using a memory structure called \"Trace Reuse cache\" (TRC). The TRC is used at the same level of memory hierarchy as conventional instruction cache. TRC reuses the retired instructions from the pipeline back-end of a processor and efficiently delivers the instructions in the form of traces. Thus, TRC enables the processor to achieve a higher instruction rate, which leads to improvement in both performance and energy efficiency. Hajimiri et al. [92] integrate cache reconfiguration and code compression to improve both performance and energy efficiency of embedded systems. For a single-level cache hierarchy, their technique performs exhaustive exploration of the cache design space by varying different parameters such as line size, associativity and total size; and simulating each one of the resultant configuration. Based on the results, the configuration with minimum energy can be selected. The code compression scheme integrates synergistically with cache reconfiguration, since code compression improves performance by reducing memory traffic and bandwidth usage and thus it partially offsets the performance loss resulting from cache reconfiguration. It is well-known that there exists a large intra-and inter-program variation in cache requirement of different applications. Using this, the cache can be dynamically reconfigured for each program or program phase and the unused cache is turned off to save energy. Based on this idea, Albonesi [103] proposes selective-ways approach where some of the ways of the cache are turned off to save energy, such that performance degradation remains bounded. Zhang et al. [99] propose a highly-configurable cache architecture for facilitating dynamic reconfiguration to save energy in embedded systems. Their cache architecture contains four separate banks that can operate as four separate ways. By concatenating these ways, the associativity of the cache can be changed to either 1, 2 or 4. Also, if desired, some ways can be shut down. Further, by configuring the fetch unit to fetch different size of cache lines, the line size (block size) of cache can also be altered. Several researchers use this architecture to save cache energy. For single-core systems, Wang et al. [86] profile several possible configurations of L1 data cache, L1 instruction cache and unified L2 cache in offline manner. At runtime, different possible combinations of two-level cache hierarchy are explored to find the most energy efficient configuration. For multi-core systems with private L1 caches (data and instruction) and shared L2 cache, Wang et al. [16] propose using static profiling for exploring different possible combinations and dynamically reconfiguring L1 cache and partitioning L2 cache for saving energy. Similarly, Rawlins et al. [118] discuss their technique for saving cache energy in heterogeneous dual-core systems by tuning the size of L1 cache, while addressing the issues presented "
    },
    {
      "heading": "Using Unconventional Cores",
      "text": "As discussed before, the performance of modern computing systems is primarily shaped by power concerns. In such a regime, \"unconventional\" cores (or platforms) such as GPUs, FPGAs, ASICs and DSPs etc. [119,120] hold a great promise for improving application performance and energy efficiency. For this reason, several researchers have used unconventional cores for power management of embedded systems. Mu et al. [110] compare the energy efficiency of GPUs with that of DSPs for high performance embedded computing (HPEC) benchmark suite which includes a broad range of signal processing applications. They have observed that although GPU provides at least an order of magnitude better performance than the DSP, its energy efficiency (measured in performance per watt) is inferior to that of the DSP. Mencer et al. [112] compare the energy efficiency of FPGAs with that of DSPs for IDEA (International Data Encryption Algorithm). They have observed that the FPGA provides an order of magnitude better energy efficiency than the DSPs. Timm et al. [108] compare the performance and energy efficiency of CPU with that of GPUs for several applications, such as matrix multiplication and FFT etc. That have observed that GPU offers significant performance advantage over CPU and hence, despite consuming larger peak power, it outperforms CPU in energy efficiency. They also note that the advantage of GPU reduces for applications which do not provide substantial parallelism. Ou et al. [121] propose a technique for energy-efficient mapping of embedded signal-processing applications on FPGA. They use dynamic-programming based approach for mapping beamforming applications which are used in air-borne or sea-borne vehicles. They have shown that compared to a greedy algorithm, their technique provides much larger energy savings. Wang et al. [109] compare the performance and energy benefits of utilizing the integrated GPU and DSP cores to offload or share the compute-intensive tasks of CPU. They test three mobile computing systems viz. TI's OMAP3530, Qualcomn's Snapdragon S2, and Nvidia's Tegra 2. All these systems integrate both CPU and GPU. Further, TI's OMAP also integrates DSP and exposes it for programming by the user. They test three applications viz. FFT, matrix multiplication and 2D stencil. They have observed that by effectively using GPU and DSP along with CPU, significant improvement in performance and energy efficiency can be achieved. Stitt et al. [113] propose an approach for moving critical software loops to reconfigurable hardware for saving energy. Typical benchmark programs spend a large fraction (e.g. 80%) of their time in a small portion of code. Thus, by Amdahl's law, to improve overall performance, this portion can be implemented on a application-specific or reconfigurable hardware which provides much better performance than the software. They demonstrate their approach by using ASIC and FPGA and observe large energy savings over a softwareonly implementation. Llamocca et al. [115] compare the performance and energy efficiency of GPU and FPGA for 2D FIR (finiteimpulse response) filter. This program finds application in video processing. They have observed that although FPGA provides lower performance than the GPU, it outperforms GPU on the metric of energy efficiency. The high performance of GPU is due to higher frequency and its ability to exploit massive parallelization present in the algorithm. Fowers et al. [116] evaluate sliding window program on multi-core CPU, FPGA and GPU. This program has applications in digital signal processing. They have observed that FPGA provides an order of magnitude better performance while using an order of magnitude less energy compared to both CPU and GPU. These results demonstrate the utility of FPGAs for implementation of embedded system performing highdefinition video processing."
    },
    {
      "heading": "Concluding Remarks",
      "text": "The next generation mobile computing systems will possess capabilities for high-speed video processing and communication which will require at least an order of magnitude better energy efficiency than what is available in state-of-the-art systems. This clearly highlights the need of power management in embedded systems. To cope with these challenges, power management is necessary at all levels, viz. chip-design level, microarchitectural level, application level and system level. In this paper, we reviewed several power management techniques for embedded systems and classified them based on their key research idea. It is hoped that by providing insights into the working of power management techniques, this paper would help the researchers in addressing the challenges of power consumption and architecting highly-energy efficient embedded systems of tomorrow."
    }
  ],
  "figures": [],
  "equations": []
}