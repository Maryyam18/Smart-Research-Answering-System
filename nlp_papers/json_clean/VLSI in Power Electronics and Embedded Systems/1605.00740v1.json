{
  "paperid": "1605.00740v1",
  "title": "VLSI Extreme Learning Machine: A Design Space Exploration",
  "authors": [
    "Yao",
    "Basu",
    "Kinget",
    "Razavi",
    "Neftci",
    "Indiveri",
    "Indiveri",
    "Chicca",
    "Douglas",
    "Arthur",
    "Boahen",
    "Basu",
    "Hasler",
    "Linares-Barranco",
    "Serrano-Gotarredona",
    "Serrano-Gotarredona",
    "Brink",
    "Nease",
    "Hasler",
    "Shuo",
    "Basu",
    "Pfeil",
    "Scherzer",
    "Schemmel",
    "Meier",
    "Cameron",
    "Murray",
    "Huang",
    "Zhou",
    "Ding",
    "Zhang",
    "Eliasmith",
    "Tapson",
    "Van Schaik",
    "Tapson",
    "Basu",
    "Shuo",
    "Zhou",
    "Lim",
    "Huang",
    "Enyi",
    "Hussain",
    "Basu",
    "Huang",
    "Chen",
    "Yao",
    "Basu",
    "Huang",
    "Wang",
    "Lan",
    "Huang",
    "Zhu",
    "Siew",
    "Hoerl",
    "Kennard",
    "Delbruck",
    "Van Schaik",
    "Indiveri",
    "Chakrabartty",
    "Cauwenberghs",
    "Sarpeshkar",
    "Delbruck",
    "Mead",
    "Ho",
    "Verma",
    "Thakur",
    "Hamilton",
    "Wang",
    "Tapson",
    "Schaik",
    "Chen",
    "Enyi",
    "Basu",
    "Marr",
    "Degnan",
    "Hasler",
    "Anderson",
    "He",
    "Chang",
    "Guia De Solaz",
    "Conway",
    "Bingham",
    "Mannila",
    "Boutsidis",
    "Zouzias",
    "Drineas",
    "Rahimi",
    "Recht"
  ],
  "year": 2016,
  "abstract": "In this paper, we describe a compact low-power, high performance hardware implementation of the extreme learning machine (ELM) for machine learning applications. Mismatch in current mirrors are used to perform the vector-matrix multiplication that forms the first stage of this classifier and is the most computationally intensive. Both regression and classification (on UCI data sets) are demonstrated and a design space tradeoff between speed, power and accuracy is explored. Our results indicate that for a wide set of problems, σVT in the range of 15-25mV gives optimal results. An input weight matrix rotation method to extend the input dimension and hidden layer size beyond the physical limits imposed by the chip is also described. This allows us to overcome a major limit imposed on most hardware machine learners. The chip is implemented in a 0.35µm CMOS process and occupies a die area of around 5 mm × 5 mm. Operating from a 1 V power supply, it achieves an energy efficiency of 0.47 pJ/MAC at a classification rate of 31.6 kHz.",
  "sections": [
    {
      "heading": "I. INTRODUCTION",
      "text": "In general, it is difficult to achieve high accuracy in pure analog signal processing modules due to several reasons, a major one being device mismatch [1]. The effect of mismatch on traditional circuits like differential amplifiers and current mirrors is well documented [2]. It has also been shown that for MOS based circuits, the extra power dissipation needed to overcome effects of mismatch can be an order of magnitude higher than the limit imposed by thermal noise [1]. With transistor dimensions reducing over the years, variance in properties of transistors, notably the threshold voltage, has kept on increasing making it difficult to rely on conventional simulations ignoring statistical variations. The problem is particularly exacerbated for neuromorphic designs [3], where transistors are typically biased in the sub-threshold region [4]- [6] of operation (to glean maximal efficiencies in energy per operation) since device currents are exponentially related to threshold voltages thus amplifying its variations as well. For example, it is shown in [7] that an array of 5 -bit DACs in 0.35µm CMOS process used as tunable weights only provide an effective number of bits of 1.1 due to mismatch. In general, there has been an approach to compensate for mismatch either through floating-gates [8] or by storing calibration coefficients off-chip in the form of connection probabilities [3]. Digital calibration can be used to compensate for these effects on-chip [7] as well. However, they lead to huge area overheads due to the requirement of extra transistors for calibration and storage The authors are with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore. (email: eyao1@e.ntu.edu.sg, arindam.basu@ntu.edu.sg) of digital bits [9]. Sometimes, it is claimed that learning can compensate for mismatch and has been demonstrated in specific cases [10], [11]-but the claim needs to be further quantified using standard datasets since mismatch will exist in the learning circuits as well. The ELM algorithm is popular in the machine learning community due to its fast training speed and has been shown to produce similar or better performance compared to support vector machines (SVM) [12]. A closely related method (termed Neural Engineering Framework) has also been used to generate large scale models of cognitive systems [13]. ELM based methods have been used classify spike time based patterns recently [14] and online learning algorithms for ELM have been proposed [15]. Clearly there is a need to develop hardware implementations of the same. In this paper we present a circuit that 'utilizes' mismatch to do effective computation in the first layer of a two layer spiking neural network implementation of ELM. This approach can be used in other algorithms like liquid state machine (LSM) or echo state networks (ESN) (sometimes referred to as reservoir computing), since they require random projections of the input as well. We have earlier proposed the idea of using spiking neurons for implementing ELM [16] and described the advantages of such an architecture over standard digital implementations [17]. It should be noted that this method only exploits spiking neurons for ease of hardware implementation and does not use any spike based learning rules to perform the learning of the second stage. The major hardware benefits are the use of low-power analog circuits for the reservoir and simple digital circuits for the second stage. We demonstrated the first VLSI implementation of this principle in [18] where it was used for decoding motor intentions for implantable brain-machine interfaces. In this paper, we present a different chip utilizing the same core circuit as [18] but operating on 10 bit digital inputs instead of spikes. Instead of a specific application, this paper presents an entire design space tradeoff between speed, power and accuracy. Finally, we present a method and associated circuits to virtually expand the input and output dimensions of the chip beyond the physically implemented 128 channels. We show results of applying inputs from standard machine learning data bases such as [19]. In the next section, we present details of the ELM algorithm and training methods. Section III describes the VLSI architecture of the chip and details of the sub-circuits. The trade-offs between noise, speed and energy dissipation of this architecture are presented in Section IV. An important limitation of hardware machine learners is limited input and output dimensions. In Section V, we present a method to virtually expand the dimensions beyond the physical number of channels on the chip. Measurement results are presented in Section VI and finally we conclude in the last section."
    },
    {
      "heading": "II. ELM THEORY",
      "text": "In this section, we will present a brief description of the ELM algorithm and refer the readers to [12], [20] for details. As illustrated in Fig. 1, the ELM algorithm is applicable to a two layer neural feed-forward network with L hidden neurons having an activation function g : R → R. Without loss of generality, we consider a scalar output in this case though the method can be easily extended to multiple outputs by considering each output one by one [21]. The output of the network o is given by: where β denote the output weights, z i and H i are the input and output of the i-th hidden layer neuron. w i denotes the input weight and b i is the bias for the i-th neuron. In general, a sigmoidal form of g() is assumed though other functions have also been used. Compared to traditional back propagation learning rule that modifies all the weights, the ELM allows w i and b i to be random numbers drawn from any continuous distribution while only the output weights, β i needs to be tuned based on the training data T . For N samples (x k , t k ), the hidden layer output matrix H is defined as: The desired output weights, β are then the solution of the following optimization problem: where The ELM algorithm proves that the optimal solution β is given by β = H † T where H † denotes the Moore Penrose generalized inverse of a matrix [12]. The huge benefit of this method is that it removes the need for iterative tuning and gives a simple formula to calculate the weights. The orthogonal projection method can be efficiently used to find H † as (H T H) -1 H T if H T H is non-singular or as H T (HH T ) -1 if HH T is nonsingular. Further, using concepts from ridge regression theory [22], a small constant I/C is often added to the diagonal of H T H or HH T of the Moore-Penrose generalized inverse H-the resultant resolution is stabler and tends to have better generalization performance. The value of C is typically optimized as a hyperparameter using cross-validation techniques. Iref Vcp VDD D0 D0 D8 D8 D9 D9 IDAC 10b MOS Ladder Current DAC Iref IDAC Vbias S1 S2 S1 D9 D8 D7 D6 D3 D2 D1 D0 D5 D4 S2 S1 S1 Iout C Fig. 3: Schematic of input generation circuit (IGC) for one channel. A reference current is split according to the 10 bits of input data to create IDAC. The capacitor C ensures sufficient SNR when the current is mirrored to the L columns. An active current mirror is enabled to allow fast settling when IDAC is small."
    },
    {
      "heading": "III. SYSTEM ARCHITECTURE",
      "text": "The architecture of the proposed mixed signal classifier that exploits analog computing for the d×L random weights of the input layer is shown in Fig. 2(a). The corresponding timing diagram is shown in Fig. 2 (b). The input data (Data in) will be fed to the particular channel in the system serially through a 1 to 128 demultiplexor according to the corresponding address A < 6 : 0 > through a serial peripheral interface (SPI). The number of bits (NOB) of Data in for each channel is b in = 10. Input data will be stored in shift registers first for the configuration of the current-mode digital-to-analog convertor (DAC) in the input-generation-circuit (IGC). The function of IGC is to generate an analog DC current according to the input data which will be copied to every column using a current mirror. Multiplied by the random weights generated in the current mirror array, the current in one column will be summed according to Kirchoff's current law (KCL) and flow into a hidden layer neuron. This current is denoted as I z i for the ith neuron in Fig. 2(a) and is analogous to the variable z i in Fig. 1. Spiking oscillations with different frequency will be generated by the neuron according to their own input currents which is counted by an asynchronous counter forming a row of the matrix H. Through a column scanner, these hidden layer outputs can be transferred to the FPGA to first get the output weight β during training and later for the second stage computation of ELM during regular operation. Other timing and control signals will also be provided by the FPGA as shown in Fig. 2(b). Next, we describe the operation of each block."
    },
    {
      "heading": "A. Input Generation Circuit (IGC)",
      "text": "Figure 3 shows the schematic of the input generation circuit for each dimension of input. The reference block provides a fixed master biasing current I ref that acts as the reference current of the current DAC as well as the biasing for the active current mirror. The input data Data in is applied to configure a b in = 10 bits MOS based current splitting DAC to generate a corresponding analog current [23]. The output current of this DAC is given by: I DAC is multiplied with the input weights by current mirroring operation as described later. A capacitor C = 0.4pF is also added at the gate of the current mirror array for each row to improve noise performance and achieve the desired resolution of 8 bits in the multiplication-this will be discussed in the later section. In the conventional current mirror, bandwidth is in proportion to the input current. If Data in is too small, input currents are also small and hence the settling time of the current mirror (defined as time taken to settle to within 5% of the final value) might be too large. To solve this problem, an active current mirror is added to complement the conventional mirror. Switch S1 is closed to turn on the active current mirror if all of the 4 MSBs are zero. This ensures that the capacitor C is charged by the large bias current and not the small input currents. When all the bits of Data in are 0, switch S2 is closed to pull V bias to ground and shut off the current mirrors in that row. The logical signals to control S1 and S2 are given by: where D i are the bits of Data in.   The curves saturate at higher maximum frequencies for higher VDD. Note the logarithmic scales for both plots."
    },
    {
      "heading": "B. Neuron",
      "text": "Figure 4(a) details the circuit of the hidden layer neuron block. It is a current-controlled oscillator structure followed by an asynchronous counter. This is one of the simplest neuron circuits described in [24]. This circuit has the issue of large short-circuit current dissipation in the inverters. However, in our case we can avoid this problem by operating at very low power supply voltages (≈ V T N +V T P ) making the short-circuit current negligible. The neuron is enabled when the control signal N EU EN is high. The oscillation waveform at the nodes V mem and V out are illustrated in Fig. 4(b). V mem is charged down by the input current I z -I lk till it reaches the threshold voltage of the inverters. At that point both the inverters trip making the output switch to ground. Since the voltage change at the node of V out is VDD, the voltage change of V mem due to the feedback capacitor is given by: Also, the reset transistor turns ON charging V mem up by the current I rst + I lk -I z . The inverters trip again once V mem reaches the threshold and this process continues as long as N EU EN is high. Both the capacitors C a and C b can be digitally reconfigured as shown in Fig. 4(a). The values of the capacitors are: We can derive an equation for the oscillation period T sp . It is composed of two parts: the time T 1 for the input current I z to discharge the capacitor of node V mem and the time T 2 to reset the capacitor. Hence, T sp is given by: (7) Assuming I lk ≈ 0, the relationship between the neuron spiking frequency and the input current I z can be easily obtained as: This quadratic relationship of equation ( 8) between current and frequency is plotted in Fig. 5(a). As we can see from Fig. 5(a), if I z << I rst /2, we have almost a linear relation given by: where denotes a conversion gain from current to frequency. When I z = I rst /2, f sp will reach its maximum value f max . After this point, the spiking frequency will keep falling down till it reaches zero for I z = I rst . Since the inflection point of the curve is reached at I z = I rst /2, we refer to this current value as I f lx . The chip has digital control bits making the capacitors configurable. As shown in Fig. 4(a), an asynchronous counter counts the total number of spikes from the neuron during a fixed period of time T neu (time duration for which N EU EN is high) and generates the output H. A hard nonlinearity in the form of saturation can be implemented by stopping the counter whenever its count reaches a predefined limit 2 b . b in this case is the valid MSB of the counter output which is also configurable from 6 to 14. If only the linear region of the neuron spiking waveform is adopted (this is also the most energy efficient part as shown later), the final transfer function of the hidden layer neuron can be represented by: ) This saturating nonlinearity is shown in Fig. 5(b). This nonlinearity was preferred due to its ease of implementation and digital control. From Fig. 5(b) we can also note the current at which the H saturates is denoted by I z sat . This value depends on both T neu and b. Also, [0 I z max ] is used to denote the range of input currents to the neuron. Figure 6(a) plots SPICE simulation of the neuron spiking frequency with the variation of input current I z on a logarithmic scale and compares it with theoretical predictions based on equation 8. For this simulation, C a and C b were set to be 300fF and 50fF respectively while VDD was kept at 1V. As expected, the spike frequency increases linearly for small values of I z , reaches a maxima eventually and then starts reducing for further increase in I z . Results from a similar simulation but for three different values of VDD (0.8, 1 and 1.2 V) are shown in Fig. 6(b). Since f sp is inversely proportional to VDD, f sp is higher for small I z with a smaller VDD. However, when VDD is lower, I rst is smaller and hence f sp attains the peak value at smaller value of I z , i.e. I f lx reduces when VDD is reduced. On the other hand, for higher VDD, f sp saturates at a larger value f max and it is attained for larger value of I f lx ."
    },
    {
      "heading": "C. Current Mirror Array",
      "text": "The digital input Data in is mapped to a vector of input current I in which are copied to every neuron using a current mirror. These inputs can also be obtained from a sensor such as a photo diode. The capacitor C = 0.4pF is kept to maintain a minimum SNR [25] at the expense of bandwidth. For low-power operation, we operate the current mirrors in sub-threshold regime. Minimum sized transistors are employed in these current mirrors to exploit VLSI mismatch which is necessary for the generation of random input weights w i and bias b i of ELM. For example, the contribution of input i in,i to the total input current of neuron j is given by i in,i w 0 e ∆VT,ij/UT where U T is the thermal voltage, w 0 is the nominal current mirror gain while ∆V T,ij denotes the mismatch of the threshold voltage for the transistor copying the i-th input current to the j-th neuron. This last term is a random variable with a Gaussian distribution and hence the weights w in equation ( 1) above get mapped to random variables with a log-normal distribution in our implementation. Since in our implementation w 0 = 1, we can write: Do note that the ELM algorithm only requires random numbers from any continuous distribution [21]. Here ,we choose log-normal distribution due to the intrinsic physics of subthreshold mosfets. If biased in above-threshold regime, the distribution of random numbers would be closer to gaussian."
    },
    {
      "heading": "D. Parameter Choice",
      "text": "To determine the performance of the network, we chose two representative tasks of regression (d = 1) and classification (d = 14). For the regression task, the network was given a set of noisy samples and had to approximate the underlying function. For classification, six different data sets with widely varying dimensions and training set sizes were chosen from the UCI machine learning repository [19]. Here, we show results for only the 'brightdata' case as a representative but the conclusions drawn are valid across the other data sets. It is a two class problem that includes 1000 training data and 1462 testing data. The reasons for choosing these tasks were that the performance of the software implementation for these tasks are reported in publications as a typical benchmark [12]. For the following simulations done in MATLAB, we considered the mismatch in current mirror weights as the dominant factor. It was assumed to be log-normally distributed with a standard deviation of V T , σ VT ranging from 5 to 45 mV (as a reference, σ VT in our fabricated chip is ≈ 16 mV). Equation ( 11) was used to simulate the neuronal characteristic and the other parameters were kept at fixed nominal values of K neu = 26KHz/nA and T neu = 56µsec. In real applications, variations exist for other parameters in the neuron transfer function as well. However, simulation results show that mismatch in these do not affect the qualitative nature of the results we present here. 1) Input Mapping: For efficient use of the hardware, we need to determine how to map the compact set X = [-1 1] to input currents. First, it can be only mapped to a set in R + since we have unidirectional current mirrors. Assume the maximum input current for one dimension is I max , i.e., the set is [0 I max ]. Therefore the maximum current going to the neuron I z max = d × I max . From Fig. 5(b), we need to find out the relationship between I z max and I z sat . Though theoretically any positive set will work, it might need an unreasonably large number of neurons to get a satisfactory performance. To illustrate this point intuitively, consider a case where I z max << I z sat . Then the transfer function of the neuron is a linear function without any high order components. Also, if I z max >> I z sat , the outputs of most neurons will be saturated to 2 b , and will not encode the variations of the input. Both of these cases will require a large number of hidden layer neurons so that 'by chance' a large enough pool of neurons are obtained which encode the changes in input. Hence, there should be a range for the ratio between I z max and I z sat , such that we can achieve a good performance with a small number of hidden layer neurons. To find this desired range, we first fix a value of I z sat /I z max and evaluate the performance of the network on both tasks with different number L of hidden layer neurons. The regression error reduces initially with larger L but saturates after the L increases beyond a critical value L min . To quantify the dependence of performance on the ratio of I z sat /I z max , we now plot in Fig. 7(a) the dependence of L min on the ratio of I z sat /I z max , with lower values of L min being preferable.  We have chosen error of 0.08 as the saturation level in this case. From this figure, the ratio of I z sat /I z max ≈ 0.75 is the best trade off point between number of hidden neurons and input dynamic range for all values of σ VT . For small values of σ VT , the performance degrades rapidly on both sides of the optimal value. However, as σ VT increases, the performance degradation is much less implying the choice of I z max is less critical in highly scaled VLSI. However, it can also be noted that the performance is best (least L min ) for σ VT in the range of 15-25mV. This has been found to be true for a wide range of classification problems as well. Hence, for deeply scaled CMOS processes with larger σ VT , minimum sized transistors cannot be used. In those cases, the transistor size has to be increased (following Pelgrom's model [1]) to reduce σ VT within the desired range. However, the required area will still reduce compared to an older process with larger transistors since the coefficient A VT is reducing as transistor scaling continues [1]. 2) Resolution of Output Weight: As mentioned earlier, the digital circuits will use pre-calculated output weights, β from a memory and accumulate it based on neuronal spiking patterns. In order to implement this, we need to know how many bits are needed to represent β. Less number of bits will degrade performance of the classifier while more will waste hardware resources and power. We use the classification example here with L = 128. Figure 7(b) shows the change of error with increasing number of bits indicating 10 bits resolution is enough for good accuracy. 3) Counter resolution: Besides the resolution of β, we also analyzed the dependence of performance on the output counter resolution b in equation (11). Since we estimate the spiking frequency by using a counter to count the number of spikes in a fixed time window T neu , a small value of b will introduce large quantization errors in the estimate of frequency. This implies that the neurons have to produce more spikes in the counting window, which would on the other hand induce more power dissipation. To find a good trade-off for b, we fixed I z sat /I z max ≈ 0.75, L = 128 and resolution of β to 10 bits. Figure 7(c) shows the simulation result for the classification error with b increasing from 1 to 10. b ≈ 6 is found to be sufficient for classification."
    },
    {
      "heading": "IV. NOISE, SPEED AND ENERGY DISSIPATION A. Noise",
      "text": "Noise is an important specification to be considered in circuit design. In this section, we present the operational limits set on this architecture due to noise based constraints. Since the transistors are operating in sub-threshold region, the contribution of 1/f noise is negligible compared to the thermal noise [25]. For the current mirror circuit as shown in Fig. 8, we can easily get the input referred thermal noise spectral density as: where g m1 and g m2 are transconductance of input and output transistors respectively, i 2 n1 and i 2 n2 are corresponding transistor channel noise. Since the transistors are working in the sub-threshold region, the transconductance is in proportion to its drain current. Applying the noise model of drain current of sub-threshold transistors to be i 2 = 2qI∆f [26] where q denotes the electronics charge, we can rewrite the above equation as: For this single pole system, the noise equivalent bandwidth ∆f = κI1 4CUT where κ denotes the inverse of the sub-threshold slope [26]. Assuming I 2 /I 1 = w 0 , and substituting the bandwidth equation above, we get: Finally, the signal to noise ratio (SNR) can be expressed in the following equation: Thus, from the equation ( 16), we can see the SNR can be controlled by changing C. This reflects a direct trade-off with bandwidth which is inversely proportional to C. If an 8 bits SNR is needed in the system, and w 0 = 1, it is sufficient to add C = 0.4pF capacitance in the current mirror for each input channel. Note that only one such capacitor is needed for every row."
    },
    {
      "heading": "B. Speed",
      "text": "The conversion time for one classification operation T c comprises two parts: T cm and T neu where T neu is the neuron operation time and T cm is the current mirror settling time. If one of them is much larger than the other, we can approximate T c ≈ max (T cm , T neu ). We consider T cm to be 4 times of the inverse of the bandwidth (BW), i.e. T cm = 4 BW = 4CUT κIin where κ = 0.7, U T = 0.025V at room temperature and C = 0.4pF as derived earlier. If the average input current is I max /2, the average current mirror settling time is As discussed earlier in Section III-A, an active current mirror is utilized to boost the bandwidth for small current values. SPICE simulation result for this effect shown in Fig. 9(a) demonstrates a bandwidth increase by around 5.84X. We can with its corresponding Tneu from equation (19). find the range of T cm by considering maximum and minimum input currents: where b in = 10 is the number of bits of Data in and the factor of 5.84 is due to the active current mirror. Figure 9(b) shows the decrease of T cm with increasing I max for the conventional and active current mirror cases. To find the value of T neu , we can see from Fig. 5(b) that we want H = 2 b for I z = I z sat . Combining this observation with equation (11), we can derive the following:  Increasing I max reduces the time required for both the neuron and current mirror. T cm for the conventional current mirror is always the dominant factor. However, with the active current mirror on T neu may be larger than T cm for large values of b. These plots are done for d = 10; increasing d will have an effect of reducing T neu since I Z max = d × I max increases. Hence, to show the trade-offs between T cm and T neu as a function of b and d, we plot contours in the space of counter dynamic range 2 b and input dimension d where T cm = T neu . To do this, we equate (17) and (19) to get: where I z sat /I z max = 0.75 is used. The straight line contours defined by equation 20 are plotted in Fig. 9(c) for three different K neu values corresponding to VDD= 0.8, 1 and 1.2V. For parameter choices on these contour lines, T c = T cm + T neu = 2T cm = 2T neu . If the relation between 2 b and d sets the operation regime above any of the contour lines, T neu > T cm while the opposite condition is true if operation regime is below the contour lines. It can be seen that for b ≈ 8 -10 bits and a nominal value of VDD=1V, T neu dominates T cm for the maximum dimension of 128 supported by our chip."
    },
    {
      "heading": "C. Energy",
      "text": "The total power dissipated by the system (P t ) can be split into two parts: power from analog (P avdd ) and digital (P vdd ) supplies. The first term (P avdd ) is mainly dissipated by the voltage reference circuitry, biasing block and the IGCs. Ideally, this should be a function of input dimension. However, in the current design only unused active mirrors are turned off while the current DAC is always ON-this will be rectified in future designs. The second term (P vdd ) comprises the power dissipated by the neuron, asynchronous counter and other digital blocks including decoder and scanner. Of these terms, the power dissipated by the neuron includes the synaptic currents as the input and the counter at output and varies with different parameters such as biasing current. It is the major energy consumer in the chip when the number of hidden neurons, L is large. Hence, it is important to understand its dependence on different parameters. Thus, we can write P vdd as: where E sp is the energy dissipation per spike for the neuron. E sp can be modelled as: where I sc is the short-circuit current in the inverter that depends on the value of VDD and is negligible for small values of VDD. Here, the first term denotes the switching power dissipated in the neuron circuit, second term denotes short circuit power loss in the inverters and the third term denotes the short-circuit power dissipated on the node V mem in Fig. 4(a). If I z << I rst and I lk ≈ 0, equations ( 21) and ( 22) can be combined to give: From simulation, when VDD is 1V, α 1 ≈ 0.2pF and α 2 I sc ≈ 0.03µA. Using equation ( 22), we will now proceed to estimate average energy per conversion operation (E c ) for one neuron where an input current I z ∈ [0 I z max ] is converted to a digital count. Assuming that I z is distributed uniformly in the range of 0 to I z max , i.e. P (I z ) = 1 , E c can be estimated as: where H(I z ) is the number of spikes generated in T neu as defined in equation (11). Note that here we write E sp (I z ) and H(I z ) to make the dependence of equations ( 22) and ( 11) on I z explicit. Using the expression for T neu in equation (19), equation ( 24) can be simplified further to get: From equation (25), we can see that E c depends on I z max . The choice of I z max is guided by the design constraints. Typically, we have to either meet a minimum specified speed of operation or minimize energy of operation without any constraint on speed. To better explain the trade-offs, we can plot E c while varying I z max with b = 10 as illustrated in Fig. 10(a) for three values of VDD. The same figure is re-plotted in Fig. 10(b) but with the corresponding value of T neu instead of I z . Firstly, note that the plots for smaller VDD span a smaller range of current since I rst is correspondingly smaller (similar to Fig. 6). For each VDD, the lowest conversion energy is attained when I z max is close to I f lx = I rst /2. Intuitively, this happens because f sp is higher which leads to lower T neu and correspondingly lower energy. Thus it is beneficial to operate for a short time at a higher spiking frequency than over a longer time with a small frequency. The optimum current I z is less than I f lx since at I z = I f lx , the short-circuit power dissipation (third term in equation ( 22)) increases significantly. From Fig. 10, we can see that lowest energy per conversion is attainable for lowest VDD as expected since the short circuit current reduces drastically at lower VDD. However from Fig. 10(b), we can see that the trade-off for keeping a low VDD is large conversion time. Hence, if conversion time is a critical specification, we have to choose the minimum VDD that meets this specification. As can be seen from Fig. 10(b), higher VDD allows for lower T neu . Reg Reg Reg Rotation_Control CNT 1 CNT 2 CNT L H 1 Reg H 2 Reg H L Reg CLK_r CLK_a 0 1 0 1 0 1 N 1 N 2 N L (a) RN NEU_EN CLK_r Rotation _Control CLK_a ceil(d/k)-1 cycle clock (b) Fig. 13: (a) Schematic of circuit for input dimension extension by shifting and summing the output counter values and (b) its timing diagram."
    },
    {
      "heading": "V. INPUT DIMENSION AND HIDDEN LAYER EXTENSION TECHNIQUE",
      "text": "For some applications, dimension of the input data is quite large (over several thousands) while other applications may require a large number of hidden layer neurons (also over several thousands) to achieve the best performance. This poses a big challenge to neuromorphic analog hardware implementa- To expand the number of hidden layer neurons, we propose to do it in ⌈L/N ⌉ steps where the number of projections is increased N in every step. For the second set of N neurons, we need to shift the random matrix W comprising Here, the subscript (1, 0) is used to denote a single circular rotation of the rows of the matrix W. This notation implies W = W 0,0 = W k,0 . Using this notation, we can continue to get more random projections of the input (and thus expand the number of hidden neurons) by generating W 1,0 to W ⌈L/N⌉-1,0 . Figure 12(a) shows a simple circuit that can be added to the input side of the chip to achieve this function. The corresponding timing diagram of control signals are shown in Fig. 12(b). Once the input data is loaded and the first set of hidden layer outputs are obtained (during the N EU EN signal), the Rotation Control signal is turned high to configure the input registers as a circular shift-register. This is followed by another N EU EN signal to obtain the second set of N random projections and this process continues till L random projections are obtained. A similar method can be applied to expand the input dimension from k to d. In this case, we take the first k dimensions x 1 , x 2 ..x k of a particular input sample x ∈ ℜ d and send it to the chip to get the multiplication for the first k dimensions with the random matrix W. This generates L hidden neuron outputs which can be expanded to a larger number using the technique described in the last paragraph. For the next k dimensions of x, we shift the random matrix . This implies a circular shift along the columns of W. The hidden layer outputs obtained in this step are added to the ones obtained in the earlier step. This method can be continued for ⌈d/k⌉ -1 steps while accumulating the resulting hidden layer outs every time to get the final output for the d dimensional input x. Figure 13(a) shows a simple circuit that can be added to the previously described chip architecture at the output to implement the input dimension expansion technique. Figure 13(b) depicts the corresponding timing diagram. The circuit in Fig. 13(a) shows a register bank after the neuron output counters that can accept inputs from these counters or from other registers in this layer to effect the circular rotation of columns of W. There is a second register bank after this which accumulates the counter outputs over multiple cycles. After the conversion of first k dimensions of x during the first N EU EN signal, a clock pulse on CLK r and CLK a are used to shift this output to the accumulator. From the next cycle, the Rotation Control signal is enabled and pulses on CLK r are used to rotate the columns of the hidden layer. Another pulse on CLK a is used to accumulate this value in the second register bank. TABLE I: Chip Summary Technology 0.35 µm CMOS Die Size 5 mm × 5 mm Input Channels 128 Hidden Layer Size 128 Output Data format 14-bit Digital Input Data format 10-bit Digital Power supply voltage 1 V"
    },
    {
      "heading": "A. Characterization",
      "text": "To validate the function of the proposed design, we have implemented the system in a 0.35µm CMOS process. The ELM chip occupies a die area of 5mm × 5mm as shown in Fig. 14. The current area of the chip is dominated by the current mirror array since the layout is not optimized. Each cell in the current mirror array is pitch matched to the neuron in one direction and the IGC along another making it mostly empty. The area of the current mirror array can be reduced tremendously by following the proposal in [29] limiting the size to the pitch of the IGC. In the next version, we will reduce the pitch of the IGC by moving to a scaled process like 65nm. The mixed-signal chip implements the computationally intensive first stage while the second stage is currently implemented off-chip on a FPGA. In future, the second stage will also be integrated on the same die. Again, moving to a scaled process like 65nm enables a small layout for this digital part. The larger statistical variation in a scaled process does not hurt the performance of the analog part as shown in Fig. 7. The extra gate leakage in the current mirrors can be handled by either using thick oxide I/O devices or using active mirrors. Next, we present some characterization results to show the functionality of the chip. In all the experiments, both analog and digital power supplies are shorted together and is denoted by VDD. Unless stated otherwise, the default value of VDD= 1V is used in most experiments. First, we can get the transfer function of the 128 neurons by sweeping the digital input Data in on any one channel from 0 to 1023. The resultant curves are shown in Fig. 15(a). It can be seen that there is significant variation between the transfer curves of the neurons. Next, to characterize the random variation of the input weight matrix, we send a fixed value of Data in to each of the input channels one by one and measure the counter outputs H. For every input channel, we get L = 128 counter values indicative of the mismatch in that row. In total, there are 128 × 128 such values of H for all the input channels. These results are shown as a 3dimensional plot in Fig. 15(b) where H is plotted on the Z-axis. These same values are normalized by the median count value to get the effective weight distribution. This distribution of 128 × 128 values is plotted as a histogram in Fig. 15(c) displaying a log-normal distribution. This is to be expected since ∆V T n has a normal distribution as explained in Section III-C. Further, by fitting a gaussian distribution to the logarithm of the weight values, we obtain σ∆V T n ≈ 16mV in this process. Note that the mismatch obtained here also takes into account mismatch in the neuronal tuning curves since the count values are obtained at the output of the neuron. Further, this characterization is consistent across a set of 9 chips with minimum and maximum values of σ∆V T n being 15.36mV and 16.26mV respectively."
    },
    {
      "heading": "B. Speed and Power",
      "text": "During measurement, we found the chip to be functional for VDD down to 0.7 V. Thus we can apply the results of the design space exploration in Section IV to optimize the system for the best speed and power efficiency. During measurement, a pico-ammeter (Keithley 6485) is utilized to measure the average current from the power supply to estimate the power dissipation. For all the experiments, speed and TABLE II: Measured performance on Binary Classification Datasets from UCI repository Datasets # Features (d) # Training # Testing Miss Classification Rate (%) Software (L = 1000) [12] This work (L = 128) Diabetes 8 512 256 22.05 22.91 Australian Credit 14 460 230 13.82 12.11 Brightdata 14 1000 1462 0.69 1.26 Adult 123 4781 27780 15.41 15.57 TABLE III: Comparison Table JSSC 2013 [27] JSSC 2007 [25] IJCNN 2015 [28] ISCAS 2015 [18] This work Technology 0.13 µm 0.5 µm 65 nm 0.35 µm 0.35 µm Algorithm SVM SVM ELM ELM ELM Task Classification Classification Regression Regression Regression Classification Classification Design Style Digital Analog Mixed mode Mixed mode Mixed mode Floating gate Supply Voltage 0.85 V 4 V 1.2 V 0.6 V (Digital) 1 V 1.2 V (Analog) Power Dissipation 136.5 µW 0.84 µW -0.4 µW 188.8 µW 1 Max Input Dimension 400 14 1 128 16384 2 Energy Efficiency 631 pJ/MAC 3 0.8 pJ/MAC -3.4 pJ/MAC 4 0.47/ 0.54 pJ/MAC 5 Resolution 16 b 4.5 b 13 b 14 b 14 b Classification Rate 0.5-2 Hz 40 Hz -50 Hz 31.6 kHz Throughput 2 MMAC/s 1300 MMAC/s -0.12 MMAC/s 404.5 MMAC/s 1 This power dissipation is measured based on d = 128 and L = 100. 2 Using input dimension extension technique to expand to d = 128 × 128. Note that the circuits for rotating inputs and outputs for dimension increase are not included on this test chip. 3 Assuming 1000 support vectors. 4 Only considering first stage of ELM for d = 40 and L = 60. 5 0.47 pJ/MAC is energy efficiency of current chip implementing first stage of ELM. The total energy per operation for binary classification is 0.54 pJ/MAC using V DD = 1.5 V for digital multipliers of second stage (see section VI-B for details). power are measured for Data in = 1000 and d = 128 with L = 100 neurons activated. Conversion times T neu are estimated for 2 b = 128. At VDD= 0.7V, the power dissipation is 17.85µW at a maximum conversion speed of 4.5kHz. As can be expected from Fig. 10, there is not much variation in energy per classification when I z max is reduced. However, this difference is more obvious at a higher VDD of 1V. In this case, the fastest classification rate for this system is 146.25 kHz corresponding to T neu = 68.5µs when I z ≈ I f lx . However, the power dissipation at this speed is quite high-2.2mW. Hence for a better energy efficiency, we optimize the classification rate to be around 31.6 kHz by reducing I z max to reduce the short-circuit power dissipation on V mem (as described in Section IV-C). The measured power dissipation now becomes 188.8µW as shown in Table III. We choose this operating point as a good trade-off between speed and power efficiency. From this, we can approximate the coefficients α 1 ≈ 0.3pF and α 2 I sc ≈ 0.076µA that are close to simulation values reported in section IV-C. Also, the analog power P avdd ≈ 3.4µW . Considering the 128 × 100 multiplication-and-accumulation (MAC) operation for the first layer, we can calculate the energy efficiency for this case as 0.47 pJ/MAC. The corresponding throughput for classification rate of 31.6 kHz is 404.5 MMAC/s. Note that the current test chip does not have the digital multiplier for the second stage. Hence to estimate total system power, we have simulated a 14-bit×10-bit array multiplier in the same 0.35µm process (assuming b = 14 and resolution of β = 10). For a digital V DD = 1.5V, the energy per multiply is estimated to be 7.1pJ at a delay of 12ns. Using this value, the energy efficiency of the whole system for binary classification can be found to be ≈ 0.54pJ/MAC."
    },
    {
      "heading": "C. Regression and Classification",
      "text": "In order to verify the performance of the proposed neuromorphic ELM system in machine learning applications, we first show an example of regression (d = 1) where the system was trained on 5000 noisy samples (additive gausian noise with σ = 0.2) of a target sinc(x) function and its task was to approximate the underlying function through regression. The input data is passed through the chip and hidden layer activations are obtained. These are next used for training the output weights. This method takes care of the mismatch in the neuronal transfer curves (which is also log-normal due to sub-threshold operation) by lumping it with the current mirror mismatch and training weights that take this into account. The measured result of this experiment are shown in Fig. 16 for L = 128 hidden neurons where the noisy samples are shown in green and the regressed function is in blue. The error of 0.021 we obtain in this experiment is comparable to the error of 0.01 obtained in software simulations of ELM [21]. Next, we employ some real-world benchmark binary classification data sets from the UCI machine learning repository [19]. The reason for choosing these data sets are that they have different characteristics in terms of data dimension d and data set size in terms of number of samples: small size and low dimensions (P ima Indians diabetes, Statlog Australian credit), large size and low dimensions (Star/Galaxy -Bright), large size and high dimensions (Adult). The details of the data sets are shown in Table II. During measurements, the hidden layer matrix H is obtained by applying the training data to the chip one by one. The second layer weights are obtained offline using this H and then downloaded to the FPGA for testing. The accuracy obtained in measurements with L = 128 hidden neurons is shown in table II and is compared with software simulation results taken from [12]. This table shows that the performance of our implemented hardware ELM is comparable with the software ELM with the differences possibly due to the larger number of sigmoidal neurons (as opposed to saturating linear neurons for this chip) used in [12]."
    },
    {
      "heading": "D. Dimension Increase With Weight Reuse Technique",
      "text": "In order to evaluate the performance for the dimension extension technique, we first applied a very high dimensional dataset (leukemia) with d = 7129. Sizes for the training and testing data are 38 and 34 respectively. During measurement, we obtain a miss-classification rate of 20.59% with L = 128 neurons, which is comparable with the error rate of 19.92% obtained using the software ELM reported in [12]. Next, we separately prove the concept of artificially increasing number of hidden layer neurons. The measured errors in table II are close to optimal and do not reduce much with further increase TABLE IV: Sinc function regression using normalized hj Power supply (V) Error (%) Error (%) (Non-normalized) (Normalized) 0.8 0.5924 0.076 1 0.045 0.0629 1.2 0.1538 0.065 in L. Hence, we instead take L = 16 neurons and use weight reuse method to expand to L = 128. For the dataset diabetes, the error for L = 16 is 27.1%. This reduces to an error of 22.4%, comparable to that in tableII, when L is increased to 128 by weight reuse. Note that since our chip did not have the circuits described in Section V to perform on-chip dimension expansion, we shifted the input data before applying it to the chip. Also, the output data was shifted in the FPGA before accumulation."
    },
    {
      "heading": "E. Comparison",
      "text": "Our work is compared with other recently reported hardware machine learners in Table III. Our design is the most power efficient machine learner reported so far due to the low power analog multiplications. The energy efficiency of commercial digital processors are saturating at ≈ 100pJ/MAC [30]. Even custom digital multipliers have energy efficiencies of 10 -70pJ/MAC [17], [31], [32]. This explains the higher energy requirement of [27] in Table III. [25] uses analog floating-gate based multipliers and can hence achieve lowpower multiplication. However, our approach does not require high voltages for programming floating-gates and is also much more compact due to the use of only one transistor without capacitors in the multiplier cell. [28] also uses random mismatch (and a systematic offset) in 65nm CMOS to perform the calculations in the first stage of ELM. However, they only have a single dimensional input and only show regression. Moreover, they do not report any energy or speed metrics. Lastly, compared to [18] which also uses the same core circuit of current mirrors to perform ELM computations for neural decoding, the current work is more energy efficient due to the faster operation (as explained in section IV-C). Also, the current work shows a method of expanding input dimension to a maximum of d = 16, 384 while [18] could only support a maximum of d = 128."
    },
    {
      "heading": "F. Robustness",
      "text": "It is important to consider how the performance of the chip varies in the face of variations of power supply voltage (VDD) and temperature. We use the normalization method suggested in [18] to increase the robustness of our chip with respect to common-mode variations in VDD and temperature. Following, [18], we define the j-th normalized hidden layer value (h j,norm ) as: To show the effectiveness of normalization, we first consider its effect on variations in VDD."
    },
    {
      "heading": "1.2V).",
      "text": "It can be seen that there is a huge variation in h j (maximum of 22.7%). In contrast, when the same values are normalized (Fig. 17(b)), the variation due to change in VDD is reduced a lot (maximum of 4.2%) while variation due to change of D in is still retained. This proves effectiveness of the normalization method. We have further used the normalized and non-normalized values to perform the sinc function regression task described in Section VI-C. In this case, the weights are obtained for a nominal VDD of 1V while testing is performed at all three VDD values. The result is reported in Table IV. It can be seen that normalization enables the error to be low for all three values of VDD. Next, we studied the effect of temperature variations on the hidden layer outputs. We expect the temperature dependent weights (e ∆V T U T ) to be the major contributor to variations in hidden layer outputs h j . To confirm this prediction, we made a MATLAB model and obtained the variation of h j when temperature varied by ∆T = ±20 • C about a nominal value of T 0 = 300K. Then we benchmarked this variation with a SPICE simulation of the same circuit to confirm our earlier assumption-henceforth, we used the MATLAB model for simulations. Similar to the earlier case, we found that applying normalization reduced the maximum variation of hidden layer outputs from 9% to 1.6% over this temperature range. Next, we trained output weights for classification problems at the nominal temperature T 0 while the temperature was again varied over the same range during testing. We plot the results for h j and h j,norm for two different datasets in Fig. 18  rapidly when temperature varies on either side of T 0 while using h j . On the other hand, the error changes much more slowly when using h j,norm again confirming the benefit of normalization. Further, we have observed that retraining the weights can reduce the error close to the original value for both h j and h j,norm . Hence, to get good performance over a wider range of temperature, we can store different weights for different tmperature ranges. One disadvantage with using the normalization is that now the second layer has to perform L divisions on top of the L × C multiplications. But given the benefits provided, we believe that normalization is still a favourable choice. We do not have the normalization circuits included in this test chip but plan to include them in the next version."
    },
    {
      "heading": "VII. CONCLUSIONS",
      "text": "We have presented a low-power hardware neuromorphic IC in 0.35µm CMOS for machine learning applications using randomized neural networks such as random vector function link (RVFL), reservoir computing methods or extreme learning machines (ELM). Our hardware can also be used as a dimension reduction mechanism prior to applying unsupervised algorithms like k-nearest neighbors for clustering if the nonlinear saturation in the neuron is not applied [33], [34]. The particular algorithm we employed in this work is extreme learning machine (ELM). The mismatch in silicon spiking neurons and synapses are used to perform the vector-matrix multiplication that forms the first stage of this classifier and is the most computationally intensive. Our results indicate that for a wide set of problems, σV T in the range of 15 -25mV gives optimal results. A design space exploration is performed to show that minimum energy per operation at a specific VDD is obtained by operating for a short time at the highest spiking frequency achievable at that VDD. Linear neurons with a saturating non-linearity are used due to ease of implementation. Operating from a 1 V power supply, this system can achieve an optimum energy efficiency of 0.47 pJ/MAC with a corresponding classification rate of 31.6 kHz making it one of the most energy efficient machine learners reported. Though this hardware can only implement randomized neural networks which might require a penalty of 2 -3X more number of hidden nodes compared to networks with full tunability [35] in many applications, the 10 -20X lower energy required by random coefficient multiplications in our method overcome this penalty for lowering overall system energy. We also show a normalization method that enables a more robust operation of the circuit over changes in power supply and temperature. In future, we will apply this chip to classify multi-class image datasets such as MNIST. We will also explore the possibility of using it for dimension reduction prior to unsupervised clustering."
    }
  ]
}