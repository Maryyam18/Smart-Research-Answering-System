{
  "paperid": "2310.10691v1",
  "title": "Enhancing ML model accuracy for Digital VLSI circuits using diffusion models: A study on synthetic data generation",
  "authors": [
    "Srivastava",
    "Kumar",
    "Abbas",
    "Goodfellow",
    "Pouget-Abadie",
    "Mirza",
    "Xu",
    "Warde-Farley",
    "Ozair",
    "Courville",
    "Bengio",
    "Danisetty",
    "Mylaram",
    "Kumar",
    "Kingma",
    "Welling",
    "Ho",
    "Jain",
    "Abbeel",
    "Amuru",
    "Zahra",
    "Vudumula",
    "Cherupally",
    "Gurram",
    "Ahmad",
    "Abbas",
    "Jafari",
    "Sadri",
    "Zekri",
    "Li",
    "Wang",
    "Li",
    "Zhou",
    "Lin",
    "Devi",
    "Tilwankar",
    "Zele",
    "Afacan",
    "Lourenço",
    "Martins",
    "Dündar",
    "Kahraman",
    "Yildirim",
    "Agarwal",
    "Jain",
    "Amuru",
    "Abbas",
    "Amuru",
    "Ahmed",
    "Abbas",
    "Munappy",
    "Bosch",
    "Olsson",
    "Arpteg",
    "Brinne",
    "Amuru",
    "Zahra",
    "Abbas",
    "Wong",
    "Gatt",
    "Stamatescu",
    "Mcdonnell",
    "Mikołajczyk",
    "Grochowski",
    "Asif",
    "Nazeer",
    "Javaid",
    "Alkhammash",
    "Hadjouni",
    "Tan",
    "Guo",
    "Shorten",
    "Khoshgoftaar",
    "Kortylewski",
    "Schneider",
    "Gerig",
    "Egger",
    "Morel-Forster",
    "Vetter",
    "Chalongvorachai",
    "Woraratpanya",
    "Zhang",
    "Zhou",
    "Abbasnejad",
    "Sridharan",
    "Nguyen",
    "Denman",
    "Fookes",
    "Lucey",
    "Dhariwal",
    "Nichol",
    "Yang",
    "Zhang",
    "Song",
    "Hong",
    "Xu",
    "Zhao",
    "Zhang",
    "Cui",
    "Yang",
    "Bank",
    "Koenigstein",
    "Giryes",
    "Theis",
    "Oord",
    "Bethge"
  ],
  "year": 2023,
  "abstract": "Generative AI has seen remarkable growth over the past few years, with diffusion models being state-of-the-art for image generation. This study investigates the use of diffusion models in generating artificial data generation for electronic circuits for enhancing the accuracy of subsequent machine learning models in tasks such as performance assessment, design, and testing when training data is usually known to be very limited. We utilize simulations in the HSPICE design environment with 22nm CMOS technology nodes to obtain representative real training data for our proposed diffusion model. Our results demonstrate the close resemblance of synthetic data using diffusion model to real data. We validate the quality of generated data, and demonstrate that data augmentation certainly effective in predictive analysis of VLSI design for digital circuits.",
  "sections": [
    {
      "heading": "Related Works",
      "text": "Data scarcity is defined as the inadequacy in quantity or diversity of training data, which can restrict the learning ability of an ML model. It is a universal issue which affects the deployment of AI/ML in multiple domains as discussed by A. Munappy et al. [13]. Many of the proposed AI/ML applications in VLSI design domain [8,10,12] rely on a large amount of training data. For example in [12] and [14] authors have achieved precise training with 15K and 50K samples respectively. Hence in VLSI domain, training data scarcity stemming from cost, time, and quality constraints poses a challenge which needs to be addressed. Many studies have proposed synthetic data generation to create large scale training datasets in various other domains and provide a significant improvement to existing AI/ML models [15,16,17,18,19,20,21,22,23]. To the best of our knowledge, this is the first time a data augmentation for circuit data is studied with latest diffusion models. Generative models such as Variational Autoencoders (VAE) [3], Generative Adversarial Networks (GAN) [1], and Diffusion Probabilistic models [4] are known to produce large and diverse synthetic data for image datasets. Diffusion models have been observed to have an edge over other generative models in terms of quality for image data generation [24]. Diffusion models have found applications in various domains ranging from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines [25]."
    },
    {
      "heading": "The dataset",
      "text": "We gathered extensive datasets covering design, process, and performance parameters for twelve core digital cells, see Table 1. These parameters are meticulously chosen to enable subsequent performance assessment using a predictive ML model, validating the utility of dataset. For training, we employ simulated data from Electronic Design Automation (EDA) tool HSPICE [26]. Training data for Digital cells (Table 1) comprises vectors of random values, drawn from Gaussian distributions of process parameters. We account for ±10% variations at 3σ in CMOS standard cells at 22nm High-K MGK via Predictive Technology Models (PTM). Twelve process parameters (PMOS and NMOS) are included. In addition to statistical distributions, temperature samples spanning -55 • C to 125 • C and supply voltage deviations of ±10% from the nominal (0.8V) are integrated. Load capacitance varies similarly to process parameters. We perform propagation delay estimations via HSPICE Monte-Carlo simulations to obtain training data, encompassing PVT (Process, Voltage, Temperature) variations."
    },
    {
      "heading": "Synthetic circuit-data generation using Diffusion Models",
      "text": "Parametric data from VLSI designs is continuous data that holds valuable insights for ML-driven automation of VLSI performance assessment, design, and testing. As mentioned before, this paper aims to apply denoising diffusion probabilistic models for generating synthetic data for VLSI designs, enhancing the training of ML models in data-scarce scenarios. This indirectly advances automation in VLSI design tasks. Our work illustrates the development of an accurate diffusion model-based synthetic data generation method for delay estimations in various 22nm CMOS technology-based digital VLSI circuits. The methodology can be divided broadly into the following steps"
    },
    {
      "heading": "Formulation of a Denoising Diffusion Probabilistic Model tailored for VLSI circuit-data",
      "text": "Diffusion models define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise. Diffusion models have two processes to follow: Forward Process -Here, random noise is incrementally added to data over multiple time steps. This sequential noise introduction is mathematically characterized as follows: Where x t represents the data at time step t, where 0 < t < T and T are the total number of steps. This process starts with the original data x 0 and iteratively adds noise over T steps, with β t controlling the amount of noise added at each step. The variance schedule β t determines the trade-off between introducing noise and maintaining data fidelity. As t increases, the noise contribution becomes more significant due to the increasing value of β t . Reverse process -This phase involves predicting the noise added to each data point during the forward process. A neural network, denoted as f θ , predicts noise ϵ t from noisy data x t as ϵ t = f θ (x t ) Generating new data -New data samples are generated by performing the reverse process on random noise samples ϵ t drawn from N (0, I). The reverse process reconstructs data by iteratively removing predicted noise contributions: The number of steps T controls the balance between data fidelity and noise injection, influencing the quality of generated samples. Existing models deal with very complex data modalities such as images. For image generation a UNET architecture with residual connections was proposed. Since our target dataset is relatively less complex than images, we propose a simple encoder-decoder architecture [27] instead of a UNET for reverse denoising process."
    },
    {
      "heading": "Qualitative evaluation of generated artificial data",
      "text": "The quality assessment of synthetic data involves evaluating measures like inception score, Frechet inception distance, average log-likelihood, Parzen window estimates, and visual fidelity. However, these metrics primarily cater to image data, making it unclear which measure is optimal for other data modalities. Theis et al. [28] suggested that evaluation for generative models should align with the intended application. Thus, we evaluate diffusion models for circuit data by directly comparing them to the source of our data, which is the simulator. We extract specific features from the synthetic dataset, designating them as input features, while the rest are deemed output features. Subsequently, we feed the input features into the simulator and compare the resulting output feature values with the generated output feature values. Our evaluation metric in this study is the mean absolute percentage error. Refer to Figure 1 for a visual representation of the comprehensive evaluation procedure for the generated data. The diffusion model is trained using just 500 real data samples. Subsequently, artificial samples are generated and used for performance evaluation. Training continues through epochs until desired performance is reached, with hyperparameter adjustment for subpar results. 5 Experimental setup and model architecture We use Python-3.8.16 and Google Colab for the training of Diffusion Denoising Probabilistic models. Moreover, our implementation uses Keras-2.9.0 and Tensorflow-2.9.2. As discussed in 4.1, a diffusion model is devised for each dataset, encompassing forward and reverse processes for circuit data. For the forward process, we adopt a variance of β t , transitioning linearly from 0.001 to 0.02, following the approach by Ho et al. [4] The reverse process is realized using an encoder-decoder architecture [27] with continuous batch normalization. The model utilizes Leaky ReLu activation. Training data undergoes noise addition through the forward process, gradually transforming to pure noise, and the reverse process learns to de-noise and predict original distribution. After training the diffusion model, the synthetic data is then generated by first sampling a pure random noise then using trained reverse diffusion model to generate the desired sample."
    },
    {
      "heading": "Results",
      "text": "As discussed in 4.2, the output feature values from the model and the simulator are compared to get the right idea of performance. We start the hyper-parameter search by finding the optimal number of layers. Table 2 depicts the model's performance across varying hidden layer counts. A fivelayer architecture emerges as the best choice for the NOT gate dataset featuring 17 attributes. This architecture also holds well for datasets up to 19 attributes. It was observed that for datasets featuring 21 attributes, a six-layered architecture boosts the learning capacity effectively. Our subsequent exploration involves different learning rates. Figure 2 indicates that a learning rate near 0.0005 ensures consistently low percentage errors across all features. The Table 3 provides the mean absolute percentage errors (MAPE) attained for all datasets, it can be seen that low MAPE is obtained for the various datasets. Additionally, Figure 3 showcases that density distribution of generated and original data for the NOT dataset exhibit a high degree of proximity. Furthermore Table 4 shows a significant improvement in a gradient-boosting regression (GBR) model using artificial data, to predict CMOS NOT gate delays. Thus, validating the proposed approach's efficacy in predicting parameters that concern digital circuit design. For brevity, the data generation with other generative models such as GANs or VAEs were not as effective, and hence not shown in results."
    },
    {
      "heading": "Conclusion",
      "text": "Achieving model accuracy hinges on high-quality training data, yet obtaining ample data for electronic circuits can be expensive or practically difficult to obtain."
    }
  ]
}