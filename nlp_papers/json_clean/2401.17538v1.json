{
  "paperid": "2401.17538v1",
  "title": "Post-Quantum Cryptography for Internet of Things: A Survey on Performance and Optimization",
  "authors": [
    "Tao Liu",
    "Gowri Ramachandran",
    "Raja Jurdak",
    "M Hasan",
    "P Shor",
    "L Grover",
    "L Malina",
    "P Dzurenda",
    "S Ricci",
    "J Hajny"
  ],
  "year": 2024,
  "abstract": "Due to recent development in quantum computing, the invention of a large quantum computer is no longer a distant future. Quantum computing severely threatens modern cryptography, as the hard mathematical problems beneath classic public-key cryptosystems can be solved easily by a sufficiently large quantum computer. As such, researchers have proposed PQC based on problems that even quantum computers cannot efficiently solve. Generally, post-quantum encryption and signatures can be hard to compute. This could potentially be a problem for IoT, which usually consist lightweight devices with limited computational power. In this paper, we survey existing literature on the performance for PQC in resource-constrained devices to understand the severeness of this problem. We also review recent proposals to optimize PQC algorithms for resource-constrained devices. Overall, we find that whilst PQC may be feasible for reasonably lightweight IoT, proposals for their optimization seem to lack standardization. As such, we suggest future research to seek coordination, in order to ensure an efficient and safe migration toward IoT for the post-quantum era.",
  "sections": [
    {
      "heading": "I. INTRODUCTION",
      "text": "I NTERNET of Things (IoT) is a promising technology that emerged in recent years. IoT has many potential applications, ranging from connected personal gadgets to large-scale industrial drone networks. It has been estimated that 12.2 billion IoT devices were active in 2021, and this number will grow to 27 billion by 2025 [1]. The security of IoT technologies remains a concern, as the computation cost of cryptography algorithms used for digital encryption and authentication may be burdensome for resource-constrained IoT node devices that were designed to be deployed en masse, and sometimes in inaccessible remote areas. Very often, factors such as production cost, size and battery life are prioritized over security for such low-end lightweight devices. If IoT technologies are indeed to integrate with agriculture, industrial production, medical service, military and everyday lives in general, adequate digital encryption and authentication standards should be enforced. Currently, one significant issue with applying cryptography in IoT is their high computational cost, which often scales with the key sizes of algorithms used. On the other hand, an upgrade for cryptographic standards will soon be necessary due to the latest development in quantum computing, as this advanced computation technology may force the world to use even more complex cryptographic algorithms, further exacerbating the current problem for IoT systems. A sufficiently large quantum computer can use Shor's algorithm [2] to solve Integer Factorization and Discrete Logarithmic problems, both used as fundamental hard problems for today's public-key cryptosystems. Additionally, Grover's algorithm [3] makes symmetric keys easier to search by providing quadratic speed up for the unstructured search problem. As such, whilst symmetric ciphers need to double the key size in order to maintain their security level, asymmetric cryptography must undergo a fundamental change in response to the emergence of quantum computing. These developments threaten the security of contemporary cryptographic schemes used in IoT applications.Formally, cryptography that is secure against an attacker with large quantum computers is categorized as Post-Quantum Cryptography (PQC). Naturally, to ensure security in future IoT systems, lightweight devices should also be protected by PQC. Yet a very practical problem remains: in general, quantum-resistant encryption and signature schemes are more computationally intensive than any public-key cryptosystems currently in-use [4]. As a result, the migration toward PQC may have an undesired impact on IoT systems, where devices typically have limited energy supply, memory sizes, and processing speed. Recently, such complications have been noticed by researchers, and efforts were made to evaluate and optimize the performance of PQC algorithms in resource constrained devices. A comprehensive survey on this topic is [5], underscoring the problem that complex PQC algorithms may not be efficient enough on resource constrained IoT devices. This work reviews typical IoT network structures and past efforts for PQC migration. Furthermore, it surveys literature on the performance of candidates of a major PQC standardization project [6] led by the United States National Institute of Standards and Technology (NIST), and compares their suitability for IoT systems. The content of this survey remains largely relevant today, but more recent developments warrant revisiting its findings. For example, this survey concluded that few works have proposed Post-Quantum solutions specific to resource-constrained IoT devices, but recent projects, such as pqm4 [7], have gained traction in this direction since the survey's publication. We also noticed that some algorithms recommended by this survey were eliminated in the NIST PQC standardization project, rendering them less relevant in today's context. There are also other works that investigate the Post-Quantum security issues for IoT. Malina et al. [8] is a survey on privacy-protecting technologies for IoT systems. Although it surveys the performance of several round-3 NIST PQC candidates, we found the information it provides is not sufficiently up-to-date: for example, NIST later selected Kyber and Dilithium as the primary KEM and signature schemes respectively [9], but the article only includes performance evaluation for one variation from each algorithm: Kyber1024 and Dilithium-III. Furthermore, data for both algorithms were drawn from sources published before 2019. The authors of [10], on the other hand, review the security requirement at different layers of IoT and the application of lattice-based cryptography (LBC) for IoT security. The authors further propose a classification framework for the application of LBC in resource-constrained devices. However, an IETF classification for resource-constrained devices published in 2014 [11] was used, and the paper concludes that LBC is inefficient in many use cases."
    },
    {
      "heading": "A. Data Encryption for IoT",
      "text": "In order to investigate the feasibility of applying PQC in IoT, it is important to first identify the application of contemporary pre-quantum cryptography in IoT. Figure 1 demonstrates the basic IoT network structure: the design goal is to connect the \"Things\", i.e., IoT sensor/actuator devices to the Internet but full-blown Internet protocols such as TCP/IP do not fit into the resource-constrained hardware they operate on. Therefore, a gateway is required as a medium between the \"Things\" and the Internet [12]. The node-to-gateway connection is often wireless, allowing adversaries in proximity to eavesdrop if the broadcast is not encrypted. On the other hand, connections beyond the gateway can support more functionality due to the availability of network stacks and increased device capacity. A gateway may connect to an edge device, a cloud server, or join a many-to-many network over protocols such as Message Queuing Telemetry Transport (MQTT). It is also easier to implement security measures at this level, although the resource constraints may still be an issue for gateway devices that are mobile or in remote areas where energy and/or bandwidth are scarce. In general, we assume that the computation cost of securely transferring data to and from gateways and nodes may be heavy in the post-quantum era. To date, Zigbee and LoRaWAN are two common technologies used for device-to-gateway communication, whilst MQTT is a publish-subscribe protocol used for data transmission from the gateways. In Table I we present their currently supported encryption methods, and possible upgrades for them to be Post-Quantum secure. It is noted that both Zigbee and LoRaWAN rely only on 128-bit AES encryption, and the use of pre-installed keys is always necessary, whilst MQTT can use Transport Layer Security (TLS) and any custom payload encryption at application level. One issue about key preinstallation is the balance between scalability and security: on one hand, key diversification is usually preferred to limit the impact of side channel attacks [13] (i.e., compromising one key will only compromise the device that uses it). On the other hand, keeping track of pre-installed keys can also be burdensome considering an IoT application may use thousands of small devices. Malik et al. [14] surveyed IoT key bootstrapping protocols based on public key cryptography (PKC) and summarized proposals into five categories: Raw Public Key, Certificate-Based, Identity-Based, Self-Certified and Certificate-less. However, for most of these sophisticated protocols to function, public-key cryptographic functions need to be executed frequently in both server and client devices. Therefore, especially under a post-quantum context, it would be crucial to understand if public-key cryptographic functions are feasible to run on resource-constrained devices."
    },
    {
      "heading": "B. Cryptography and Information Security",
      "text": "Cryptography is closely associated with encryption. By using a key and an algorithm (consists of permutations, substitutions and other operations) to \"scramble\" a plaintext, it is possible to produce a ciphertext which can only be recovered with the correct key. Whilst the primary objective of cryptography is to protect confidentiality, modern cryptography can be used to achieve three other objectives: data integrity, authentication and non-repudiation [15]. The usage of keys differentiates modern cryptography into two distinctive categories: symmetric cryptography and asymmetric cryptography (also known as public-key cryptography). In symmetric cryptography, the same key is used to both encrypt and decrypt data. In asymmetric cryptography, a \"private key\" is kept secret, and the \"public key\", as its name suggests, is made public. such arrangement allows interesting functionalities which is critical for information security: when the public key is used to encrypt, it ensures that a secret message can only be read by the intended recipient; when the secret key is used to encrypt, on the other hand, entities with the public key can verify that a non-repudiable message is indeed made by the secret key owner. Together, these two categories of cryptography guarantee the secure transmission and storage of information in the digital world. One of the most important applications of cryptography is the Transport Layer Security (TLS) protocol, which is the standard method for establishing confidential and authenticated communication channels over the open Internet. With TLS, two distant parties can authenticate each other using digital signatures and third-party certificates, and then establish mutual secrets using asymmetric key exchange such as the Diffie-Hellman algorithm (without actually transmitting the secret, thus eliminating the risk of eavesdropping). Then, the mutual secret can be used as the key for symmetric ciphers, which are much more efficient for bulk data encryption and decryption."
    },
    {
      "heading": "C. Quantum Threats Against Cryptography",
      "text": "It is known that quantum computing poses threats to both symmetric and asymmetric cryptography, but the threat against asymmetric cryptography is more significant [16]. An overview of existing cryptosystems and how they are threatened by quantum computing is presented in Table I. As shown in the table, state-of-the-art asymmetric cryptography -including the RSA, the Elliptic-Curve Diffie-Hellman (ECDH) key exchange, and the Elliptic-Curve Digital Signature Algorithm (ECDSA) -offer no security against quantum computing. The security of RSA relies on the hardness of factoring large bi-prime numbers, also known as the Integer Factorization (IF) problem. On the other hand, Elliptic Curve Cryptography (ECC) takes the assumption that Elliptic Curve Discrete Logarithm (ECDL) problems are hard to solve. Integer Factorization and the Discrete Logarithm Problem are believed to be hard for classic computers, yet they can be solved in polynomial time by a quantum computer large enough to run Shor's algorithm. Whilst small, experimental quantum computers today are incapable of solving practical ciphers, researchers have been estimating the quantum resources needed to achieve such a goal [17], [18]. Other studies, such as [19], [20] explore the possibility of using variations of TABLE I: List of modern cryptosystems, their fundamental problems and security against quantum computing. Cryptosystem Fundamental Problem Current State Secure Against QC? Diffie-Hellman (DH) Discrete Logarithm Outdated No DSA Outdated RSA Integer Factorization In Use ECDH Discrete Logarithm over Elliptic Curves In Use ECDSA In Use Symmetric Ciphers Exhaustive search & collision-finding in large key spaces In Use Yes 1 PQC Lattice Problems, Error-Correction Codes, Multivariate Systems, Hash Trees, Isogeny Graphs Developing Yes 1 Increasing key size may be required for maintaining security level. Shor's algorithm to factorize small bi-prime and tri-primes in quantum architectures. The quantum approach to solving the Discrete Logarithm problem over elliptic curves has also been discussed in theory in [21]. On attacking symmetric cryptography, Grover's algorithm can potentially decrease the security strength of existing ciphers by offering a quadratic speed-up for exhaustive keysearching [3], but the practical implication of this attack is still debated as Grover's algorithm requires queries to be run sequentially. Nevertheless, researchers have estimated that a quantum architecture with 2953, 4449, 6681 qubits can carry out Grover's search against the Advanced Encryption Standard (AES) with 128-, 192-and 256-bit security, respectively [22]. Other works, such as [23] and [24], outlines additional quantum attacks that can potentially reduce the hardness of exhaustive key-searching. Whilst theoretical estimations suggest at least several thousands of qubits are needed in a quantum computer to break existing cryptography, global technology giants are certainly making their efforts to reach this goal. Currently, IBM takes lead in quantum computing hardware with its 127-qubit processor [25] and ambition to go beyond 1000 qubits in 2023 [26]. It has been predicted that the likelihood of having a scalable quantum computer in the next decade is significant [27]. As such, quantum-resistant encryption and signature schemes are urgently needed."
    },
    {
      "heading": "D. Migration Toward PQC",
      "text": "By formal definition, post-quantum cryptography is cryptography under the assumption that the attacker has a large quantum computer (i.e. the quantum threat discussed in the previous section is realistic), and the central challenge in PQC is to maintain cryptographic usability and flexibility without sacrificing confidence [28]. This requires cryptography to stop relying on the assumed hardness of integer factorization and the discrete logarithmic problem and start using problems that even quantum computers cannot efficiently solve. To date, there have been calls for the Internet to prepare for migration TABLE II: Taxonomy of PQC, their fundamental problems and notable cryptographic schemes submitted to the NIST PQC standardization project. PQC Category Fundamental Problems Notable Schemes Lattice-based Problems based on structured lattices, such as the (Modular) Learning With Error problem and the NTRU problem Kyber, Dilithium, Faclon Code-based Problems related to decoding linear binary error-correcting code, to which random errors were added. McEliece, HQC, BIKE Hash-based Signatures Combining multiple one-time signature key pairs into a hash tree to generate signatures. SPHINCS+, XMSS Isogeny-based Problems based on finding isogenies on a large isogeny graph. SIKE Multivariate Problems based on finding solutions to large systems of quadratic equations over finite fields. Rainbow toward Post-Quantum Cryptography (PQC) despite the fact that a large quantum computer has not been built, and the reasons are two-fold: first, secret communications today may be stored and decrypted with QC later and second, pre-quantum public-key cryptosystems like Rivest-Shamir-Adleman (RSA) and Elliptic Curve Cryptography (ECC) have been embedded within numerous applications and protocols. It is not easy to upgrade software and standards used globally, and such effort must be organized with care. The U.S. National Cybersecurity Center of Excellence's Migration to Post-Quantum Cryptography project [29], which collaboratively involves many global technology giants, is an example in this direction. The NIST's PQC standardization project [6] is another major effort aimed to \"solicit, evaluate, and standardize one or more quantumresistant public-key cryptographic algorithm\". Started in 2017, the NIST PQC standardization project initially received a total of 69 key exchange mechanism (KEM) and signature scheme proposals, which can be separated into five categories based on their underlying hard problems, as listed in table II. Over the course of 5 years, many have been found unsafe or impractical. Recently, four public-key post-Quantum algorithms have been selected [9], including one key encapsulation mechanism (KEM) and three digital signature schemes. It is expected that at least one additional KEM will also be selected in the near future. A summary of these algorithms is provided in Table III. Note that two additional stateful hash-based signature schemes are also included in Table III: the Leighton-Micali Signature (LMS) system and the eXtended Merkle Signature Scheme (XMSS). These two signatures are not candidates of the main NIST PQC standardization project due to the fact that stateful hash-based signatures need to maintain internal states and cannot be implemented using the standard API (i.e. Key Generate, Sign and Verify). As a result, they were approved by the NIST in a separate publication [30]."
    },
    {
      "heading": "III. METHODOLOGY",
      "text": "We searched digital databases for works that include \"Post-Quantum Cryptography\" and \"Internet of Things\" in the full text. The databases are IEEE Xplore and ACM Digital TABLE III: List of PQC Algorithms standardized and currently under evaluation by the NIST. Algorithm Foundation Type NIST Evaluation Kyber Lattice-based KEM Recommended Dilithium Lattice-based Signature Recommended Falcon Lattice-based Signature Alternative SPHINCS+ Stateless Hashbased Signature Alternative HIKE Code-based KEM Under Evaluation HQC Code-based KEM Under Evaluation Classic McEliece Code-based KEM Under Evaluation XMSS Stateful Hashbased Signature Recommended 1 LMS Stateful Hashbased Signature Recommended 1 1 Recommended in NIST SP 800-208 [30], separate from the main NIST PQC standardization project. In general, stateful hash-based cryptography requires careful management of internal states and is harder to implement securely. Library due to their dominance and reputation in the field of computer science. Additionally, we searched the Cryptology ePrint Archive for the newest development. Although works found on the Cryptology ePrint Archive are not peer reviewed, the archive itself has a reputation for its fast processing. Since our work aims to survey the most up-to-date information, this suits our purpose as long as the quality of papers found is critically analyzed. The search results were then reviewed and manually filtered. In our scope, a relevant article should: • be about PQC algorithms and their application on resource-constrained platforms, and • evaluate the performance of PQC algorithms, or propose a solution to optimize PQC algorithms for performance, and • be published after 2020, as [8] has already conducted a survey in 2019. A diagram illustrating our filtering process is shown in figure 2. After the initial filtering of search results, 83 publications remained for detailed review and categorization. We found that a total of 34 publications were relevant to the scope of survey: 9 publications are dedicated to the performance evaluation of PQC algorithms on certain resource constrained platforms, whereas the other 25 propose new software designs, hardware designs, or software/hardware co-designs to optimize the performance of PQC algorithms. Tables IV, V and VI provide lists of literature reviewed, categorized by their primary topics; a pie chart showing the distribution of the research topics is also presented in Figure 3."
    },
    {
      "heading": "IV. PQC PERFORMANCE EVALUATION",
      "text": "The foremost problem that needs to be addressed when discussing Post-Quantum security for IoT systems is the performance of PQC algorithms, as it was discussed in [5] that complex computation may be burdensome for IoT devices. In Table IV, we summarize recent works on this topic. We notice that the majority of the literature (except for [31], [32]) include algorithms approved by the NIST (Kyber, Dilithium, Falcon and SPHINCS+) in the evaluation. All works except for one ( Fig. 2: Process used to determine publications relevant to our survey's scope. The number of results found by using the search criteria from each database is accurate as of Jan 2023. [33], which focuses only on PQC power consumption) evaluate the speed of PQC algorithms, either in real time or CPU cycles. Such a trend is understandable, as speed/latency is the most intuitive representation of efficiency, and a key requirement for the Next-Generation smart IoT systems [34]. However, we must keep in mind that IoT systems often consist of large numbers of small devices, and it is also important to keep other metrics in check: for example, code size (which consumes flash memory), transmission size (which increases bandwidth requirement), RAM usage, and energy/power consumption. As shown in Table IV, we are yet to find a publication which presents a uniform evaluation framework that considers all those metrics for one or more NIST-selected PQC algorithms operating in resource constrained environments. Lastly, we also note that while [31]- [33], [35], [36] focused on PQC primitives in resource constrained environment, [37]- [40] implemented the primitives as part of Post-Quantum Transport Layer Security (PQTLS) protocol and evaluated the performance at the transport level. Overall, recent works tend to be optimistic in regard to the performance of lattice-based PQC for IoT: despite significant performance overhead, PQC is still shown to be feasible in some reasonably constrained devices. Bürstinghaus-Steinbach et al. [38] suggested that the NIST-selected, lattice-based algorithm Kyber runs faster than ECDH by at least one order of magnitude, when both algorithms are executed as primitives on four lightweight devices. However, the code-based SPHINCS+, which was selected by the NIST as an alternative to lattice, demonstrates slower execution than ECDSA. [38] further discovered that a Post-Quantum TLS protocol using Kyber-SPHINCS+ cipher suite would introduce a significant burden in terms of latency and memory usage, mainly due to the cost of computing signatures with SPHINCS+. The efficiency of Kyber (and lattice-based cryptography in general) is also shown in [35], [36]. In [35], Mayes demonstrated that the encryption arithmetic of Kyber768 can be run inside a MULTOS Trust Anchor [41] with just 13 kilobytes of RAM, although the process requires approximately 9.8 seconds even with the aid of a cryptography co-processor. For more powerful devices, [36] showed that a Raspberry Pi 4 requires 100-300 microseconds to compute main functions (key generation, encapsulation and decapsulation) of Kyber and SABER, yet lattice-based signatures (Dilithium and Falcon) are more expensive to compute in general. Another study, [33], investigated the power consumption of using Dilithium and Falcon for signature-based authentication. It was found that Dilithium and Falcon's power consumption is in fact on par with RSA-4096, which is commonly used in the pre-quantum era. A key topic related to the efficiency of PQC in IoT is the Post-Quantum Transport Layer Security (PQTLS). As public-key cryptography is used for key exchange and signature verification in the pre-quantum TLS, their eventual upgrade to PQC should be expected. On a Cortex-M4 microcontroller unit (MCU), [40] tested PQKEMs in place of ECDH in TLS 1.2 and measured the overhead, both in terms of latency and energy consumption. It was suggested that the overhead induced by PQC is feasible for IoT, especially with latticed-based KEMs which consumed less energy than ECDH. Furthermore, the authors noted that for lattice-based KEMs, a large portion of latency overhead was caused by larger bandwidth requirements, rather than the actual computation of PQC. As such the potential of hardware acceleration may be limited for PQTLS. A more recent work on this matter is [37], which used the OQS-OpenSSL from Open Quantum Safe to establish TLS 1.3 handshake over MQTT protocol. A key observation noted in this work is that lattice-based KEMs (Kyber, NTRU, NTRU Prime and SABER) achieved faster key exchange than EDCH on both RPi3 and RPi4, despite having to transmit larger packets. However, the same cannot be said for signature authentication as ECDSA outperformed all PQ signature schemes. A key difference between [40] and [37] is software implementation. [40] integrated PQC ciphers within the WolfSSL library, which only supported TLS 1.2 at the time, while the OQS-OpenSSL library used by [40] supports TLS 1.3 and was develo"
    },
    {
      "heading": "V. SOFTWARE OPTIMIZATION FOR PQC",
      "text": "As shown in Table V, recent works on software optimization for PQC focused heavily on lattice-based cryptography (LBC), as 7 out of a total 9 papers described means to optimize lattice-based KEM and signatures. However, papers published in 2020 did not seem to focus on a specific type of PQC: ways to optimize hash-based (XMSS), lattice-based (NTRU Primes, Saber) and a cipher based on zero-knowledge proofs, namely Picnic, were discussed. However, after 2021 there seems to be a shift in direction, as all 5 papers focused on optimizing LBC algorithms, including Dilithium and Falcon which were officially selected by the NIST for standardization. In regard to optimization goals, we note that proposals for PQC software optimization aim all to improve operation speed (latency) and memory usage. In general, the quest for optimization is a trade-off problem between speed and memory usage. For example, a relatively straight-forward optimization technique for reducing minimum memory requirement is to generate data required for computation (i.e. coefficient matrices of large polynomial systems) only when needed, instead of storing all data in the memory space simultaneously. This technique was used in [43]- [45] to drastically reduce memory usage at the cost of extra computation time. For LBC, the most computational intensive task is polynomial multiplication: as practical LBC algorithms need to compute multiplications of large and high-degree polynomials, the optimization of this operation is rewarding. In a \"school book\" implementation, polynomial multiplication can be done by multiplying each pair of coefficients, which has a time complexity of O(n 2 ), where n is the degree of two polynomials being multiplied together. Alternatively, an algorithm which operates on the similar principles of Fast Fourier Transform (FFT) -called Number Theoretic Transform (NTT) -can reduce this complexity to the order of O(n) log n [46]. Broadly speaking, like FFT and inverse-FFT, NTT and inverse-NTT transforms polynomials to and back from points representation using values evaluated at roots of unity. Yet unlike FFT which uses n-th complex roots of unity (i.e. complex number z where z n = 1), NTT uses roots of unity in a finite field (i.e. modular arithmetic over a prime number). Some papers demonstrate that it is possible to modify the standard NTT algorithm for further enhancement in performance: for example, [43] notices that some polynomials used in Dilithium have small coefficients and proposes a parallel algorithm which runs faster with \"small\" polynomials than the standard NTT. [44], on the other hand, suggests that by performing NTT over a smaller prime number, some polynomial coefficient values can be stored as 16-bit value instead of 32-bit. However, this deviates from the standard specification of Dilithium, which raises questions about whether or not such modification affects the scheme's security strength -unfortunately, such issues were not fully discussed. Whilst other works optimize the algorithms by making trade-offs between speed and memory usage, Alkim et al. [47] claimed to achieve both. The authors present three means to optimize the NTRU Prime KEM and claim that one is optimized for speed using Good's trick [48], whilst the other two use mixed-radix NTT multiplication and are optimized for memory usage. Despite the fact that these methods focus on different metrics, all of them seem to outperform the standard pqm4 implementation -which is already a PQC library optimized for the Cortex-M4 platforms [7] -in both speed and memory. The only downside appears to be the fact that those optimizations only support a limited set of parameters used in NTRU Prime. Nevertheless, we should note that NTRU Prime is no longer a KEM being considered for NIST standardization, but this work does raise the question of whether a joint speed-and-memory optimization is possible for other LBC algorithms. On the other hand, for the stateful hash-based signature (HBS) XMSS, [49] discusses an interesting technique for maximizing speed for signature verification on lightweight devices. One key characteristic of stateful HBS is that the private key used by the signer consists of a large set of onetime signature (OTS) keys, and an OTS key should never be reused [30]. Additionally, the verification of XMSS signatures is not a constant-time operation, as the time to verify signatures (of the same message) generated by different OTS varies for each OTS. Therefore the authors propose an altered XMSS algorithm, in which the signer uses a probabilistic method to search for signatures that are faster to verify. The addition of this \"search\" process, in practice, seems to drastically increase the signing time: on a general purpose CPU, the increase in signing time may reach one minute, and this does not seem feasible for any real-time network application that requires frequent signature-based authentication -for example, a TLSlike protocol for server"
    },
    {
      "heading": "VI. HARDWARE OPTIMIZATION FOR PQC",
      "text": "Another practical way to optimize cryptography for performance is implementing cryptographic functions in hardware, such as using Field Programmable Gate Arrays (FPGAs) or Application Specific Integrated Circuits (ASICs). In general, hardware implementation is faster than software implementation of the same function, yet the benefit in this case is not only execution speed: the need for cryptographic functions to compete with application logic for memory space can also be eliminated by moving those functions to separate hardware, and this may be crucial for memory constrained IoT devices. However, such perks come with a cost: the introduction of extra hardware would also cost extra energy, and this problem is more prominent for FPGAs than ASICs [54]. On the other side, ASICs sacrifice flexibility (i.e. unlike FPGAs, ASICs cannot be re-programmed once produced) for performance, and generally have higher development cost along with longer time-to-market [55]. Before the discussion of specific works in Table VI, we should keep in mind that a common issue with hardware optimization is the lack of standards. For software, NIST's reference implementations in C and the pqm4 library are used as the de facto standard. For hardware optimization, many work report improvement in some aspects over a previous similar hardware design, but as a common baseline does not seem to exist, cross-comparing is rather difficult. Therefore, on the right-hand-side of Table VI where we indicate whether each optimization \"improves\" or \"sacrifices\" a metric, the indications are not relative to a global or absolute baseline, but a summary of comparison made to certain previous works selected by the authors. Nevertheless, we surveyed papers listed in Table VI. We notice that unlike optimized software implementations, recent works on hardware PQC are less focused on NIST PQC candidates. 4 out of 13 surveyed papers propose optimization for the Binary Ring Learning With Error (Bin-RLWE) [56], which seems to be a popular cipher among hardware designers [57]- [60]. One possible reason for this is that Bin-RLWE is by design lightweight and implementation-friendly, as the binary error values can be efficiently generated in hardware [61]. Yet the Bin-RLWE was not submitted as a NIST standardization candidate. As such, the likelihood of it being widely accepted is not as high as other lattice-based schemes, despite its inherent suitability for hardware. Overall, we note that the most promising hardware optimization for Bin-RLWE is [60], where He et al. demonstrate that using the combination of algorithmic derivation and parallel computation, a high areadelay product (ADP) can be achieved by improving area and speed simultaneously, rather than sacrificing one for another. TABLE V: Summary of literature on the topic of software optimization for PQC algorithms on resource-constrained devices Work Year Algorithm Target Hardware Optimization Technique Metrics Considered 1 Speed Memory [49] 2020 XMSS Cortex-M4 Using probabilistic method to generate signatures that are easier to verify, at the cost of longer signing time. + -[47] 2020 NTRU Prime Cortex-M4 Good's trick for improving speed and mixed radix NTT for improving memory usage. + + [43] 2020 Picnic -Generating data on demand; avoid caching; streaming signature output. -+ [52] 2020 Saber -Using alternative algorithms to convert polynomial multiplication into large integer multiplication suitable for acceleration with CCoPs. + ? [51] 2021 General LBC RISC-V Efficient instruction sets for 32-bit RISC-V processors suitable for polynomial multiplication. + ? [44] 2022 Dilithium Cortex-M4 Streaming and compressing data; alternative algorithms for polynomial multiplication. -+ [53] 2022 Dilithium Cortex-A series Using parallel small polynomial multiplication to accelerate Sign and Verify operations for Dilithium. + -[50] 2022 Falcon Cortex-A series Utilizing NEON engine and vector instructions to parallelize FFT/NTT-based polynomial multiplication. + ? [45] 2022 Saber RISC-V Finding optimized NTT parameters for polynomial multiplication; generating data on demand. 1 Symbols are used to indicate whether a proposed implementation intends to improve (+), sacrifice (-) a metric. Question mark (?) indicates that a metric is not included in the evaluation. Kim et al. [62] is another recent work that we found significant for the current PQC optimization landscape. This work uses a look-up-table-based approach for NTT modular multiplication, and a real-time processing for polynomial sampling. These optimization techniques were applied on ASIC and an overall improvement in all metrics (speed, area, memory and energy) were reported. As reported in the paper, this implementation can be used for any LBC where the modulus parameter (q) is smaller than 2 24 -which both Dilithium (q = 8, 380, 417) and Kyber (q = 7, 681) satisfy. Therefore, it would be interesting to investigate how well this design (and ASIC crypto-acc"
    },
    {
      "heading": "VII. GRAPHIC PROCESSOR UNIT (GPU) ACCELERATION",
      "text": "FOR PQC Another category of papers we surveyed is on the topic of GPU acceleration for PQC algorithms. GPU has long been used for cryptographic computation due to the advantage offered by its inherent parallel architecture [71]. Aside from its original purpose (graphic rendering), a major use case of GPU computation is the mining of crypto-currencies [72], in which a GPU's large number of parallel cores can be used to reach very high throughput compared to general-purpose CPUs. Similarly, the advantage offered by GPU computation can be leveraged for PQC -especially for LBC, as the key to accelerating LBC is parallelizing expensive polynomial multiplication (as per earlier discussion). Among works we TABLE VI: Summary of literature on the topic of hardware optimization for PQC algorithms on FPGA and ASIC Work Year Algorithm Hardware Optimization Technique Metrics Consideredfoot_2 Speed Area Power Memory [57] 2020 Bin-RLWE FPGA Novel masked implementation method against differencial power attack without using a pseudo random number generator. + + ? × [65] 2020 RLWE ASIC Using novel Approximate Modular Multiplier design for faster RLWE. + + + ? [66] 2020 LEDAkem FPGA Optimized block partitioning and lazy accumulation techniques. ? + ? ? [67] 2020 LEDAsig FPGA Compact design with novel technique for quasi-cyclic block rotation by using read-first feature of BRAMs. -+ ? -[64] 2021 SPHINCS+ FPGA Area-efficient implementation using a data path around a sequential SHA-256 core, and a control path with nested Finite State Machines. -+ ? -[58] 2022 Bin-RLWE FPGA Efficient vector multiplication with simple shift-and-add algorithm over a finite ring. + -? + [62] 2022 General LBC ASIC Look-up-table-based modular multiplication and real-time processing for polynomial sampling. + + + + [68] 2022 General LBC ASIC Accelerating NTT by parallelizing calculations with multiple arithmetic units in circuit design. + -? ? [69] 2022 XMSS FPGA Hardware implementation with optimized node traversal for the Ltree and Merkel tree; utilizing the Buchmann-Dahmen-Schneider algorithm for faster signature generation. + ? ? × [70] 2022 XMSS FPGA Two designs that use multiple hash cores for concurrent computation. + -+ × [59] 2022 Bin-RLWE FPGA Novel algorithmic operations for polynomial multiplication and addition. +/-foot_3 -/+ 2 ? × [60] 2022 Bin-RLWE FPGA Novel implementation-oriented algorithmic derivation for achieving low implementation complexity. + + ? × [63] 2022 Dilithium and Falcon FPGA Overall high performance design for Dilithium, and first hardware implementation for Falcon-verify. + + ? - surveyed four papers are dedicated to GPU acceleration for LBC [73]- [76] as shown in Table VII. A commonality among these papers is that their \"optimization\" is purely aimed for computation speed/throughput without consideration for other metrics -after all, the GPUs used in their implementation are powerful hardware with high energy demand, which can execute hundreds of thousands LBC key encapsulation and decapsulation per second. For example, [74] reports that with an optimized GPU algorithm for SABER, a single GPU can reach a key generation throughput that is equivalent to 64 CPU cores, and [76] shows that a NVIDIA RTX3080 GPU can process 124,418 key exchanges per second using SABER, whereas a Intel Core i9-10900K can only process 29,836 per second. For smaller devices, Lee et al. [77] show that the same technique is also feasible on a Jetson Nano, which is an embedded device with a Cortex-A53 processor and a small 128-core GPU. It is shown that by utilizing GPU acceleration on the Jetson Nano, Kyber-512 can achieve a throughput performance of 1,345 key encapsulations per second and 918 key decapsulations per second. Although [77] does not mention how this result compares to using CPU, it is possible to crosscheck it against [36], which reports that on a Raspberry Pi 3 (which uses the same Cortex-A53 processor), the average execution time for Kyber-512 is 1.1 ms -or 909 operations for second -for both encapsulation and decapsulation. As the same paper reports that Kyber performs more than 9 times faster with GPU acceleration (RTX2060) than using CPU alone (Intel i9-9700F), it's reasonable to suspect that for lightweight devices, accelerating PQC computation with small GPUs may not be as rewarding. However, more data is needed on this topic to obtain a definitive answer. Regardless of the size, we should note that the usage of GPU is not common in resource-constrained IoT nodes. An argument repeatedly used by Lee et al. [73], [75], [76] is that GPUs may be used in gateway devices to provide \"Encryptionas-a-Service\" for IoT nodes in proximity. Whilst such novelty may prove its value in a sophisticated IoT network, it does not directly contribute to solving the resource-constraint problem at the node layer: unless the node devices are also capable of dynamic post-quantum key-exchange and authentication, this arrangement does not improve the "
    },
    {
      "heading": "VIII. CRITICAL REFLECTION AND FUTURE DIRECTIONS",
      "text": "In this paper, we surveyed 35 recent publications on the performance and optimization of PQC for IoT applications. First we reviewed papers which evaluate the performance of PQC -both as cryptographic primitive functions and as parts of TLS-like protocols -in resource-constrained devices. Then we examined optimized software and hardware PQC implementations, which may extend the protection of PQC to lightweight systems. Finally, we reviewed recent attempts TABLE VII: Summary of literature on the topic of GPU acceleration for PQC algorithms Work Year Algorithm GPU Optimization Technique [77] 2021 Kyber NVIDIA RTX2060; 128core Maxwell GPU Accelerating Kyber KEM's encapsulation and decapsulation using a fine-grain parallel GPU implementation. [74] 2021 Saber NVIDIA A100 Accelerating Saber key generation on GPU with Physical Unclonable Functions (PUFs). [75] 2022 NTRU, LAC, FrodoKEM NVIDIA RTX2060 Accelerating LBC polynomial multiplication with NVIDIA Tensor Core Technology. [76] 2022 FrodoKEM, Saber NVIDIA RTX 3080, V100 and T4 Accelerating LBC polynomial multiplication with dot-product instructions available on NVIDIA GPUs. GPU acceleration for PQC, which may be possible on the IoT gateway layer. On performance evaluation, we note that lattice-based KEMs seem to be feasible in reasonably constrained devices, and some KEMs were reported to outperform the current state-of-the-art key exchange algorithms in terms of speed. However, post-quantum signature schemes appear to have slower computation speed and heavier memory footprint than their pre-quantum counterparts. As a result, the KEMTLS -an alternative TLS-like protocol which does not need onthe-fly signature generation and uses long-term KEM public keys to implicitly authenticate users -seems to be a convenient arrangement for resource-constrained IoT networks. Furthermore, we realize that recent works all focus on a limited selection of PQC algorithms, and metrics used in these works are not uniform. We suggest that a comprehensive PQC evaluation framework should be proposed specifically for resource-constrained platforms so that IoT applications in the post-quantum era can be designed efficiently and safely. On optimized software designs, recent development in this area mainly focuses on the trade-off between speed and memory usage. We reviewed the implementations and summarize that the recent proposals generally fall into one of the three categories: lazy initiation (for reducing peak memory usage at the cost of speed), parallelization (for accelerating speed at the cost of memory usage and design complexity) and alternative polynomial multiplication for lattice-based algorithms. In general, there is no \"free lunch\" in this regard and the feasibility of those trade-off designs depends on the application context. We also note that some proposals were made to optimize PQC algorithms that are no longer being considered for standardization by the NIST, making them less relevant today. As a result, we believe that exploration in this direction should focus on algorithms already selected by the NIST to maximize research contribution. On hardware implementations, we found that recently proposed schemes are no longer trade-offs, and it is possible to improve several metrics simultaneously by utilizing efficient designs. We observed that many proposals report improvement in both speed and area complexity, but power consumption seems to be a metric on which less attention has been paid. Overall, the performance improvements achieved by ASIC designs seem to be more significant than that of FPGA designs, but it should be kept in mind that ASICs lack flexibility and generally have longer time-to-market. Additionally, we note the lack of \"baseline\" for hardware makes cross-comparison difficult, especially when specific algorithms have received significantly less attention in this regard. The lack of attention on hardware implementation for the NIST-selected candidates might be due to the fact that an alternative LBC algorithm -the more lightweight, hardware-friendly Bin-RLWE -is simpler to implement. However, we argue that because the Bin-RLWE is not a NIST PQC candidate, the likelihood of it receiving global recognition is low and more attention should be paid to the primary NIST recommendations (i.e. Kyber and Dilithium) in the future. Finally, GPU acceleration for PQC is a less explored field to date despite the long history of using GPU for (prequantum) cryptographic computation. We note that as far as high-end GPUs are concerned, the benefit of GPU acceleration is significant: it is possible to reach a throughput of hundreds of thousands of key exchanges per second on a single GPU. However, it seems that smaller GPUs found on SoCs do not offer the same advantage against lightweight Cortex-A series CPUs. This may potentially make GPU acceleration less suitable for resource-constrained IoT systems, as highend GPUs often have high power demand and the possibility of i"
    },
    {
      "heading": "IX. CONCLUSION",
      "text": "Quantum computing poses great threat to public key cryptosystems that are widely used today, but the computation of PQC may be heavyweight for IoT applications, which typically employs a large number of lightweight and resourceconstrained devices. In this paper, 35 recent publications are surveyed and the latest developments on performance evaluation, software/hardware optimization, and GPU acceleration for PQC are discussed in the context of IoT. By tabulating and analyzing their findings, we summarize and present key discoveries in this area. We further conclude that whilst some PQC schemes are feasible for reasonably lightweight IoT devices, proposals for their optimization seem to lack focus and standardization. As the NIST is set to publish the first standard for PQC in 2024, we suggest future research to seek coordination, in order to ensure an efficient and safe migration toward IoT for the post-quantum era."
    }
  ],
  "figures": [],
  "equations": []
}