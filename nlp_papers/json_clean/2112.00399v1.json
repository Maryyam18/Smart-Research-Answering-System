{
  "paperid": "2112.00399v1",
  "title": "Quantum-Resistant Cryptography",
  "authors": [
    "John Mattsson",
    "Ben Smeets",
    "Erik Thormarker",
    "S Aaronson",
    "C Kerry",
    "C Director",
    "M Hamburg",
    "P Kocher",
    "M Marson",
    "D Stebila"
  ],
  "year": 2024,
  "abstract": "Quantum-resistant cryptography is cryptography that aims to deliver cryptographic functions and protocols that remain secure even if large-scale fault-tolerant quantum computers are built. NIST will soon announce the first selected public-key cryptography algorithms in its Post-Quantum Cryptography (PQC) standardization which is the most important current effort in the field of quantum-resistant cryptography. This report provides an overview to security experts who do not yet have a deep understanding of quantumresistant cryptography. It surveys the computational model of quantum computers; the quantum algorithms that affect cryptography the most; the risk of Cryptographically Relevant Quantum Computers (CRQCs) being built; the security of symmetric and public-key cryptography in the presence of CRQCs; the NIST PQC standardization effort; the migration to quantum-resistant public-key cryptography; the relevance of Quantum Key Distribution as a complement to conventional cryptography; and the relevance of Quantum Random Number Generators as a complement to current hardware Random Number Generators.",
  "sections": [
    {
      "heading": "Scope",
      "text": "Quantum-resistant cryptography is cryptography that aims to deliver cryptographic functions and protocols that remain secure even if large-scale fault-tolerant quantum computers are built. In this report we summarize the current state of quantum-resistant cryptography and report on the progress of the most important effort in this area: the NIST Post-Quantum Cryptography standardization. We also give a summary of the security and practicality of Quantum Key Distribution (QKD) since it has been mentioned as a hypothetical quantum-resistant solution in the past. As part of the background for the report we give a high-level overview of the computational model for quantum computers, the quantum algorithms that affect cryptography the most, and what we know about the progress of building machines that can execute these algorithms. This report is not an implementation guide, rather it aims to provide an overview of the state-of-the-art and what we can expect in the coming years. A paragraph or section marked with contains information that is slightly more technical than other parts of the report. These parts are not necessary to read to understand the overall report. We include a discussion on quantum random number generators (QRNGs) in Appendix A. This is a technology that is sometimes mentioned in connection with QKD and quantum computers, however QRNGs do not really fit into our overall scope here since our current hardware randomness generator technology is secure and quantum-resistant. Also, QRNGs themselves do not solve the current urgent question to have secure random number generators in virtualized (VMs, containerized) products. However, as we discuss in Appendix A, if trustworthy vendors make QRNG technology in the future that is as well-understood and certified to the same degree as common RNG technology, then QRNGs could be evaluated as alternatives to common RNG technology."
    },
    {
      "heading": "Terminology",
      "text": "We call our current traditional computers \"classical\". A \"classical attacker\" is one that uses a classical computer and not a quantum one. Similarly, a \"classical algorithm\" is one that can execute on a classical computer."
    },
    {
      "heading": "Quantum computers",
      "text": "Following [131], we define a Cryptographically Relevant Quantum Computer (CRQC) as a quantum computer of sufficient size and fault tolerance to break today's public-key cryptography using Shor's quantum algorithm 2 ."
    },
    {
      "heading": "The computational model of quantum computers",
      "text": "âž¢ Quantum computers are not general-purpose (super) computers, rather they are potential special-purpose machines for certain problems where we can leverage their peculiar nature through clever quantum algorithms. This section contains an informal high-level introduction to the quantum computational model. It is not necessary to read to understand material outside of this section or the conclusions in the subsequent subsections of this section. It is however necessary to read the section in order to understand a few of the details in those other subsections. Quantum algorithms are typically expressed in the circuit model. In this model, one typically starts from a set of qubits, applies a series of quantum gates to them, and measures in the end to produce an output. The state of a qubit ð‘„ is a linear combination of the states 0 Ì… and 1 Ì… with coefficients ð›¼ and ð›½ that are complex numbers 3 . That is, ð‘„ = ð›¼0 Ì… + ð›½1 Ì… . The states 0 Ì… and 1 Ì… can be thought of as the 0 and 1 state of a classical bit. It is sometimes said that the state of the qubit is \"between\" 0 Ì… and 1 Ì… . Figure 1 shows some example operations in a very simple circuit. Figure 1: Example operations in the circuit model. 2 One could also define a CRQC as a quantum computer that is sufficiently powerful to threaten today's cryptography in general, thereby also covering, for example, implementations of Grover's algorithm against symmetric cryptography. However, the importance of Shor's algorithm (see Section 3.2) together with the extremely long running time of Grover's algorithm (see Section 3.3), for cryptographically relevant problems, makes it reasonable to let the purpose of the definition be a kind of shorthand to describe a machine that can threaten today's public key cryptography using Shor's algorithm. A special case that would be somewhat problematic with this definition would be if quantum computers turned out to be much faster than classical computers and if the possible circuit width (see Section 3.1) of implementations turned out to be restricted in a way so that cryptographically relevant implementations of Grover's algorithm that target a low width usage (see e.g., [132]) are possible, while cryptographically relevant implementations of Shor's algorithm are not. It is implicitly assumed in [131] that CRQCs can execute Shor's algorithm on cryptographically relevant problems. We make this explicit in our definition. At the start of the circuit (to the left in the figure), we have two qubits, and each qubit is independently in the simple state 0 Ì… . That is, ð‘„ = ð›¼0 Ì… + ð›½1 Ì… , with ð›¼ = 1 and ð›½ = 0 in the notation from above. As we apply quantum gates, the qubits become entangled, which roughly means that their collective state can no longer be expressed as a simple function of their individual states. If we have N many qubits, then after we applied a series of gates, our collective qubit state is a linear combination of 2 N many distinct states. Measurement is a process that collapses the linear combination of the 2 N states into a single one state among them 4 . The single state present after the collapse is the output state of the computation. So, while we operate on exponentially many states at once in a sense when we apply gates throughout the circuit, we can only output a single state in the end. Worse yet, the output state is completely randomly chosen according to a probability distribution that depends on the coefficients in the linear combination of the 2 N states. So, if the output is not useful, then one needs to run the algorithm again. Our selection of gates decides the probability distribution on the coefficients, but the gates have certain limitations on them as well. The design of quantum algorithms is now the art of applying gates -chosen from a typically pretty small, limited set of possible gates -in a way so that the probability of getting a useful output state when measuring is sufficiently high. This inherent limitation of quantum computing -together with the common assumption that quantum computers will remain slower and more expensive than classical computers -hints at an important observation: Quantum computers are not general-purpose super computers, rather they are potential special-purpose machines for certain problems where we can leverage their peculiar nature through clever quantum algorithms. Shor's quantum algorithm in Section 3.2 is precisely such a clever algorithm that solves two central problems whose intractability public-key cryptography relies on for its security. Another potential very important application of sufficiently powerful quantum computers is physics simulations, an original intended application of quantum computing was indeed to simulate quantum mechanics itself [138]. Aaronson suggests that common misconceptions about the power of quantum computers originate from focusing only being able to operate on \"exponentially many states at once\", and then jumping to the incorrect conclusi"
    },
    {
      "heading": "Shor's quantum algorithm",
      "text": "âž¢ If CRQCs are built, then Shor's quantum algorithm will break today's public-key cryptography. Variants of the algorithm apply to both the discrete logarithm problem (which breaks elliptic curve cryptography and finite field Diffie-Hellman) and to factoring large integers (which breaks RSA). âž¢ Shor's algorithm is efficient (relatively shallow depth in the circuit model). In the case of using Shor's algorithm to attack RSA, it is essentially as efficient as performing an RSA signing/decryption operation, but on a quantum computer. Shor's algorithm is certainly the most important quantum algorithm with regard to its impact on cryptography. Shor's algorithm is also one of the most important quantum algorithms in general, since it was this algorithm that showed in 1994 that large-scale quantum computers can efficiently solve important problems (such as factoring very large integers) which appear to be intractable on classical computers. Shor's algorithm consists of a quantum subroutine and some post-processing that can be done on a classical computer. The quantum subroutine for the variant of the algorithm which factors integers is shown in Figure 3. The most expensive operation in the circuit of Figure 3 is the sequence of gates on the bottom wire(s), the ones whose names start with \"U\". This sequence of gates performs a modular exponentiation modulo N, where N is the number (e.g., RSA modulus) that we are trying to factor. Just as for classical computers, this can be done efficientlyfoot_4 using e.g., the square-and-multiply algorithm. This means that Shor's algorithm is essentially as efficient as performing an RSA signing/decryption operation, but on a quantum computer. This is typically estimated to take a few hours on a CRQC [130]. However, the width of the circuit (number of wires) is thousands of qubits 7 for sizes of N that are relevant to cryptography. As discussed in Section 3.1 such qubits are typically envisioned to be implemented through error correction on layers of lower layer qubits (physical qubits in Section 3.1) of poorer quality. By typical estimates we then expect to need millions of such lower quality qubits in the lower layers (which are not visible in Figure 3) [129][20][100] [128]. The rightmost boxes in Figure 3 are measurement operations, as discussed in Section 3.1. These operations output classical information from which we have a good probability of being able to factor N after post-processing. The variant of Shor's algorithm that finds discrete logarithms looks similar but, e.g., in the case of elliptic curve discrete logarithms, we perform two scalar multiplications instead of a modular exponentiation (see Figure 1 in [57]). This results in similar efficiency. The number of qubits required in Shor's algorithm against elliptic curve cryptography (ECC) is estimated to be somewhat lower than the number of qubits required in Shor's algorithm against RSA (factoring) [57]. This is not too surprising since an ECC problem of, for example, size 256 bits is typically estimated to be roughly equivalent in cryptographic strength to an RSA problem of size 3072 bits.This difference in problem size is due to the fact that only very inefficient (exponential) classical attacks are known for ECC [55], while somewhat more efficient (sub-exponential) classical attacks are known against RSA [56]. Thus, we naturally see differences when we also consider quantum attackers that can use Shor's algorithm against both ECC and RSA. However, the number of logical qubits needed for Shor's algorithm against ECC is still estimated to be in the thousands, as for Shor's algorithm against RSA [57]."
    },
    {
      "heading": "3.3",
      "text": "Grover's quantum algorithm and symmetric cryptography âž¢ When attacking today's symmetric cryptography, Grover's algorithm is an extremely long-running (extremely large depth in the circuit model) computation that does not parallelize efficiently. âž¢ Our most important symmetric primitives, ciphers, MAC algorithms and hash functions are quantum-resistant as is at e.g., the 128-bit security level. In particular, no \"doubling of key length\" is necessary due to CRQCs. Grover's algorithm is in a sense an algorithm that finds a needle in a haystack. It quickly puts the collective qubit state in a linear combination of 2 N states where each state would be as likely to be outputted if we immediately measured. It then iteratively, roughly 2 N/2 times, applies a set of gates such that each iteration, just so slightly, makes a single target state more likely to be found if we would measure. When we measure in the end, after all 2 N/2 iterations, we find the target state with very high probability. When using this method to attack AES-128, the target state is an encoding of the unknown key which was used to encrypt a target plaintext-ciphertext pair which we assume that we have somehow collectedfoot_6 . The previously mentioned \"set of gates\" used in each of the 2 N/2 iterations are actually an implementation of encrypting the target plaintext and checking if the result matches the target ciphertext. As discussed in Section 3.1 we can do this for each of the 2 N states that we start from in parallel, each state being the encoding of a candidate key. Since we have 2 128 candidate keys in AES-128, we have N = 128. We then also see that we require 2 64 iterations of this procedure, before measuring in the end. One might ask why we cannot use a set of gates so that we need only 1 iteration instead of 2 64 , but this is where the limitations on the set of gates which can be used that we mentioned in Section 3.1 comes in. In fact, one can show that Grover's algorithm solves the generic problem using an optimal number of iterations [60]. The depth of each iteration (measured in the number of serial gates applied) is a problem here. If each iteration contains a serial application of, say 2 11   gates 9 , then Grover's algorithm as a whole requires a serial application of 2 64 â€¢ 2 11 = 2 75   gates. And this is on a single quantum computer, counting only the logical gates in the circuit model. On top of this we have potential overhead from e.g., error correction on the physical qubits as discussed in Section 3.1. In comparison, a classical 5 GHz CPU core executes about 2 57 cycles in a year. Actually, NIST mentions 2 40 as an estimate for the number of serial logical gates that could be applied in a year for presently envisioned quantum computing architectures [59]. These numbers mean that in practice we would need split up the key space and search in each smaller partition of the key space on a separate quantum computer [61]. Grover's algorithm does not parallelize efficiently. For classical computers we can split up a brute force search for a key over S many computers and find the key S times faster. But Grover's algorithm is not a brute force search. In fact, for Grover's algorithm we need S 2 quantum computers to find the key S times faster. As an example, under the 2 75 and 2 40  estimates mentioned above, it would take one billion CRQCs, one million years of uninterrupted computation to find a sought AES-128 key. The up-shot is that AES-128 is a relevant security level even in the presence of quantum computers and indeed AES-128 is one of the relevant security categories in the NIST PQC standardization [59]. This means that NIST thinks it is relevant to standardize new public key cryptography that is as costly to break on a quantum computer as it is to do key search for AES-128 using Grover's algorithm. Attacks on MAC algorithms (e.g., HMAC-SHA-256) using Grover's algorithm are implemented similarly to attacks on AES using Grover's algorithm, and the same conclusions hold. This means that our most important symmetric primitives, ciphers, and MAC algorithms are quantum-resistant as is. In particular, no key length changes due to quantum computers are known to be necessary at this point [44]. We think that the view of NIST [44], namely that current deployments using 128-bit symmetric keys do not need to be upgraded because of quantum computers by current knowledge, is well supported by arguments from research as outlined above. Of course, this does not rule out that there could be a quantum algorithm for exploiting a specific weakness in a specific algorithm such as AES-128. But no specific examples of such attacks are known right now for our most relevant symmetric primitives. Newer deployments may want to also consider 256-bit symmetric primitives due to e.g., compliance, and also because the effect on performance may be quite small as commented by e.g., ENISA [64]."
    },
    {
      "heading": "Grover's quantum algorithm and cryptographic hash functions",
      "text": "âž¢ There are no known quantum attacks for finding collisions for hash functions that outperform classical attacks for relevant cost measures. The typical relevant attack against a cryptographic hash function is to find a collision: Two distinct inputs that hash to the same output. There is a quite famous variant of Grover's quantum algorithm [87] that finds hash collisions after only roughly 2 N/3 quantum evaluations of the target hash function, where N is the bit-size of the hash digest (e.g., 256 for SHA-256). For an ideal hash function, no classical algorithm can do it in less than approximately an expected 2 N/2 hash function evaluations (the \"birthday bound\" [88]). This is however only of theoretical interest; the number of hash function evaluations is only indirectly relevant; a directly relevant cost measure is the hardware cost times the running time. As it turns out, the quantum algorithm actually uses roughly 2 N/3 quantum-accessible hardware units as well. So, the hardware cost times running time actually grows roughly like 2 2N/3 , and that is on a quantum computer. At the same time, as discussed by Bernstein [89], a classical computing cluster made up of 2 N/6 cheap devices that implement the hash function and work in parallel can find a collision in time proportional to 2 N/3 , by together performing the total 2 N/2 hash function evaluations 10that are expected to be necessary by the birthday bound. The fact that quantum attacks against hash functions are no better than classical attacks is visible in the NIST PQC standardization security levels. As explained in Section 4.5, these levels correspond to the optimal attacks (quantum or classical) for key recovery against an ideal block cipher (exemplified by AES) or collision finding for an ideal hash function (exemplified by SHA-2). The attack costs for SHA-2 are based on the most efficient classical attack; no quantum attacks are mentioned."
    },
    {
      "heading": "Building quantum computers",
      "text": "âž¢ It is very unclear when, or even if, a CRQC will ever be built. The gap between today's quantum computers and envisioned CRQCs is huge, and the field faces some near-term challenges such as for example no known applications for the Noisy Intermediate-Scale Quantum (NISQ) computers that are expected to be built these coming years. âž¢ The best current estimate we have is that an expert committee in 2019 concluded that the emergence of a CRQC during the next decade would be highly unexpected. The collective effort of working towards quantum computers that can execute large-scale circuits in the circuit model is broad and complex. There are several publicly known engagements both in academia and industry [93][92]. There is also a variety of potential quantum computer realizations (e.g., in terms of how to realize the physical qubits and quantum gates) being studied and proposed, with superconducting qubits and qubits based on trapped ions being popular candidates. However, there is a huge gap between today's noisy small quantum computers and envisioned CRQCs [26]. IBM wrote \"Knowing the way forward doesn't remove the obstacles; we face some of the biggest challenges in the history of technological progress.\" when presenting a roadmap for scaling quantum technology in 2020 [99]. The latest and best estimate we have is that a committee of experts from academia and industry concluded in a 2019 report that the emergence during the next decade of a CRQC is highly unexpected [26]. The same report states that there are no known practical applications for the Noisy Intermediate-Scale Quantum (NISQ) computers that we may see in these coming years. Since the report argues that it was the virtuous cycle of incomes and reinvestments that allowed transistor counts in classical computers to grow according to Moore's law for decades, a key finding of the report is"
    },
    {
      "heading": "\"Research and development into practical commercial applications of noisy intermediate-scale quantum (NISQ) computers is an issue of immediate urgency for the field.\"",
      "text": "The report argues that the field of building quantum computers may become dependent strictly on government funding otherwise, if useful applications in the near-term cannot be found. Aaronson said in 2021 [137] that there isâ€¦ allowed researchers to craft a collision in 2 63 hash function evaluations [91], which is better than the 2 80 generic bound described above. \"Some hope to eke out a near-term advantage [over classical computers] for e.g., quantum simulation with little or no error correction. But claims that we know how to get near-term speedups for optimization, machine learning, etc. are >95% BS!\" As explained in Section 3.1, quantum computers are not general-purpose super computers, rather they are potential special purpose machines for certain problems where we can leverage their peculiar nature through clever quantum algorithms. Aaronson thinks that the original intended application of quantum computers -simulating quantum physics -will be the \"killer app\" for quantum computers [2]. Besides technical and usability arguments, judging the progress of quantum computers is further complicated by other aspects. Pornin discusses how judging the progress of quantum computers involves in fact the people -who not only have the best insight into the progress -but are also the same ones who depend on funding to keep the ball that is this very expensive research endeavor rolling [95]. Essentially, what actors in the security community (i.e., industry, academia, governments, and standardization organizations) are doing is watching each other to assess how other actors appear to judge the risk. We can then note that the security community as a whole appears to be calmly awaiting the outcome of NIST PQC standardization that we will discuss in the next section. Media reporting on quantum computers naturally often focuses on simple metrics such as physical qubit count. As we have discussed, qubit count is not the only relevant metric, but we note that current quantum computers typically have about 100 physical qubits [99][100] [135]. We talked about in Section 3.2 how a CRQC is estimated to require a number of physical qubits in the millions. Given the short history of quantum computers and the many competing technologies, it is difficult to estimate how qubit counts will grow the coming years and decades [98]. Simply assuming a Moore's law type scaling in physical qubit counts, it would take 25-30 years to go from 100 qubits to millions of qubits. Of course, there could also be for example improvements in quantum error correction techniques. IBM's roadmap in 2020 aimed to unveil a 127-qubit machine by 2021, a 433qubit quantum computer by 2022, and a 1121-qubit quantum computer by 2023 [99]. The 127-qubit machine was recently unveiled in 2021 as planned in the roadmap [135]. A claim that a team of researchers had shown \"Quantum supremacy\" through a computation on a quantum computer consisting of 54 physical qubits caused some stir in 2019 [18]. Quantum supremacy is the achievement of doing a computation on a quantum computer that cannot be achieved in any feasible time on a classical computer. The computation does not have to be useful for anything else than showing Quantum supremacy and the computational task can apparently be chosen in a biased way so that it is naturally much easier for the specific quantum computer at hand than for classical computers 11 . Borcherds criticized this notion of Quantum supremacy by arguing that Quantum supremacy failed at the teapot test [133]. Borcherds explained that teapots achieve \"Teapot supremacy\" over classical computers because it is very hard to compute on 11 In this case the computational task was to estimate the probability distribution that repeatedly executing a short sequence of quantum gates on a certain number of physical qubits in a specific quantum computer architecture and then measuring the result gives rise to. The classical hardness of the problem appears to still be debated, see [127]. a classical computer exactly how a teapot will shatter if it is tossed in the floor. However, using a teapot, we can compute this by tossing the teapot in the floor and observing the result. While teapots do achieve Teapot supremacy, they are clearly not superior computing devices compared to classical computers. Since Quantum supremacy is similarly biased in the favor of quantum computers, the concept of Quantum supremacy fails in Borcherds's teapot test. Aaronson on the other hand, suggested that we should feel some excitement about the Quantum supremacy experiment [19], seeing it as an important milestone for the field. Aaronson does however stress that the gap between the Quantum supremacy machine and CRQCs is huge and that the research community has no idea how long it will take to close that gap [20]. Some researchers believe that the gap might never be closed in practice [21][22]. Dyakonov [22] says: \"I believe that, appearances to the contrary, the quantum computing "
    },
    {
      "heading": "Why the wait?",
      "text": "âž¢ Establishing trust in new cryptographic primitives takes time. Especially for publickey cryptography where typically a new class of mathematical problems must be analyzed and well understood to ensure security. âž¢ Elliptic curve cryptography is too efficient, flexible, and rich to give up easily. âž¢ Updating standards and libraries takes time, and updating procedures and protocols is costly. âž¢ Hybrid solutions can offer important additional assurance when migrating to PQC. One might ask why actors are waiting to implement PQC if the threat of quantum computers cannot be ruled out and if information that is confidentiality protected today through public-key cryptography can be recorded and potentially attacked in the future. The main reasons are randomness used in creating the encapsulation is revealed to the recipient during the decapsulation procedure. The recipient then asserts that the encapsulation was generated in an honest manner using this randomness. This is the so-called Fujisaki-Okamoto transform [97] which is a generic method to strengthen the security of the recipient's static key by making sure that only honestly generated encapsulations are completely processed during decapsulation (see also [122] for variants). In contrast, for DH we have instantiations such as X25519 [51] where it is secure to reuse a key pair in multiple DH key exchanges without using a transform like Fujisaki-Okamoto. These DH instantiations are the result of carefully developing parameter sets and (domain-aware) checks that avoid and mitigate known attacks, see for example [66]. This can naturally be more efficient than a generic mitigation through a transform like Fujisaki-Okamoto. Renes suggests that the heavy usage of the Fujisaki-Okamoto transform can make the NIST PQC candidates more vulnerable to certain physical side-channels such as power analysis [116]. âž¢ Establishing trust in cryptographic primitives is a slow process. Ideally, we want as many experts as possible to try to break a primitive for as long time as possible before we put it into production use. Some of the PQC systems are relatively new and for new public-key cryptography there is typically a new class of mathematical problems that must be analyzed and well understood to ensure security. After the security community jointly establishes trust in a system, actors also need to wait for standardization. Part of the trust-establishment step has merged with standardization in the case of the NIST PQC standardization. After standardization, actors additionally need access to well-reviewed libraries and hardware that can support operations with the new cryptographic algorithms. Standardization also may take place in stages; first the cryptographic primitives may be standardized, then protocols such as TLS may be updated to support them. âž¢ There is a cost associated with the PQC systems. Elliptic curve cryptography (ECC) provides a wide variety of very efficient primitives. We also have very efficient and secure implementations and standards available. Nobody wants to give up ECC 13 . As we will see, one typical difference between PQC and ECC is larger keys or signatures. Furthermore, changing algorithms (affecting public keys and certificates), and updating security critical software and hardware obviously costs money. There is thus a balance between prudent preparations for switching to quantum-resistant cryptography on the one hand, and making sure that the investment in implementing quantum-resistant cryptography will be a long-term secure and good choice, on the other hand. A potential solution to the issue in the first bullet point is to run two cryptographic systems in parallel, one that is quantum-resistant and one that is well-studied and trusted to be secure against classical adversaries. So, for example, we could derive a final session key in a key establishment protocol from a combination of KPQC and KECDH, where KPQC is the key from a PQC key establishment mechanism and KECDH is the shared secret from an elliptic curve Diffie-Hellman key exchange. This is for example what was done when Cloudflare and Google tested PQC schemes in TLS [102], and NIST plans to incorporate this possibility into a future revision of SP 800-56C that deals with key establishment [101]. An attacker would need to break both primitives independently to recover the final derived key. This is sometimes called a hybrid scheme, but note that hybrid is used for other things as well in cryptography. The obvious downsides to the hybrid solution are poorer efficiency, additional communication overhead, and additional code complexity. These downsides can be manageable though and hybrid solutions can offer important additional assurance when migrating to PQC. One could consider a similar hybrid solution for signatures as well [1][101], however, here the overhead would be more significant if hybrid signatures were used throughout a long certificate chain. Also, incorporating PQC into"
    },
    {
      "heading": "Fitting PQC into existing protocols",
      "text": "âž¢ The main difference with fitting PQC into existing protocols is larger public key or signature sizes. The performance of the PQC schemes with regard to running time is often comparable to that of today's public-key cryptography. Taking TLS as an example, a KEM could replace the key establishment done through Diffie-Hellman key exchange today in TLS 1.3. For forward secrecy 14 , the client could send an ephemeral KEM public key to the server to which the server responds with an encapsulation of a shared a key K. The session key is then derived from K, much like it is derived from the shared Diffie-Hellman secret in TLS 1.3 today. In TLS, the handshake is authenticated by the server towards the client using a digital signature over all the information exchanged during the handshake. The client authenticates towards the server either through digital signature, or in the application layer (by e.g., \"logging in\") as is often the case in human-to-server communication on the Web. These signatures could of course be replaced by PQC signatures. But for this kind of pairwise (interactive) communication there is also another possibility. The server could authenticate by proving knowledge of a key K that is decapsulated from an encapsulation which is encapsulated towards a KEM public key tied through PKI to the server. This means that we replace a signature sent from the server with an encapsulation sent from the client to the server and e.g., a MAC tag computed using K (over the information in the handshake) sent from the server to the client. Of course, the most relevant overhead in practice may be a certificate chain that certifies the server public key rather than the size of the signature or KEM public key used by the server for authentication. See [33] for a discussion about using KEMs for handshake authentication in TLS. Existing versions of TLS could also have relied on authentication through static DH keys (instead of RSA-based signatures), but one problem with this would be the overwhelming popularity of certificates with RSA keys. Static DH keys are used for authentication in for example Signal X3DH [123] and EDHOC [124]. Note that the DH primitive is more flexible than a KEM. In the X3DH and EDHOC examples, an ephemeral DH public key (which can be thought of as the encapsulation when mapping to a KEM) can be combined with multiple different DH keys to form shared secrets, something which is not possible with KEM encapsulations in general. As applications 14 Forward secrecy typically implies that new ephemeral public keys are used for key establishment in essentially every session. After the session ends, all keys are securely deleted. Thus, session keys cannot leak at a later time. Such ephemeral public keys must of course be authenticated by some means to provide any security. migrate to PQC, new certificates will be needed anyway and maybe there will be more widespread usage of KEMs for pair-wise authentication. As shown in Section 4.5 the main difference with fitting PQC into existing protocols is communication overhead. The performance of the PQC schemes with regard to running time is typically comparable to that of today's public-key cryptography. NIST summarizes the status of replacing current public-key cryptography with any of the different (classes of) PQC algorithms as \"Unfortunately, each class has at least one requirement for secure implementation that makes drop-in replacement unsuitable.\" [67] The earlier mentioned PQC TLS experiment by Google and Cloudflare gives some observations about using PQC in TLS [118][16]. In this experiment, Diffie-Hellman key exchange in TLS 1.3 was experimentally replaced with two hybrid alternatives: a) ECDH + NTRU-HRSS-KEMfoot_10 , and b) ECDH + SIKE. As can be seen in Table 1 of Section 4.5, alternative b) has smaller communication overhead but slower running times, while alternative a) has the opposite properties. While the clients were biased towards more powerful platforms (x64 and AArch64) -which could favor the slower running times of alternative b) -the overall TLS handshake times tended to be faster for alternative a)."
    },
    {
      "heading": "Fitting PQC into constrained scenarios",
      "text": "Over constrained network links, the typically larger signature and encapsulation sizes of the PQC schemes may lead to for example packet fragmentation and increased energy spent in the radio or wire-line interfaces on sending more bits. For constrained devices, there has been quite a lot of implementation work on the 32-bit Cortex M4. In the NIST PQC standardization third round conference, the pqm4 project reported on the performance of implementations of almost all the finalist candidates in the standardization [114]. The results where for a platform with the Cortex M4, 128 KB RAM and 1 MB of flash storage. On the KEM side, SABER performed well, while the results on the signature side were more mixed. Implementations can be challenged by the fact that the RAM is not even large enough to hold the whole public key for some of the standardization candidates. In [115], implementers work with Classic McEliece (whose public key is 260 KB for its smallest parameter set) on a platform with 192 KB of RAM. In the call for proposals, NIST asks implementers to also consider even more constrained 16-and 8-bit microprocessors [113]. Atkins [112] says that RAM appears to be the bottleneck with storage currently growing at a faster rate over time. So, Atkins suggests that NIST should focus on RAM usage in the standardization, rather than code size, when considering devices that are even more constrained than typical platforms with the Cortex M4."
    },
    {
      "heading": "Stateful hash-based signatures",
      "text": "âž¢ Stateful hash-based signature schemes depend on security-critical dynamic state. Otherwise, their security depends only on some generally accepted assumptions about cryptographic hash functions. âž¢ The standardized stateful hash-based signature schemes are currently the only option for applications that o can handle the dynamic state, o cannot wait for the outcome of the NIST PQC standardization, and o are prepared to deal with using a stateful algorithm that may not yet be supported by common tools for PKI. Two stateful hash-based signature schemes have been standardized by IETF and NIST 16 : LMS and XMSS. We discussed in Section 3 how the security of hash functions is unaffected by quantum computers. These schemes are indeed PQC schemes, and this is the reason why there was a sudden interest to standardize them. The idea of using hash functions to build signature schemes is otherwise quite old but has not been interesting up to now since signature sizes are much larger than for signature schemes currently deployed and the stateful hash-based signature schemes have the serious burden of depending on security critical dynamic state. We explain the idea behind the standardized stateful hash-based signature schemes in Appendix B. It is critical for security that two distinct messages are not signed using the same private key state (leaf WOTS private key in Appendix B). This dynamic security-critical state is the reason that NIST and NSA consider these schemes not to be suitable for general use [41][42]. These schemes, LMS and XMSS, are currently the only option for applications that can handle the dynamic state, cannot wait for the outcome of the NIST PQC standardization and are prepared to deal with using a signature algorithm that may not be supported by tools for PKI."
    },
    {
      "heading": "NIST PQC standardization -candidates and progress",
      "text": "âž¢ Lattice-based cryptography will offer a good middle-way for PQC with efficient running times and medium-sized communication overhead. These schemes are represented both as finalist KEM and signature algorithm candidates in the standardization. NIST aims to announce the first selected algorithms for standardization close to the end of 2021 and provide draft standards in 2022-2023 17 . From the initial 69 accepted 1 st round candidates, the 3 rd round candidates consist of 7 finalists and 8 alternate candidates. The first algorithms selected by NIST will mainly be chosen from the finalists. Additional algorithms may be selected after an additional round of evaluation. Four of the finalists are 16 See [110] for a comparison between LMS and XMSS written by authors from the LMS team. 17 Unless otherwise stated, the implicit reference in this section is the NIST Status Update on the 3 rd Round [69] from the Third PQC Standardization Conference (June 7-9, 2021). KEMs and three are signature schemes. NIST has successfully attracted world-leading experts in the field of PQC to make submissions in the standardization. To assess the security of the PQC candidates, NIST has asked the submissions to rank the security of the candidates according to security levels that are based on the optimal classical and quantum attacks on ideal ciphers (exemplified by AES) and hash functions (exemplified by SHA-2). For example, the schemes in Table 1 and 2 are believed to be security level 1. This means that any significant quantum or classical attack on them should be as expensive as a key recovery attack on AES-128 (using either classical brute-force or Grover's quantum algorithm) [59]. Some experts have argued that security level 1 or 2 should not be used due to cryptanalysis for the PQC candidates still not being completely settled down [136]. âž¢ Three of the finalist KEMs (Kyber, NTRU, SABER) are so-called structured latticebased schemes. NIST expects to select at most one of these KEMs for standardization, but may standardize more than one. Lattice-based cryptography uses hard problems on mathematical objects called lattices as the foundation for its security [71]. While security reductions that relate the security of some of these lattice-based schemes to known conjectured-to-be-hard computational problems on lattices exist, for efficiency, concrete schemes are typically instantiated with heuristic assumptions and parameters are set only with respect to best known attacks in practice [72]. An example of a heuristic assumption could be that a certain structure in a problem does not give an attacker any advantage compared to the corresponding unstructured mathematical problem. Overall, there is quite strong confidence in the security of lattice-based schemes. Cryptographic applications have been studied extensively the last two decades [71]  As stated earlier, there are three different finalists in the digital signature track. NIST is concerned about the lack of diversity in these finalists (two being structured lattice-based schemes and the third having an unstable security history), and has therefore declared that there will be a new call for proposals for additional quantum-resistant digital signature schemes at the end of the third round of the original standardization effort. The third round is expected to end in late 2021 or early 2022. âž¢ Two of the finalist signature schemes Dilithium and Falcon are structured latticebased schemes. They vary in their construction, and Falcon has significantly smaller signatures (roughly 800 bytes). In their talk at the Third PQC Standardization Conference, the Dilithium team pointed out that the design choice of Falcon that enables small signatures also makes secure implementation of the scheme significantly more complicated and difficult than in the Dilithium case [80]. Like for their KEM counterparts, there is quite strong confidence in the security of these structured lattice-based signature schemes, and they have had a stable security history through the NIST PQC standardization. Like for the structured lattice-based KEM schemes, NIST expects to standardize at most one of these two schemes and that scheme has a good chance to be the main general purpose PQC signature scheme considered for adoption in protocols and industry in the years following the NIST PQC standardization. âž¢ The third finalist signature scheme is Rainbow which is based on multivariate cryptography. Rainbow's security relies on the hardness of finding solutions to certain polynomial equations with certain structure. There have been relevant attacks on Rainbow during the standardization process [81]. The Rainbow team responded to the latest attack by only updating the way that attacker cost is estimated in their security model, thus leaving their parameter sets unchanged [82]. NIST has expressed concern about the security of multivariate cryptography [83], and ENISA states that \"the security analysis of Rainbow canno"
    },
    {
      "heading": "Possible adoption of quantum-resistant cryptography in future 3GPP networks",
      "text": "The cryptographic protection in the radio access of 3GPP networks relies mostly on symmetric cryptography today (e.g., UE authentication and radio access ciphering/integrity protection). As we saw in Section 3.3 there is no need to update this protection specifically due to the threat of large-scale quantum computers 21 . In contrast, the SUPI protection (SUCI [125]) that relies on public-key cryptography is affected by quantum computers and so is any 3GPP network domain security that relies on public-key cryptography (e.g., IKEv2 in the IPsec suite and TLS) as explained in the previous section. A possible upgrade path for SUCI is HPKE [126] (a public-key encryption system based on the KEM+DEM paradigm which is under development in the IETF) which is expected to be updated with PQC algorithms at the same time as TLS."
    },
    {
      "heading": "5",
      "text": "Summary of the security of Quantum Key Distribution (QKD) âž¢ QKD typically needs to rely on many of the same computational assumptions as conventional efficient cryptography. âž¢ QKD has less well-understood of a risk profile and implementation security than conventional cryptography. âž¢ QKD is inherently tied to custom hardware. This can increase cost and risk, compared to conventional cryptography that can often be patched and upgraded in software. âž¢ QKD is inherently tied to the physical layer, this gives a quite different attack surface than conventional cryptography. âž¢ QKD is fundamentally a point-to-point protocol. This can imply a dependency on trusted intermediate nodes that is not compatible with the modern technology environment which is moving towards end-to-end encryption and Zero Trust principles. âž¢ QKD may be more sensitive to Denial-of-Service attacks than conventional cryptography. âž¢ There is a consensus in the security community that QKD has many fundamental issues that would need to be solved before being considered as a secure complement to conventional cryptography. QKD is the primary example of so-called Quantum Cryptography but it is important to understand that it has nothing necessarily to do with quantum computers. The idea was first described in the 80's [34] and several commercial systems exist. We refer the interested reader to Wikipedia for a simple description of QKD using photons [34]. Essentially, in theory, QKD allows Alice and Bob to agree on a shared (classical) key by Alice sending qubits (photons in the Wikipedia example) to Bob over a quantum communication channel. By sending and receiving the photons using a clever encoding scheme Alice and Bob can determine with almost certainty if an attacker has interfered and eavesdropped (i.e., measured the qubits). If this is the case, they abort. Otherwise, they can derive a short key that a potential attacker has supposedly essentially no information about. An important condition is that Alice and Bob need an authenticated conventional communication channel (in addition to the quantum one) to make comparisons and exchange information. Also, harmless noise on the quantum channel and potential interference by an eavesdropper are indistinguishable. It is sometimes said that QKD is provably secure from \"the laws of physics\" or similar (see e.g., [23] for references). Bernstein [23] criticizes these kinds of formulations as misleading. Indeed, if the security of QKD is provable from the laws of physics, then it sounds like QKD is a well-defined physical process that has some proven physical security property. But the published physical attacks on commercial QKD (see e.g., [24] for references) contradict this. Bernstein in fact argues that it is unlikely that a physical process similar to how QKD is typically described could be absolutely provably secure since information about the agreed upon (classical) shared key presumably leaks (to some small, albeit non-zero amount) through various standard physical phenomena such as e.g., electromagnetic radiation 22 . The way that we will make sense of the connection between the security proofs for QKD and the physical systems claiming to perform QKD in the rest of this report is the following. There is a protocol called QKD taking place in a mathematical model that is modelled after quantum mechanics. Attackers are limited in the mathematical model to certain well-defined mathematical actions and physical leakage through sidechannels or other physical attacks on the protocol is out of scope in the model. Presumably, the theoretical model cannot be implemented absolutely securely in the physical world due to e.g., side-channel attacks, as argued by Bernstein. Then we have hardware components provided by actors that are claimed to provide a sufficiently secure implementation (a kind of practically realizable approximation) of the theoretical protocol 23 . As we will argue below, this is where part of the security problems with QKD begin since the most important attack surface against modern cryptography is implementation details. QKD is sometimes said to be provably secure without relying on computational assumptions. At the same time, we mentioned above that QKD needs external authentication. For this reason, QKD has been described as a \"key expansion primitive\" [9]: A short pre-shared symmetric key used for authentication is expanded through QKD into an 22 See also the assumption about \"Sealed laboratories\" in device independent QKD later in the text. 23 It is common to have security proofs (in a mathematical model) building on computational assumptions for conventional cryptography as well. In the gap, between that security proof and a concrete implementation executing in a given threat model, lives side-channel attacks that depend on implementation details which are out of scope in the mathematical model. unconditionally secure, which -once again -means that the system as a whole is not uncond"
    }
  ],
  "figures": [],
  "equations": []
}